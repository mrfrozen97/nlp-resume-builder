{
  "0": {
    "id": "4182146309",
    "Company": "New Jersey Prevention Network",
    "title": "Junior Data Engineer",
    "description": "Organization: New Jersey Prevention Network (www.njpn.org)\u00a0Position Available: Junior Data Engineer\u00a0Location: Tinton Falls, NJ\u00a0\u00a0The New Jersey Prevention Network is a public health agency working to prevent substance misuse, addiction, and other chronic diseases by building capacity among professionals, fostering positive collaboration among providers, and strengthening the field of prevention through the use of evidence-based practices and strategies.\u00a0\u00a0New Jersey Prevention Network has a supportive work environment, a reputation for excellence and offers competitive salaries and benefits. Being a part of the NJPN team provides our staff with opportunities to develop new skills and to make an impact in the work you do.\u00a0\u00a0NJPN is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.\u00a0\u00a0Job Summary:\u00a0The Junior Data Engineer is responsible for database management, maintaining SQL, visualizations, scripting, programming and building models for data driven solutions.\u00a0\u00a0The Junior Data Engineer will need to respond quickly and accurately to ad hoc data requests.\u00a0\u00a0Specific Duties:\u00a0\u00a0Collaborate with department leaders to develop and refine data collection instruments and improve data systems and processes.\u00a0Consult with NJPN\u2019s Data Engineer Consultant to create database mapping and ensure consistent integration of database development.\u00a0\u00a0Develop, maintain, and test infrastructures for data generation.\u00a0Design and develop database systems, ensuring high levels of performance and security.\u00a0Transform data into a format that can be easily analyzed and develop dashboards for data visualization.\u00a0Perform complex data extraction, storage, manipulation and analysis.\u00a0Create and maintain optimal data pipeline architecture using scripting/programming languages like Python and PowerShell.\u00a0Create API's and effectively integrate them.\u00a0Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs.\u00a0Prepare routine reports and presentations of program performance with data graphs/service trends; Clearly communicate (oral and written) results of analyses to program staff/leadership and other non-technical audiences.\u00a0Required Skills/Abilities:\u00a0\u00a0Must be able to work a flexible hybrid (mix of at home, in office and onsite) schedule, including Saturdays, when needed.\u202f\u202f\u00a0Proven experience as a Data Engineer.\u00a0Familiarity in all versions of SQL and experience with structured and unstructured database systems.\u00a0Experience with data architecture, data modeling, schema design, and ETL development.\u00a0Strong scripting and programming skills with experience in Python, PowerShell, etc.\u00a0Proficient in C#, VB.Net, JavaScript, jQuery, and Crystal Reports.\u00a0Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.\u00a0Excellent interpersonal and customer service skills.\u00a0\u00a0Excellent organizational skills and attention to detail.\u00a0\u00a0Excellent time management skills with a proven ability to meet deadlines.\u00a0Preferred Experience:\u00a0Behavioral Health experience.\u00a0Experience working with diverse teams.\u00a0Two years minimum, or equivalent experience in Cloud Infrastructure and systems integration.\u00a0\u00a0Academic Requirements: Bachelor's degree in Computer Science, Statistics, IT or related field, Master's degree preferred.\u00a0Other Requirements:\u00a0\u00a0Maintain all NJPN issued equipment in good working order, requesting IT support as needed.\u00a0Adherence to NJPN policies is expected.\u00a0\u00a0\u00a0\u00a0\u00a0Assisting with annual conference and other agency-wide events as needed. \u202f\u00a0\u00a0Travel as required for meetings, events and conferences.\u202f\u00a0Other duties as assigned.\u00a0\u00a0Physical Requirements:\u00a0\u00a0Subject to possible prolonged periods of sitting at a desk and working on a computer.\u00a0Must be able to lift up to 25 pounds at times.\u00a0\u00a0Benefits:\u00a0Salary: $65,000 annualized \u00a0Comprehensive medical, dental, vision, life and & LTD plan options\u00a0Generous paid leave (vacation, sick, personal)\u00a0401K plan with employer match.\u00a0Employee Assistance Plan\u00a0\u00a0The Junior Data Engineer is a 6.5-month exempt position, requiring 35 hours per week with a flexible work schedule. We\u2019re looking for a motivated individual with a passion for data engineering and a strong attention to detail.\u00a0\u00a0A cover letter highlighting your interest and experience is required. Submit cover letter and resume to the following: Career@NJPN.Org.\u00a0\u00a0\u00a0Please Note: Cover letter and Resume are required.\u00a0\u00a0\n",
    "location": "Tinton Falls, NJ"
  },
  "1": {
    "id": "4125056035",
    "Company": "Guac",
    "title": "Data Engineer",
    "description": "At Guac, we're on a mission to solve grocery food waste with predictive ML. We forecast exactly how much of each product will sell, to put an end to the millions of tons of food that goes to waste every single day due to bad inventory replenishment.We're currently working with major supermarket chains in the US (you've probably shopped at some of them before), and we're backed by Y Combinator, 1984 Ventures, Collaborative Fund, and angels from Instacart and Citadel Securities.We're seeking a talented Data Engineer to join our team in-person in New York to help shape the future of grocery!About the RoleAs a Data Engineer at Guac, you'll have ownership over building and maintaining our ETL infrastructure and pipelines, as well as optimizing ML systems for forecasting grocery demand. You\u2019ll also work directly with our customers to ensure successful technical integrations.\u00a0Around 80% of your time will be spent on engineering:Designing and implementing scalable data pipelines for processing large-scale data across multiple customersOptimizing machine learning systems for demand forecastingExpanding our recommendation engine to meet the needs of new customersContributing to our backend services to serve our mobile and web applicationsYou\u2019ll be working across languages and technologies such as: Python, FastAPI, Dagster, GCP (including BigQuery), Dask, and SQL.\u00a0Around 20% of your time will be spent on customer facing work:Collaborating directly with customers' technical teams to ensure smooth data integrations and system implementationsWorking with customers\u2019 internal supply chain and store operations teams to understand and then implement their unique business logic\u00a0About YouAt least 3 years of relevant work experience and a minimum of a Bachelor\u2019s degree or equivalent in Computer ScienceStrong background in data engineering and experience with large-scale distributed data processing systemsProven experience in implementing and deploying machine learning models, particularly for time series forecastingExcellent communication skills with experience in customer-facing technical rolesFamiliarity with modern data infrastructure and cloud computing platformsExisting background in backend development (Python/FastAPI) is a big plusWhy Work with UsIntellectually challenging problems that require a lot of thinking and creativity (like how to scale ML systems across diverse grocery chains). Since we\u2019re still a small team, you won't be pigeon-holed into one problem, product, or technologyMake a real-world impact: the grocery industry is enormous (in the US, it accounts for ~4% of GDP), and we're excited about the potential for a huge impact we can make with better inventory operations on both grocers' bottom-line but also on the climate and food securityEngage with interesting customers and see the on-the-ground operations of the grocery industry\n",
    "location": "New York, NY"
  },
  "2": {
    "id": "4142541482",
    "Company": "MGIC",
    "title": "Data Engineer Intern",
    "description": "Why work at MGIC?Are you someone who wants to play a critical role in our company\u2019s success? Do you enjoy solving puzzles and finding a better way to get things done? Are you someone who likes to Take The Lead and make an impact? If so, then imagine yourself at MGIC. At MGIC we are a team of dedicated professionals on a fearless mission. A team that fosters a culture of career development and continuous learning opportunities to help you rise to new heights. We are passionate about providing outstanding customer service and making a difference in our community. #WeAreMGICAre you someone who wants to play a critical role in our company\u2019s success? Are you interested in learning more about how data is the lifeblood of every organization? Interested in joining a team that is smack dab in the middle of a strategic transformation? We are looking for a hardworking and curious team member to join our evolving Data Delivery and Engineering team. The ideal candidate must be someone that is intrigued and passionate about learning how data is acquired, managed, transformed, and ultimately made available to the Enterprise for analytics and reporting needs.How will you make an impact?Utilize your programming skills to design, develop, and test data pipelines to ingest data to Cloud data platform Enhance and maintain existing data pipelines that are built using Cloud tools Perform troubleshooting of data issues and date pipelines Assure data consistency and reliability in the Cloud data platform Gain exposure to technologies such as AWS, Snowflake, Fivetran, and Dbt. Do you have what it takes?Pursuing a bachelor's degree in computer science, data analytics, business analytics, data science, or related degree Hours: Summer is full-time and could potentially lead to a year-round internship Onsite / Hybrid work environment based in our headquarters in downtown Milwaukee, WI Familiar with Data Warehousing and Business Intelligence concepts Proficient or exposure to SQL and Python Collaborative and self-motivated with a desire to learn and adopt new technologies Demonstrated creative thinking, proactive in seeking additional work and strong initiative Enjoy these benefits from day one: Competitive Salary & pay-for-performance bonus Financial Benefits (401k with company match, profit sharing, HSA, wellness program) On-site Fitness Center and classes (corporate office) Paid-time off and paid company holidays Business casual dressFor additional information about MGIC and to apply, please visit our website at www.mgic.com/careers.Note to all recruitment agencies:MGIC does not accept unsolicited agency resumes. Any unsolicited resumes sent to MGIC, directly or indirectly, will be considered MGIC property. MGIC is not responsible for any agency fees associated with unsolicited resumes. A recruiting agency must have a valid, written and fully executed agency agreement to assist with a requisition.\n        ",
    "location": "Milwaukee, WI"
  },
  "3": {
    "id": "4183103063",
    "Company": "Manifold Bio",
    "title": "Data Engineer",
    "description": "Manifold Bio is a biotech company pursuing a pipeline of protein therapeutics using novel molecular measurement technologies and library-guided protein engineering. Our drug discovery engine is differentiated by massively parallel screening in vivo from the beginning of our discovery process. This unique platform is powered by a proprietary protein barcoding technology that allows multiplexed protein quantitation at unprecedented scale and sensitivity. We combine this and other high-throughput protein engineering approaches with computational design to create antibody-like drugs and other biologics. Our world-class team of protein engineers, biologists, and computational scientists are working together to aim the platform at therapeutic opportunities where precise targeting is the key to overcoming clinical challenges.PositionManifold Bio is seeking an exceptional Data Engineer to join our team. This role will help lead the full life cycle of Manifold\u2019s platform data including modeling, design, coding, testing and deployment of solutions across our scientific research effort. This role will help build standard-driven data integration and automation processes to manage the integrity and quality of Manifold platform data used for reporting and analytics. This role will work closely with the Computational Team and other wet-lab scientists to identify unmet data needs and implement novel solutions as Manifold grows. Expertise in the life sciences and experience with the Benchling Data Model is a plus. The ideal candidate will have experience implementing solutions from previous roles.ResponsibilitiesWork closely with Manifold\u2019s Computational Team and wet-lab scientists to identify and deploy solutions to augment our ability to capture, store, and make decisions based on our dataCreate tools, models, algorithms and data pipelines to support novel data streamsCreate interfaces for researchers to access data without engineering supportPresent and report on data model and infrastructure updates to the teamOwn interfaces and integrations with partner services, including BenchlingQualifications5+ years of relevant programming experience (including Python)Demonstrated and proven experience modeling and building data solutions (e.g. Postgresql)Experience developing, orchestrating and supporting ETL pipelinesCloud computing experience with Amazon Web Services (AWS)Experience with data profiling, data quality, master data management, metadata managementExperience across multiple operating systems: Unix/Linux, Mac, and [tolerance of] WindowsDetail-oriented with excellent problem identification and problem-solving skillsDemonstrated ability to work both independently and as part of a teamA deep passion for data modeling and developing new methodsPREFERRED: master\u2019s degree, project management experience, relevant certs in data science or project managementPREFERRED: experience working with Next Generation Sequencing (NGS) dataIf you\u2019re excited to build a platform that combines these technologies to revolutionize how protein therapeutic discovery happens, please apply! We value different experiences and ways of thinking and believe the most talented teams are built by bringing together people of diverse cultures, genders, and backgrounds.\n",
    "location": "Boston, MA"
  },
  "4": {
    "id": "4186483473",
    "Company": "SeemaS",
    "title": "Data Engineer",
    "description": "About SeemaS, Inc.At SeemaS, we\u2019re transforming the corporate tax compliance landscape with our AI-driven platform, designed to simplify and streamline tax management for businesses worldwide. Our innovative solution autonomously handles complex tax compliance tasks, including tax planning, economic analysis, and global documentation. By harnessing the power of artificial intelligence, SeemaS reduces the burden of manual tax compliance, enabling companies to focus on growth and strategy. Whether navigating international tax laws or ensuring regulatory alignment, SeemaS delivers accuracy, transparency, and efficiency at every step.\u00a0Key ResponsibilitiesDesign, develop, and optimize scalable data pipelines to ingest, process, and manage global financial data from multiple sources.Implement and maintain ETL workflows for structured and unstructured financial datasets, ensuring data integrity and consistency.Facilitate the harmonization of data across multiple sourcesAutomate data ingestion and transformation using AWS services.Develop real-time and batch processing pipelines to support financial modeling, transaction valuation, and anomaly detection.Build monitoring and validation systems to ensure high-quality, reliable, and performant data pipelines.Implement\u00a0 methods of data quality and assurance.Implement AI-readiness of data\u00a0 for AI models.Work closely with Data Architects and Data Scientists to provide clean, and well-governed data for analytics and AI models.\u00a0Optimize data workflows for performance, scalability, and cost efficiency in a cloud-native environment.Devise visualization dashboards to track processes and tell data stories.Basic QualificationsBachelor\u2019s degree in Computer Science, Data Engineering, or a related field.\u00a03+ years of experience in data engineering, ETL development, and cloud-based data processing.\u00a0Experience designing and optimizing data pipelines for large-scale financial datasets.Strong proficiency in Python, SQL, and distributed data processing frameworks.\u00a0Familiarity with AI/ML model integration into data pipelines for financial analytics.Hands-on experience with AWS services.Preferred QualificationsBachelor\u2019s, or Master\u2019s degree in Computer Science, Data Engineering, Data Analytics, or a related field.\u00a0Hands-on experience with infrastructure as code for data pipeline automation.\u00a0Knowledge of data governance, security, and compliance best practices for financial data.Join Us in Redefining Tax ComplianceAt SeemaS, you\u2019ll work on cutting-edge technologies, solve complex challenges, and contribute to groundbreaking products that redefine financial compliance and management. If you\u2019re ready to take on this exciting journey, we\u2019d love to hear from you!How to Apply\u00a0Please apply using the following link:\u00a0https://seemas.bamboohr.com/careers/28?source=aWQ9MTg%3D\n",
    "location": "United States"
  },
  "5": {
    "id": "4179630745",
    "Company": "Medidata Solutions",
    "title": "Data Engineer Intern",
    "description": "About our Company:Medidata: Powering Smarter Treatments and Healthier PeopleMedidata, a Dassault Syst\u00e8mes company, is leading the digital transformation of life sciences, creating hope for millions of people. Medidata helps generate the evidence and insights to help pharmaceutical, biotech, medical device and diagnostics companies, and academic researchers accelerate value, minimize risk, and optimize outcomes. More than one million registered users across 2,000+ customers and partners access the world's most trusted platform for clinical development, commercial, and real-world data. Known for its groundbreaking technological innovations, Medidata has supported more than 33,000 clinical trials and 10 million study participants. Medidata is headquartered in New York City and has offices around the world to meet the needs of its customers. Discover more at www.medidata.com and follow us on LinkedIn , Instagram , and X .The Program:At Medidata, interns will have the opportunity to accelerate their careers by working closely with experienced professionals and gain valuable, hands-on, full-time work experience. By being a part of our global organization, interns have the opportunity to work alongside our talented and committed professionals helping them to build a strong foundation for achieving their career goals. For 12 weeks, beginning May 19, 2025, interns will have an opportunity to gain a deep understanding of what it means to be a Medidatian. United around a single goal of empowering smarter treatments and healthier people. Medidatians work in a culture of curiosity, innovation and fun. You will be contributing to the line of business with sustainable and meaningful work.Our Summer Internship program also includes instructor led training, guided mentorship, exposure to senior leadership and community service. In addition to individual and specific related responsibilities, each intern will participate in our Intern Innovation Lab. Assigned to cross-functional teams, interns will work closely to develop an innovative solution to a business problem currently facing Medidata. As they work diligently to present their final solutions to a panel of top Medidata leaders, we are confident that our interns will make a significant impact on our business.About the Team:We are seeking a motivated and detail-oriented Data Engineer Intern to assist in the design, development, and optimization of data pipelines and databases that support business insights and decision-making. In this role, you will work closely with the Office of Social Impact and Engagement team, as well as our data and IT teams, to build scalable data solutions. This is a great opportunity for an individual looking to gain hands-on experience in data engineering, ETL processes, and database management.Responsibilities:Data Pipeline Development: Assist in designing, building, and maintaining ETL (Extract, Transform, Load) processes to ingest and transform data from various sources.Database Design & Management: Support the development and optimization of databases to ensure efficient data storage, integrity, and retrieval.Data Integration: Work with structured and unstructured data sources to integrate information into centralized databases or data warehouses.Query Optimization: Assist in writing and optimizing SQL queries to improve database performance and enable efficient data access.Big Data Processing: Learn and apply big data technologies such as Apache Spark, Hadoop, or cloud-based data services.Data Quality & Validation: Implement data validation and cleansing techniques to ensure accuracy, consistency, and completeness of data.Collaboration: Work with cross-functional teams to understand data needs and contribute to scalable data solutions.Documentation: Maintain detailed documentation of data models, ETL processes, and database structures for future reference.Security & Compliance: Assist in ensuring data security, privacy, and compliance with relevant policies and best practices.Qualifications:Technical Skills: Familiarity with SQL and relational database management systems (e.g., MySQL, PostgreSQL, SQL Server).Programming Knowledge: Exposure to Python, Scala, or Java for data manipulation and pipeline development.Data Processing: Understanding of ETL processes and tools such as Apache Airflow, dbt, or Talend is a plus.Cloud & Big Data Tools (Preferred): Experience with cloud platforms like AWS (Redshift, S3, Glue), Google Cloud (BigQuery, Dataflow), or Azure Data Services.Problem-Solving: Strong analytical and problem-solving skills with attention to detail.Communication: Ability to explain technical concepts to non-technical stakeholders.Collaboration: Ability to work effectively in a team-oriented environment.Adaptability: Willingness to learn and experiment with new data technologies and methodologies.Preferred Qualifications:Experience with data warehousing concepts and tools.Familiarity with version control systems like Git.Basic understanding of data security and privacy principles.Exposure to data visualization tools (e.g., Tableau, Power BI, Looker) is a plus.The salary range posted below refers only to positions that will be physically based in New York. As with all roles, Medidata sets ranges based on a number of factors including function, level, candidate expertise and experience, and geographic location. Pay ranges for candidates in locations other than New York, may differ based on the local market data in that region. The base hourly pay range for this position is $32.00 an hour and a $3,500 sign on bonus.Note: Please be on the lookout for job scams. Medidata recruiters will never ask applicants for monetary compensation, credit card, or banking details.Equal Employment Opportunity:In order to provide equal employment and advancement opportunities to all individuals, employment decisions at Medidata are based on merit, qualifications and abilities. Medidata is committed to a policy of non-discrimination and equal opportunity for all employees and qualified applicants without regard to race, color, religion, gender, sex (including pregnancy, childbirth or medical or common conditions related to pregnancy or childbirth), sexual orientation, gender identity, gender expression, marital status, familial status, national origin, ancestry, age, disability, veteran status, military service, application for military service, genetic information, receipt of free medical care, or any other characteristic protected under applicable law. Medidata will make reasonable accommodations for qualified individuals with known disabilities, in accordance with applicable law.Applications will be accepted on an ongoing basis until the position is filled. Diversity As a game-changer in sustainable technology and innovation, Medidata, Dassault Syst\u00e8mes company, is striving to build more inclusive and diverse teams across the globe. We believe that our people are our number one asset and we want all employees to feel empowered to bring their whole selves to work every day. It is our goal that our people feel a sense of pride and a passion for belonging. As a company leading change, it\u2019s our responsibility to foster opportunities for all people to participate in a harmonized Workforce of the Future.\n",
    "location": "New York, United States"
  },
  "6": {
    "id": "4177939049",
    "Company": "Visa",
    "title": "Data Engineer, Intern - Austin, TX",
    "description": "Company DescriptionVisa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose \u2013 to uplift everyone, everywhere by being the best way to pay and be paid.Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.Job DescriptionOur team is dedicated to providing systems and services to make data secure, high quality, rich, fast, and easy to use, therefore enabling Visa the ability to leverage its data asset in an effective and timely manner to maximize technology/business development and differentiate Visa from others in the payment industry.We are seeking an intern to join the team in the summer of 2025, and as an intern, you will get the chance to work with multiple teams, systems, and products through a wide variety data sources using Big Data and Cloud technologies. As an intern, you will get the opportunity to work on optimizing or redesigning our company's data architecture to support our next generation of products and data initiatives.Our summer internship also comes with social and professional networking events and developmental workshops, designed to provide you the resources and training to have an impactful summer. You will get to work on one or several hands-on projects with the team.Some job duties and projects could include:Self-serve deployment of Kafka clusters using Docker and Kubernetes with auto scale up capability. Creating a natural language Chatbot for searching CDISI knowledge base. Open NSFW & Apache SeaTunnelData Infused Presentation BuilderSmart Client NarrationQualificationsBasic QualificationsStudents pursuing a Bachelor\u2019s or Masters degree in Computer Science, Computer Engineering, CIS/MIS, Cybersecurity, Business or a related field, with less than 2 years relevant work experience, graduating December 2025 or later. Strong communications skills, specifically, the absence of repeated grammatical or typographical errors, clear and concise written and spoken communications that demonstrate professional judgment. Preferred QualificationsProficiency in at least one programming language or technology including, but not limited, to: Python, Java, SQL, LINUX/UNIX, React.js, Scala, Spark, PySpark, Hadoop, Kafka, Rust, NoSQL, or KubernetesCoursework or Knowledge of Gen AI, LLM, Machine LearningKnowledge of Distributed Systems Design, Scalability, High Availability ResiliencyKnowledge in Mobile Device delivery. E.g. Cloud Services, pluginsWell-versed in MS Excel and knowing how to use function(s) such as V-Look Up or FormulasThe ability to take on challenges and address problems head-onStrong ability to collaborateHighly driven, resourceful and results orientedGood team player and excellent interpersonal skillsGood analytical and problem-solving skillsDemonstrated ability to lead and navigate through ambiguityAdditional InformationU.S. APPLICANTS ONLY: The estimated hourly range for a new hire into this position is $27/hr to $31/hr which may include potential sales incentive payments (if applicable). Salary may vary depending on job-related factors which may include knowledge, skills, experience, and location. In addition, this position may be eligible for bonus and equity. Visa has a comprehensive benefits package for which this position may be eligible that includes Medical, Dental, Vision, 401 (k), FSA/HSA, Life Insurance, Paid Time Off, and Wellness Program.Work Authorization: Visa will not sponsor applicants for work visas in connection with this position. Future sponsorship will not be considered.Work Hours: Varies upon the needs of the department.This is a hybrid position: Expectation of days in office will be confirmed by your hiring manager.Travel Requirements: This position requires travel 5-10% of the time.Mental/Physical Requirements: This position will be performed in an office setting. The position will require the incumbent to sit and stand at a desk, communicate in person and by telephone, frequently operate standard office equipment, such as telephones and computers.Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.Visa will consider for employment qualified applicants with criminal histories in a manner consistent with applicable local law, including the requirements of Article 49 of the San Francisco Police Code.\n        ",
    "location": "Austin, TX"
  },
  "7": {
    "id": "4133960732",
    "Company": "Cerrowire",
    "title": "Data Engineer Intern",
    "description": "Cerro Wire LLCCome join a team where People make the difference! As a part of Marmon Holdings, Inc., a highly decentralized organization, we rely heavily on people with the aptitude, attitude, and entrepreneurial spirit to drive our success, and we're committed to attracting and retaining top talent.A Summer Experience. A Lifetime Of Value.We\u2019ll meet you where you are and help you go further. This summer, we\u2019re committed to bringing early-in-career talent together, trust you to own your work and help you level up through professional development, networking, and exposure to real-world projects.We\u2019re doing things that matter. Energizing North America: That\u2019s the impact of your work at Cerrowire. Copper building wire doesn\u2019t just supply electricity. It turns houses into homes. It allows hospitals to deliver the care patients need. It keeps every single industry we rely on up and running. And it all starts with you.As a part of Marmon, your impact also goes way beyond North America. You\u2019re helping keep millions around the world healthy, connected and safe.Join the Biggest Small Business You\u2019ll Ever Find. Headquartered in Hartselle, Alabama, Cerrowire is a copper wire manufacturing company with plants in Alabama, Georgia, Indiana, and Utah. We build, energize, and inspire by supplying building wire and cable throughout North America for commercial, industrial and residential use. We bring power to life.Cerrowire is hiring talented and innovative interns looking to power up their futures and forge a meaningful career with us. Be part of our efforts to help improve the quality of life for millions of people by engineering solutions to many of the world\u2019s greatest needs.You will gain substantial personal learning and career development opportunities through ownership of real-world job assignments, strong coaching and mentorship, and networking opportunities with senior leaders and other interns across our organization. Our interns receive the opportunity to showcase their achievements to our Leaders for the opportunity to grow and make an impact in the current workplace, across a group, and around the world.At Cerrowire, a Marmon company, you get the best of both worlds. The strength and stability that comes with being part of Berkshire Hathaway, plus the autonomy and opportunity that comes with working at one of our 120+ companies.What You\u2019ll DoThe Data Engineer Intern will design and implement an automated ETL pipeline to collect and process data from production equipment, sensors, and other sources for analysis. They will manage and evaluate solutions for large scale data processing systems, data warehousing services and solutions using advanced database technologies. Additional projects and assignments as needed.What You\u2019ll NeedPursuing an undergraduate or graduate degree in Computer Science, or related field Rising Junior or Senior Required experience with SQL, Power BI, Azure Preferred experience with Python, Snowflake, Apache, Git, or related tools Excellent written, verbal communication and presentation skills Ability to effectively communicate issues and solutions to all levels of the team Strong analytical and problem-solving skills Compensation$20.00 - $24.00 per hour, commensurate with relevant experience and educational background Work Hours/Length Of ProgramThe internship will run for 12 weeks from May to August Full Time, targeting 40 hours per week (possibility for more flexible hours and Part Time if necessary) Exact start and end dates are flexible based on school schedules and the needs of the business This is a paid internship Working Conditions And Physical DemandsThis position is based in our Corporate Office, and employees are expected to wear business casual dress. The office is connected to a state-of-the-art manufacturing facility and may involve general exposure to a manufacturing environment, including exposure to noise, temperature fluctuations, or other factors. Employees entering the manufacturing facility are required to wear high visibility vests, eye protection, ear protection, and steel-toed shoes.Successful completion of a drug screening is required for this role This position involves sitting for extended periods of time. May involve close vision, color vision, depth perception, focus adjustment, and viewing computer monitor for extended periods of time. Involves manual dexterity for using keyboard, mouse, and other office equipment. Location:Hartselle, AL; Remote with 20% on-site, Must be located near Hartselle/Huntsville/Birmingham, etc.Following receipt of a conditional offer of employment, candidates will be required to complete additional job-related screening processes as permitted or required by applicable law.We are an equal opportunity employer, and all applicants will be considered for employment without attention to their membership in any protected class. If you require any reasonable accommodation to complete your application or any part of the recruiting process, please email your request to careers@marmon.com, and please be sure to include the title and the location of the position for which you are applying.\n        ",
    "location": "Hartselle, AL"
  },
  "8": {
    "id": "4145173850",
    "Company": "Evvy",
    "title": "Data Engineer & Analyst",
    "description": "Why Join Evvy?At Evvy, we believe the female body shouldn\u2019t be a medical mystery. We\u2019re on a mission to close the gender health gap by leveraging overlooked female biomarkers, starting with the vaginal microbiome. Evvy\u2019s groundbreaking Vaginal Health Test is the world\u2019s only certified and peer-reviewed test to uncover the entire vaginal microbiome with a single, at-home swab. Combined with personalized insights, precision treatment pathways and 1-1 coaching, Evvy provides data-driven, personalized healthcare for vaginal infections, fertility, menopause, and more. Evvy now has the world\u2019s largest dataset on the vaginal microbiome \u2014and we\u2019re pioneering novel research that will transform female health outcomes.Why join now?We\u2019re growing fast and moving faster. Now is the time to shape the future with us.Impact at scale: We\u2019ve helped tens of thousands of patients and are rapidly growing every day.Continuous innovation: Our personalized, precision care platform and proprietary data power our research to transform how vaginal health is treated and understood. Our research insights then feed back into our care platform.Top tier investors: Backed by leading healthcare investors such as General Catalyst and LabCorp, the Evvy team includes leading OB/GYNs and vaginal microbiome researchers with decades of experience at organizations like UCSF, Stanford, Harvard, and more.About the RoleWe\u2019re looking for our first data owner who will own the full stack of Evvy\u2019s data ecosystem. You are both a data engineer and an analyst. You are a creative problem-solver, strategic thinker, and a relentless executor. You don\u2019t reinvent the wheel, but also are excited to break the mold of traditional BI.You\u2019ll collaborate closely with our VP of engineering and leaders across finance, marketing, product, operations, and clinical to design, build, and maintain a modern data architecture that drives critical decisions, supports self-service analytics, and fuels rapid, data-driven innovation. This is a unique opportunity to own the full data stack from data engineering to analytics and insights, helping us transform vaginal healthcare through data.What you\u2019ll do:Own our data end-to-end. Oversee the entire data lifecycle from ingestion, transformation, data modelling, analysis, and reporting.Collaborate with cross-functional teams to understand the \u201cwhy\u201d behind stakeholder questions, translating them into clear analytical workflows that inform decisions at all levels of the company.Develop our data infrastructure. Build and maintain robust ETL pipelines, our data warehouse, and BI platform. Help explore, evaluate and integrate AI & new data technologies to continuously improve the way we use data to drive answers.Empower self-service analytics. Partner with business stakeholders to understand our key business needs and build intuitive data models, integrate new technologies (like LLMs), so teams can independently explore data and find answers to complex questions.Conduct advanced data analytics, from LTV and retention to lab operations, to understanding clinical treatment efficacy \u2013 helping drive key decisions across the entire businessEstablish data best practices. Champion data governance, version control (Git), rigorous documentation, and organization, ensuring our data stack scales efficiently with our growth.What we're looking for3+ years of data analytics, analytics engineering or data engineering experience at a startupExperience with ecommerce data and analytics (cohorting, waterfall visualization, LTV, retention, forecasting)SQL wizard, Python experienced, data modelling expert, Git (version control) proficiencyExperience with and opinions on our current data stack - ETL tool like Stitch, Snowflake, DBT for data modelling, Metabase (or similar), or other newer BI tools (Hex, Sigma)A deep understanding of the strengths and pitfalls of traditional BI teams. You have experience across the traditional BI setup, know when not to reinvent the wheel, but also when you should break the mold.You will integrate AI into Evvy\u2019s data analytics workflows, continuously exploring and leveraging new technologies to improve our data stackYou\u2019re an owner and a self-starter who can see the big picture of how data serves our company\u2019s mission and guide improvements proactively.You have strong written and verbal communication, able to work effectively with technical and non-technical teams.You understand key business concepts and can frame out analyses to answer business questions provided guidance from non-technical teammates It\u2019s a plus if you haveFirst or early data hire at an early stage startupExperience with HIPAA and working with sensitive health dataAI and data-driven products and personalizationWorking at EvvyEvvy is based in New York, NY. Our tech team works primarily in person, believing in live collaboration and building strong relationships but we are open to remote for the right candidate!Our values:Start with the why. We have an urgent mission, ambitious goals, and big hearts \u2014 our whys should motivate and focus us. We tie everything we do back to a company goal or user need. We start by listening and center our patients, always.Figure it out. We may not know how to solve a problem at first, but that doesn\u2019t scare or slow us. We roll up our sleeves, call on our resources, and take ambiguity head on. We reframe constraints as fuel for creativity.Create win-wins. We are building something bigger than us \u2014 so we leave everything better than we found it. We\u2019re hungry for finding problems, but we come with solutions. We invent creative win-wins for our company, team, patients, and partners.Own it to the end. We\u2019re all-in owners \u2014 there is no task too big or small. We do what we say we\u2019re going to do, and we don\u2019t wait to be told to do it. We close every loop. We constantly seek out feedback, iterating on ourselves and our work.Stay eleven steps ahead. We do the impossible and achieve excellence that sets a new status quo. We always lead the pack maintaining an appreciation for the complexity in which we operate, building trust and speed at the same time.Company benefitsUnlimited PTO$1K learning and development budget per employeeHealth, dental and vision insurance offered, and HSA plans available401k plan (Roth and traditional)Monthly company social eventsAnnual offsitesPaid parental leaveEvvy is an equal opportunity employer that is committed to diversity and inclusion both in the workplace and in our application process. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n",
    "location": "New York, NY"
  },
  "9": {
    "id": "4190018410",
    "Company": "Worldpay",
    "title": "Data Engineer I",
    "description": "Job DescriptionWe are Worldpay. Our technology powers the world\u2019s economy and our teams bring innovation to life. We champion diversity to deliver the best products and solutions for our colleagues, clients and communities. If you\u2019re ready to start learning, growing and making an impact with a career in fintech, we\u2019d like to know: Are you Worldpay?About The RoleWe are seeking talented and motivated Data Engineers to join our dynamic team. The ideal candidate will have a strong background in data engineering, with experience in various technologies and tools. This role involves designing, developing, and maintaining data pipelines, ensuring data quality and integrity, and supporting data-driven decision-making processes.ResponsibilitiesDesign, develop, and maintain scalable data pipelines and ETL processes.Collaborate with data scientists, analysts, and other stakeholders to understand data requirements.Implement data architecture and data modeling solutions.Ensure data security and compliance with industry standards.Optimize and troubleshoot data workflows and processes.Develop and maintain documentation for data processes and systems.Required SkillsProficiency in SQL for querying and managing relational databases.Experience in designing and structuring data systems (Data Architecture).Knowledge of Pyspark for big data processing and analytics.Familiarity with Snowpark for building data pipelines and processing data within Snowflake.Ability to automate data workflows (Data Pipelines).Experience in writing and optimizing stored procedures.Proficiency in using Databricks for big data analytics and machine learning.Expertise in Snowflake for data warehousing and analytics.Knowledge of AWS services (S3, Lambda, IAM, Boto3) for data storage, processing, and security.Understanding of secure methods for transferring data files (Secure File Transfer Processes).Additional SkillsAbility to design and document process flows.Experience with PGP encryption for securing data.Knowledge of Terraform for infrastructure as code.Proficiency in Tableau for data visualization and reporting.Experience with Power BI for creating interactive dashboards and reports.Familiarity with Google Cloud Platform (GCP) services.Understanding of data clean rooms for secure data collaboration.Knowledge of data sharing techniques.Basic understanding of machine learning algorithms.What We Offer YouA career at Worldpay is more than just a job. It\u2019s the change to shape the future of fintech. At Worldpay, we offer you: A voice in the future of fintech Always-on learning and development Collaborative work environment Opportunities to give back Competitive salary and benefitsWorldpay is committed to providing its employees with an exciting career opportunity and competitive compensation. The pay range for this full-time position is $77,100.00 - $125,680.00 and reflects the minimum and maximum target for new hire salaries for this position based on the posted role, level, and location. Within the range, actual individual starting pay is determined by additional factors, including job-related skills, experience, and relevant education or training. Any changes in work location will also impact actual individual starting pay. Please consult with your recruiter about the specific salary range for your preferred location during the hiring process.Privacy StatementWorldpay is committed to protecting the privacy and security of all personal information that we process in order to provide services to our clients. For specific information on how Worldpay protects personal information online, please see the Online Privacy Notice.EEOC StatementWorldpay is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, marital status, genetic information, national origin, disability, veteran status, and other protected characteristics. The EEO is the Law poster is available here supplement document available here.For positions located in the US, the following conditions apply. If you are made a conditional offer of employment, you will be required to undergo a drug test. ADA Disclaimer: In developing this job description care was taken to include all competencies needed to successfully perform in this position. However, for Americans with Disabilities Act (ADA) purposes, the essential functions of the job may or may not have been described for purposes of ADA reasonable accommodation. All reasonable accommodation requests will be reviewed and evaluated on a case-by-case basis.Sourcing ModelRecruitment at Worldpay works primarily on a direct sourcing model; a relatively small portion of our hiring is through recruitment agencies. Worldpay does not accept resumes from recruitment agencies which are not on the preferred supplier list and is not responsible for any related fees for resumes submitted to job postings, our employees, or any other part of our company.\n        ",
    "location": "Massachusetts, United States"
  },
  "10": {
    "id": "4178115774",
    "Company": "Chirisa Technology Parks",
    "title": "Data Center Chief Engineer",
    "description": "Job Description: Chief Engineer - Datacenter and Colocation FacilityPosition type: Full-Time ExemptLocation: Chester, VAAbout Chirisa Technology Parks:Chirisa Technology Parks provides customers with managed cloud and colocation infrastructure and services. We focus on delivering the most cost-effective management of distributed workloads across private, public, and managed infrastructures, including colocation. With diverse networks and power infrastructure combined with skilled engineers working 24x7, we deliver highly secure and reliable colocation and managed solutions.Position Summary:As the Chief Engineer, you will be responsible for overseeing the operation, maintenance and troubleshooting of all critical infrastructure systems within the datacenter and colocation facility in Chester, VA, including electrical, mechanical, cooling, and fire safety systems, ensuring maximum uptime and operational efficiency while prioritizing safety and compliance with industry standards; acting as the primary point of contact for all facility-related issues and leading a team of engineers to maintain the facility's critical environment. You will lead a team of technicians and engineers to ensure the continuous operation, maintenance, and improvement of the facility's mechanical infrastructure. This role requires a strong background in mechanical engineering, expertise in datacenter cooling and HVAC systems, and a proactive approach to managing the facility's critical environment.Responsibilities:Actively monitor and maintain critical data center systems like UPS systems, generators, chillers, HVAC units, electrical distribution panels, and fire suppression systems.Diagnose and resolve complex technical issues related to facility equipment, coordinating with vendors for repairs when necessary.Oversee facility upgrade projects, including new equipment installations, system expansions, and infrastructure improvements. Team Leadership: Lead and manage a team of mechanical technicians and engineers, providing guidance, training, and support to ensure efficient and effective facility operations.Preventive Maintenance: Develop and implement a comprehensive preventive maintenance program for all mechanical equipment to minimize downtime and ensure uninterrupted operations.Troubleshooting and Repairs: Rapidly respond to mechanical system failures, conduct root cause analysis, and coordinate repairs to minimize downtime and ensure the facility's reliability.Energy Efficiency: Identify and implement energy-efficient strategies to optimize cooling and HVAC systems, reducing operational costs while maintaining industry-leading performance.Compliance and Safety: Ensure the facility complies with all relevant regulations, safety standards, and industry best practices related to mechanical systems and environmental controls.Project Management: Coordinate and oversee mechanical infrastructure projects, from planning and design to implementation and commissioning.\u00a0Qualifications:Bachelor's degree in electrical, mechanical Engineering or a related field is preferred.Minimum of 5 years of experience in datacenter facility management, with a focus on electrical or mechanical systems.Proven experience in managing and leading teams.In-depth knowledge of datacenter cooling methodologies, HVAC systems, and environmental controls.Strong problem-solving skills and ability to troubleshoot complex mechanical issues.Familiarity with relevant industry standards and regulations (BMS, NFPA, etc.).Excellent communication and interpersonal skills to collaborate with cross-functional teams and external partners.Ability to work under pressure and handle multiple tasks in a fast-paced environment.Job Type: Full-timeSchedule:\u00a08-hour shift, M-F, On-site, In personWe are an equal opportunity employer. All applicants will be considered for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.\n",
    "location": "Chester, VA"
  },
  "11": {
    "id": "4164169385",
    "Company": "WHOOP",
    "title": "Data Engineer I",
    "description": "At WHOOP, we're on a mission to unlock human performance. WHOOP empowers members to perform at a higher level through a deeper understanding of their bodies and daily lives.WHOOP is seeking a dynamic Data Engineer who thrives on innovation and is ready to revolutionize our data operations. In this role, you'll design, build, and optimize scalable data pipelines and platforms that serve as the backbone of our data-driven insights. With a strong focus on crafting robust ETL/ELT processes and managing cutting-edge AWS infrastructure, you'll integrate modern data tools\u2014including Snowflake, DBT, Kafka, and Spark\u2014to elevate our analytical capabilities. If you're excited about harnessing AI to supercharge efficiency and drive breakthrough innovations, we want you to join our forward-thinking team and make a tangible impact on the future of our data ecosystem.ResponsibilitiesWrite and maintain high-quality, reusable code in Python and Pyspark to develop and maintain ELTs and data pipelines.Utilize Kafka and Spark for real-time streaming and batch data processing.Implement and optimize data warehousing solutions using Snowflake.Create and manage transformation models with DBT to ensure consistent data quality and agile analytics.Architect and manage AWS infrastructure (e.g., EC2, S3, Lambda, RDS) to support scalable and secure data processing.Leverage AI tools to automate tasks, optimize workflows, and drive overall efficiency.Collaborate with cross-functional teams (data scientists, analysts, etc.) to understand and meet evolving data needs.Document processes and continuously seek improvements in the data platform.QualificationsBachelor\u2019s degree in Computer Science, Engineering, or a related field; or equivalent practical experience.Experience designing and implementing ETL/ELT processes.Solid understanding of SQL and modern data warehousing concepts.Familiarity with AI tools like Copilot / ChatGPT and their use on driving efficiency in the software development life cycle. Proficiency using DBT for data modeling and transformation is a plus.Hands-on experience with Kafka and Spark for data processing is a plus.Knowledge of containerization, orchestration (e.g., Docker, Kubernetes), and infrastructure-as-code is a plus.Excellent problem-solving skills, strong communication abilities, and the capacity to work collaboratively in an agile environment.This role is based in the WHOOP office located in Boston, MA. The successful candidate must be prepared to relocate if necessary to work out of the Boston, MA office.Interested in the role, but don\u2019t meet every qualification? We encourage you to still apply! At WHOOP, we believe there is much more to a candidate than what is written on paper, and we value character as much as experience. As we continue to build a diverse and inclusive environment, we encourage anyone who is interested in this role to apply.WHOOP is an Equal Opportunity Employer and participates in E-verify to determine employment eligibility. It is unlawful in Massachusetts to require or administer a lie detector test as a condition of employment or continued employment. An employer who violates this law shall be subject to criminal penalties and civil liability.\n",
    "location": "Boston, MA"
  },
  "12": {
    "id": "4187972283",
    "Company": "FreeWheel",
    "title": "Data Engineer 1",
    "description": "FreeWheel, a Comcast company, provides comprehensive ad platforms for publishers, advertisers, and media buyers. Powered by premium video content, robust data, and advanced technology, we\u2019re making it easier for buyers and sellers to transact across all screens, data types, and sales channels. As a global company, we have offices in nine countries and can insert advertisements around the world.Job SummaryWe are seeking a Data Engineer to design, build, and optimize scalable, high-performance data pipelines. The ideal candidate will have expertise in big data processing frameworks, data storage technologies, and monitoring platforms while thriving in a collaborative, fast-moving environment. If you are a problem solver, a mentor, and someone who enjoys pushing the boundaries of data engineering, this is the opportunity for you!Job DescriptionCore Responsibilities:Own the complete software development lifecycle\u2014from design to deployment\u2014of scalable batch and streaming data pipelines to ingest and transform large datasets from various sources. Optimize streaming and batch data processing to ensure performance, scalability, and cost efficiency. Monitor system performance, troubleshoot issues, and implement solutions to ensure high availability and reliability. Maintain and enhance FreeWheel's internal monitoring platform, ensuring comprehensive observability and operational excellence for the entire Streaming Hub and Audience Platform. Collaborate with product managers and stakeholders to understand data requirements and deliver solutions that drive business impact. Effectively communicate technical concepts to non-technical stakeholders. Stay ahead of emerging data engineering technologies and best practices, identifying opportunities to improve processes, tools, and infrastructure. Document data pipelines, processes, and systems to ensure clarity and maintainability. Must be available to work Eastern Standard Time (EST) hours and be open to variable schedules, including nights, weekends, and overtime as needed. Consistent exercise of independent judgment and discretion in matters of significance. Regular, consistent and punctual attendance. Must be able to work nights and weekends, variable schedule(s) as necessary. Other duties and responsibilities as assigned. QualificationsBachelor\u2019s or master\u2019s degree in computer science, Engineering, or a related field. Familiarity with programming languages, including Scala, Java, Python, and SQL. Exposure to big data processing frameworks, such as Apache Flink, Apache Spark (Batch & Streaming), and Databricks Delta Live Tables (DLT). Understanding of data storage technologies, including AWS S3, Delta Lake, Apache Iceberg, Hadoop HDFS, and Apache Druid. Familiarity with Data Lakehouse architectures, data modeling techniques, ETL processes, and relational databases (e.g., MySQL). Basic Knowledge of Data Modeling techniques and Relational databases such as MySQL Exposure to AWS services such as EMR and Lambda and workflow orchestration tools such as Airflow Awareness of NoSQL databases and their pros and cons Strong problem-solving skills with a keen attention to detail. Excellent communication and collaboration skills to work effectively with technical and non-technical teams. Generalist mindset with the ability to work across different technology stacks and polyglot programming skills, demonstrating fluency in multiple programming languages beyond primary expertise. Employees At All Levels Are Expected ToUnderstand our Operating Principles; make them the guidelines for how you do your job.Own the customer experience - think and act in ways that put our customers first, give them seamless digital options at every touchpoint, and make them promoters of our products and services.Know your stuff - be enthusiastic learners, users and advocates of our game-changing technology, products and services, especially our digital tools and experiences.Win as a team - make big things happen by working together and being open to new ideas.Be an active part of the Net Promoter System - a way of working that brings more employee and customer feedback into the company - by joining huddles, making call backs and helping us elevate opportunities to do better for our customers.Drive results and growth.Respect and promote inclusion & diversity.Do what's right for each other, our customers, investors and our communities.Why Join FreeWheel?Cutting-Edge Technology: Work with modern data stacks, cloud platforms, and real-time processing frameworks.Innovation-Driven Culture: Collaborate with top engineers, data scientists, and industry leaders shaping the future of ad tech.Growth & Impact: Solve complex, large-scale data challenges with tens of billions of daily events in a high-impact role.Flexible & Inclusive Work Environment: We value diverse perspectives, work-life balance, and professional development.About FreeWheelFreeWheel, a Comcast company, is a global leader in advanced advertising technology, enabling seamless transactions across screens, data types, and sales channels. Operating in nine countries, FreeWheel powers global ad insertion and recently launched Universal Ads, a cross-publisher TV advertising platform in collaboration with major media players like NBCUniversal, Warner Bros. Discovery, and Roku. Committed to innovation, inclusivity, and professional growth, FreeWheel fosters a dynamic work environment where diverse perspectives thrive while shaping the future of premium video advertising.About The FreeWheel Data TeamThe FreeWheel Data Team is a global, high-impact engineering team focused on building scalable, high-performance, and efficient data platforms. We develop solutions to ingest, transform, organize, and distribute vast volumes of advertising and audience data, enabling a Unified Data Lake and Data Warehouse that powers customer-facing products, machine learning applications, and real-time analytics.We process tens of billions of ad events daily, leveraging a modern data stack that includes Databricks, AWS, Apache Spark, ClickHouse, Snowflake, and Looker. We are looking for talented Data Engineers who thrive on solving complex data challenges, working with cutting-edge technologies, and contributing to a collaborative, innovation-driven team. If you\u2019re passionate about scaling data infrastructure, optimizing pipelines, and extracting insights from massive datasets, this is the opportunity for you!Disclaimer:This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications.Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.SkillsData Engineering; Structured Query Language (SQL); Python (Programming Language)SalaryPrimary Location Pay Range: $71,903.29 - $107,854.93Comcast intends to offer the selected candidate base pay within this range, dependent on job-related, non-discriminatory factors such as experience. The application window is 30 days from the date job is posted, unless the number of applicants requires it to close sooner or later.Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits to eligible employees. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That\u2019s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality \u2013 to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary  on our careers site for more details.EducationBachelor's DegreeWhile possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.Relevant Work Experience0-2 Years\n        ",
    "location": "Chicago, IL"
  },
  "13": {
    "id": "4129675524",
    "Company": "Jackson",
    "title": "Data Engineer I",
    "description": "If you are an internal associate, please login to Workday and apply through Jobs Hub.Job PurposeThe Data Engineer I will be responsible for building and supporting the data platform. The Data Engineer will provide the development and automation of computing processes to detect, and respond to opportunities in business operations. The Data Engineer will work with a variety of disparate datasets that encompass many disciplines and business units. They will strive to transform and implement true business integration, leveraging top-notch data integration best practices. Merging and securing data in a way that reduces the cost to maintain and increases the utilization of enterprise-wide data as an asset and developing business intelligence.Essential ResponsibilitiesEngineers and implements solutions that align to architecture patterns and security guidelines for data, compute and technology platforms, meeting business needs.Works in partnership with business product owners, and DevSecOps to engineer and build mature scalable and robust business capabilities.Follows IT architecture and IT operations guidelines and requirements while implementing optimal engineering solutions for the company.Participates in engineering and planning initiatives related to capabilities, future roadmaps, operations, and strategic planning.Engineers and implements business and technology innovation that drives the organization's top and bottom lines.Stays current with industry trends, making recommendations of new technologies that deliver strategic business value and reduce costs.Engineers and implements architecture solutions that maximize reuse and are efficient, maintainable, scalable, stable, highly available, portable, secure, and implemented correctly.Strong communication competency across all levels of stakeholders, providing engineering guidance and insight into best practices.Follows organizational policies and goals for IT security, change management, and operational risk.Synthesizes information into clear and concise materials with thoughtful attention to detail and quality.Provides engineering consulting services within domain to help achieve desired business and operational outcomes across Jackson.Engineers and implements the service management processes for maintaining the technology platforms in production.Acquires and maintains a working knowledge of Jackson processes and procedures across various departments.Assists in researching, planning, and executing the migration of on-prem data infrastructure to Azure, including evaluating tradeoffs between cost, performance, and maintainability.Develops and maintains data pipelines for complex business use cases.Develops, designs, and builds solutions on a platform dedicated to large-scale processing of various data sets.Collaborates with IT architecture and IT operations to identify and implement the most optimal data engineering solutions for the company.Collaborates with team to execute and iterate on application development, including interfacing withlegacy databases, parsing raw structured and unstructured data, documenting the data warehouse, debugging, and deploying to production environments.Collaborates with other teams to clarify and answer complex business questions using statistical and graphical approaches.Employs exceptional problem-solving skills, with the ability to see and solve issues before applications are moved to production systems.Sets, communicates, and reinforces technical standards.Leads and develops best practices for larger Data Engineer Community of Practice (Data CoP).Stays current with industry trends, makes recommendations of new technologies that deliver strategic business value and reduce costs.Performs other duties and/or projects as assigned.Knowledge, Skills And AbilitiesKnowledge of industry standards, emerging technologies; and security and system best practices.Excellent verbal and written communication skills including presentation creation and delivery.Experience with the Microsoft Cloud ecosystem.Strong organizational skills; ability to independently prioritize tasks and projects to meet deadlines.Strong collaboration skills with the ability to build consensus, influencing across all levels in the organization.Ability to learn and maintain a comprehensive understanding of finance/insurance business and technology.Knowledge of Lean and Agile principles, systems, and tools.Ability to explain and communicate technical concepts clearly.Ability to conduct gap analysis and identify possible solutions for continuous improvements.Basic programming skills in Python, R, Powershell and or Java with experience parsing, manipulating, and converting data to and from a wide range of formats (CSV, json, XML, html, SQL tables, etc.).Good understanding of modern database concepts and SQL syntax, including experience with DB2, MongoDB, SQL Server, CosmosDB, Data Lake, Hadoop, etc.Experience building and optimizing \u2018big data\u2019 data pipelines, architectures and data sets.Strong analytic skills related to working with unstructured datasets.Build processes supporting data transformation, data structures, metadata, dependency and workload management.QualificationsBachelor's Degree in Computer Science/Computer Engineering and/or equivalent experience required.2+ years related IT experience required.1+ years in Data Engineering required.Software development or related position experience preferred.1+ years SQL, relational database experience and unstructured datasets required.1+ years experience Azure Databricks, Azure Data Factory, Python, C# (preferred) Java, SDLC, Terraform, Spark, Config Management and Monitoring required.1+ years experience using Agile methodologies, data streaming, data as a service (REST APIs), CI/CD pipelines, Parquet, JSON preferred.1+ years experience working with Jupyter notebooks, microservices and data modeling preferred.1+ years IT infrastructure/operations role preferred.Certification in Azure Fundamentals & Data Engineer upon hire preferred.We don't just accept difference - we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Jackson is proud to be an equal opportunity workplace. The Company subscribes to and endorses federal and state laws and regulations relating to equal employment opportunity for all persons without regard to race, color, religion, gender, age, national origin, legally-recognized disability, marital status, legally-protected medical condition, citizenship, ancestry, height, weight, sexual orientation, veteran status, or any other factor not related to the needs of the job. The Company is committed to a policy of equal opportunity. Company facilities and campuses are tobacco-free environments.\n        ",
    "location": "Nashville, TN"
  },
  "14": {
    "id": "4169804619",
    "Company": "Lenovo",
    "title": "Associate Data Engineer - Cloud Software Talent Program",
    "description": "We are Lenovo. We do what we say. We own what we do. We WOW our customers.Lenovo is a US$57 billion revenue global technology powerhouse, ranked #248 in the Fortune Global 500, and serving millions of customers every day in 180 markets. Focused on a bold vision to deliver Smarter Technology for All, Lenovo has built on its success as the world\u2019s largest PC company with a full-stack portfolio of AI-enabled, AI-ready, and AI-optimized devices (PCs, workstations, smartphones, tablets), infrastructure (server, storage, edge, high performance computing and software defined infrastructure), software, solutions, and services. Lenovo\u2019s continued investment in world-changing innovation is building a more equitable, trustworthy, and smarter future for everyone, everywhere. Lenovo is listed on the Hong Kong stock exchange under Lenovo Group Limited (HKSE: 992) (ADR: LNVGY).This transformation together with Lenovo\u2019s world-changing innovation is building a more inclusive, trustworthy, and smarter future for everyone, everywhere. To find out more visit www.lenovo.com, and read about the latest news via our StoryHub.DescriptionThis position is for an Associate Data Engineer in the Cloud Software team, part of the Advanced Innovation Center. This role will work closely with Engineering, Security, DevOps, QA, and Architecture teams to develop highly scalable and robust data pipelines in a cloud-based SaaS environment. To be successful in this role, you are expected to be highly hands-on, have knowledge of basic software engineering concepts, and be motivated to learn new technologies.ResponsibilitiesWork with the engineering team to develop and test data pipelines to ingest, extract, transform, store, serve and build data sets used by AI applications, Data Scientists and ML Engineers.Apply your knowledge of algorithms, pipelines, AI ML, data processing, and supporting tools and technologies to develop new data solutions.Review pull requests from your peers to ensure high standards, consistency, and durability of the implementation.Basic QualificationsRecent graduate with a Bachelors or Masters Degree in Engineering, Computer Science or technical related disciplineEffective communication, presentation, and strong interpersonal skillsBasic experience with public Cloud services (AWS, Azure, Google Cloud Platform)Solid understanding of object-oriented design principles and patterns, data structures, software engineering, distributed systems, and database systemsExperience with open-source compute engines (Apache Spark, Apache Flink, Trino/Presto, or equivalent)Experience with GitPreferred QualificationsPrevious internship experience in software development companies in the Data Engineering field6+ months of Data Engineering experienceExperience or familiarity with working in an Agile environmentExperience in container technology like Docker and KubernetesExperience with open-source table formats (Apache Iceberg, Delta, Hudi or equivalent)We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, religion, sexual orientation, gender identity, national origin, status as a veteran, and basis of disability or any federal, state, or local protected class.\n",
    "location": "Morrisville, NC"
  },
  "15": {
    "id": "4183169957",
    "Company": "CCC Intelligent Solutions",
    "title": "Data Engineer Intern Summer 2025",
    "description": "Pay Rate/Hourly Range:The hourly range is $20.00 - $43.00 per hour.Pay is based on factors including school year, program of study, role responsibilities, etc.CCC Intelligent Solutions Inc. (CCC) is a leading cloud platform for the multi-trillion-dollar insurance economy, creating intelligent experiences for insurers, repairers, automakers, part suppliers, and more. At CCC, we\u2019re making life just work by empowering more than 35,000 businesses with industry-leading technology to get drivers back on the road and to health quickly and seamlessly. We\u2019re pushing boundaries with innovative AI solutions that simplify and enhance the claims and repair journey. Through purposeful innovation and the strength of its connections, CCC technologies empower the people and industry relied upon to keep lives moving forward when it matters most. Learn more about CCC at www.cccis.com.The RoleWe're looking for a Data Engineer to join us for the Summer 2025 semester. At CCC, you will work and learn alongside innovative and inspiring leaders and gain valuable experience working on real business solutions in a corporate setting.Ideal candidate will be located in Chicago area.Key Responsibilities:Build streaming and batch data pipelines using Python, Kafka, Spark, Cloud (and more) which will provide insights to CCC's clients within auto property damage and repair, medical claims, and telematics IoT industries.Build and maintain complex SQL queries using Hive, Oracle, and SQL Server. Turn raw data into valuable insights using AI techniques.Work closely with product owners, information engineers, data scientists, data modelers, infrastructure support, and data governance positions.Requirements:If you have a Masters, and/or PhD, and have demonstrated excellence in your academic studies, this opportunity is right for you. We are looking for candidates who offer strong collaborative skills and work well with a team. A strong interest in computer science and/or related fields is essential to the success of each intern.If you have knowledge, through coursework or project work, of Python, Machine Learning, Deep Learning, Mathematics, or Statistics/ Modeling, this opportunity is right for youAbout CCC's Commitment to Employees:CCC Intelligent Solutions understands that our employees play an integral role in our vision to shape a world where life just works. Our team is defined by our values of Integrity, Customer-Focus, Innovation, Inclusion & Diversity, Tenacity, and Connection. Through diverse perspectives, purposeful innovation, and the strength of connections, our technologies empower the people and industry relied upon to keep lives moving forward when it matters most.At CCC, together everyone can thrive as we innovate and collaborate, creating employee experiences that just work. We are committed to providing opportunities for our people to make real-life impacts, advance in their careers, and contribute to CCC\u2019s success.CCC offers competitive compensation and benefits to support you and your families, including:401K MatchPaid time offAnnual Incentive Plan Performance BonusComprehensive health insuranceAdoption AssistanceTuition ReimbursementWellness ProgramsStock Purchase Plan optionsEmployee Resource GroupsFor more information about our benefits, please check out our careers site.Here, you belong. You are seen, valued, and respected. We celebrate you for who you are and all you bring. Every voice is heard and is important to our success. You can hear what employees have to say about our culture hereIf you require reasonable accommodation to complete a job application, pre-employment testing, or a job interview or to otherwise participate in the hiring process, please contact (800) 621-8070.\n        ",
    "location": "Chicago, IL"
  },
  "16": {
    "id": "4118833997",
    "Company": "i360",
    "title": "Data Engineer Co-Op",
    "description": "Your JobI360 is the leading data and technology provider for the pro-free market political and advocacy community, supporting organizations that promote free enterprise and smaller government. I360 is seeking a Data Engineer Co-Op to join its data operations team.This opportunity is ideal for you if you are looking for:Practical experience working on real-world data engineering projectsGuidance and mentorship from seasoned professionalsThe chance to contribute to meaningful projects that make an impactA collaborative and supportive team environmentWhat You Will DoAssist in the development and maintenance of SQL and NoSQL databases.Collaborate with the data engineering team to design and implement data pipelines.Assist in determining business requirements and planning and deploying test systems.Interface with other teams to resolve issues related to data integrity and consistency. QA data that is imported by various processes.Investigate, analyze, correct and document reported data defects.Who You Are (Basic Qualifications)Pursuing a Master's Degree in Computer Science, Computer Engineering, Statistics, or a related fieldExperience using data integrations tools and ETL processes Experience with SQL/NoSQL databases such as PostgresSQL, Elastic Search, Snowflake, Redshift Experience with programming languages such as Python, Java or Scala At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate's knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.Hiring PhilosophyAll Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here.Who We AreAs a Koch company, i360 works across industry lines to bring unique data, technology and analytics solutions to help our customers win. Our analysts and data scientists build and refine proven, sophisticated models that help customers understand and reach their target audience, visualize data and expand their reach.At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.Our BenefitsOur goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength - focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes - medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.Additionally, everyone has individual work and personal needs. We seek to enable the best work environment that helps you and the business work together to produce superior results.Equal OpportunitiesEqual Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, some offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please click here for additional information.\n        ",
    "location": "Arlington, VA"
  },
  "17": {
    "id": "4133554461",
    "Company": "BayOne Solutions",
    "title": "Data Engineer",
    "description": "Job DescriptionOnly W2This candidate is responsible for designing and developing in-house Security Information and Event Management (SIEM), Security Orchestration, Automation, and Response (SOAR), and investigative tools, with a primary focus on building robust, scalable, and automated security solutions.In this role, you will be responsible for the end-to-end design, development, testing, deployment, and maintenance of key components within our large-scale data infrastructure.This infrastructure is essential for advancing automation, enabling data-driven insights, and enhancing enterprise-wide security measures.ResponsibilitiesDevelop and deploy scalable, production-ready software to drive automation and enable data-driven decision-making within the Enterprise Cybersecurity team, prioritizing resilience, performance, and security.Provide engineering support for the Enterprise Cybersecurity Operations team, with a focus on creating automated solutions that reduce manual intervention and operational overhead.Design and build event-driven, scalable systems to deliver timely alerts and automations, enhancing responsiveness and support for both the Cybersecurity team and external stakeholders.Integrate new data sources into our data lake to enhance visibility and extend security coverage across the environment, focusing on building reusable, automated data ingestion processes.Provide development and migration support for the integration of new tools and technologies, ensuring seamless onboarding within the Enterprise Cybersecurity environment.QualificationsExtensive programming experience in object-oriented languages (e.g., Python, Go, Java) and SQL, with a proven track record in designing maintainable, scalable, and efficient solutions.Robust expertise in the following areas: distributed data processing, data engineering for high-volume data services, or developing scalable data streaming platforms for real-time analytics.Advanced proficiency in cloud and data infrastructure technologies (e.g., AWS, Databricks, Terraform, Apache Spark, Docker) with deep knowledge of development best practices, CI/CD pipelines, and cloud-native deployment.\n",
    "location": "Palo Alto, CA"
  },
  "18": {
    "id": "4181443367",
    "Company": "Galaxy",
    "title": "2025 Data Engineer Intern - NYC",
    "description": "Who We AreAt Galaxy we are building products and services to help the world invest in economic progress. We believe crypto and blockchain innovations will permeate and improve all aspects of our global economy. Our vision is a society where value and ownership flow as freely as information. Galaxy is a digital asset and blockchain leader helping institutions, startups, and individuals access and navigate the crypto economy. As one of the most well-capitalized and trusted companies in the industry, we provide platform solutions custom-made for a digitally native ecosystem across three complementary operating businesses: Global Markets, Asset Management, and Digital Infrastructure Solutions. Our offerings include, amongst others, trading, lending, strategic advisory services, institutional-grade investment solutions across passive, active and venture strategies, proprietary bitcoin mining and hosting services, network validator services, and the development of enterprise custodial technology. Galaxy\u2019s CEO and Founder Michael Novogratz leads a team of crypto enthusiasts, and institutional veterans focused on the future of finance and Web3. The Company is headquartered in New York City, with global offices across North America, Europe and Asia.Additional information about the Company's businesses and products is available on www.galaxy.com.What We ValueWe are a diverse team of free thinkers, and fast movers united to help investors and creators energize the global economy. We are looking for individuals who thrive in a culture of builders and overachievers and embrace high performance, transparent feedback, and a mission-first approach. Our culture shapes our way of working and gets us where we want to be.Seek Excellence.Be Selective To Be Effective.Be Highly Aligned, Loosely Coupled.Disagree Transparently.Encourage Independent Decision-Making.Build Dream Teams.About The Galaxy Summer Internship ProgramThe Galaxy NYC Summer Internship Program is an immersive and exciting, nine-week experience taking place on-site in our New York HQ office. During the nine weeks, you will have the opportunity to gain hands-on, practical work experience on our Transformation/Change teams. In addition to learning on the job, Galaxy interns participate in professional development programming, fun social events, and networking opportunities with senior leaders.Program Dates: June 9th \u2013 August 8th, 2025What You\u2019ll DoWork across all of Galaxy's diverse business lines (i.e., Trading, Custody, Asset Management, Mining) to improve and consolidate the company's data backboneWork on production support issues and be involved in improvements on CI/CD automation and DevOpsWhat We\u2019re Looking For Currently pursuing a degree in Computer Science, Information Technology, Software Engineering, or a related field. Knowledge of the following is an advantage: Java, Python, Go, Terraform Kafka, AWS SQS, or MQ Relational and non-relational Databases (Postgres, SQL Server, Databricks) Time series databases Spark FastAPI, Pydantic Spring Boot, Vertx Jenkins AWS Caching Strategies and Implementations In-Memory Data Management Experience with DockerStrong analytical and problem-solving skillsEagerness to learn new technologies and adapt to fast-paced environments.Eligibility Requirements Undergraduate or master\u2019s students graduating between December 2025 and June 2026 Eligible master\u2019s students must have fewer than two years of relevant, full-time work experience Students enrolled in PhD, MBA, or JD programs are not eligible to apply.Interns are classified as non-exempt employees and will be paid a rate of $55/hr. Here are some of the industry-leading benefits of interning at Galaxy:Opportunities to learn about the Crypto industryFree daily snacks and weekly lunchesSmart, entrepreneurial and fun colleagues Employee Resource GroupsBenefits may vary based on location.Galaxy respects diversity and seeks to provide equal employment opportunities to all employees and job applicants for employment without regard to actual or perceived age, race, color, creed, religion, sex or gender (including pregnancy, childbirth, lactation and related medical conditions), gender identity or gender expression (including transgender status), sexual orientation, marital or partnership or caregiver status, ancestry, national origin, citizenship status, disability, military or veteran status, protected medical condition as defined by applicable state or local law, genetic information or predisposing genetic characteristic, or other characteristic protected by applicable federal, state, or local laws and ordinances.We will endeavor to make a reasonable accommodation to the known limitations of a qualified applicant with a disability unless the accommodation would impose an undue hardship on the operation of our business. If you believe you require such assistance to complete the application process or to participate in an interview, please contact careers@galaxy.com.\n",
    "location": "New York, United States"
  },
  "19": {
    "id": "4157830638",
    "Company": "U.S. Xpress, Inc.",
    "title": "Data Engineer Intern",
    "description": "CHATTANOOGA, TNPosition open to remote: YesGrade: HR04Compensation Range: 15.00- 18.00Who We AreRelentlessly Delivering Big Ideas. U.S. Xpress is one of the nation\u2019s largest asset-based trucking companies. But the most valuable asset we offer isn\u2019t tractors, trailers, or even our exclusive, cutting-edge technology. It\u2019s the collective brainpower of thousands of visionaries and problem-solvers. Together, we are revolutionizing the transportation industry by providing innovative, custom solutions. And, here, we believe in the sanctity of a promise\u2014both to our customers, and our people. When we focus our varied talents on reshaping the future of transportation, that\u2019s what we call the POWER OF  U.S.  Why U.S. Xpress? Right Role. Right Tools. Right People. We invest in our talent starting on day one. You will be provided with personal and professional development opportunities that complement your interests and encourage you to build a career you\u2019re passionate about. Whether it is employee stock options, profit-sharing, 401K, professional development, or our competitive pay, we help prepare you for the future. Be part of an organization that values out-of-the-box thinking and rewards employees for going above and beyond. Curious about the other benefits of working with us? Check out other perks below!Medical, Dental, and VisionBasic/Supplemental LifeAccidental Death/DismembermentHealth Savings AccountsFlexible Savings AccountsCompany Paid HolidaysPaid Time Off401k with Employer Matching ContributionEmployee Stock Purchase PlanPaid Parental LeaveShort Term Incentive ProgramEmployee Assistance ProgramPet InsurancePrimary Position PurposeThe Data Engineer Intern will be part of a team responsible for owning, developing, and maintaining our Enterprise Analytics Applications specifically within our cloud environment.This position will focus heavily on the modeling & transforming of our structured & semi-structured date in the cloud that provided scalable, high-performance views/cubes for our BI Developers & field analysts to consume. This position will be working with a group of peers in an agile scrum environment dedicated to providing pipelines & data to a domain of the business.This role will work very closely with our BI Developers, Data Science & end stakeholders to build robust tools & technology to support the business. You will also work your way to becoming a subject matter expert in our stack of development tools that supports our products we produce.Position Functions Actively developing productivity skills through continuous learning and refining working style and time management. Engage in a team environment and complete well-defined tasks to support team goals. Ability to multi-task and analyze information to determine appropriate response. Display ownership in work from planning/development through completion. Develop & support Engineering pipelines. Operate and develop in an Agile team environment (including backlog refinement & user story creation & ownership). Develop expertise and understanding of core business functions of assigned domain. Work collaboratively across the team and demonstrate the ability to multi-task. Ability to explain products to both technical and non-technical audiences. Ensure opened tickets are resolved within a timely fashion. Strong collaborative mindset, good judgment with great interpersonal skills required to help solve complex business problems. Partner with data owners to design solutions that align to data governance and data management principles best practices. Develop robust, scalable solutions for collecting & analyzing large data sets. Must be proficient in creating & maintaining data pipelines. Design logical data models and their physical schema design. Collaborate in-person with all levels of the organization to harness collective intelligence of the workforce. Attend all in-person meetings and trainings on recent developments, goals, and objectives. Participate in such discussions by offering input and advice. Regularly engage remotely and in-person with customers and potential customers to establish rapport, open communication, and expectations.What We\u2019re Looking ForEducation High school diploma or equivalent required Currently pursuing a Bachelor's or Master\u2019s degree in Computer Science, Information Systems, Analytics, Supply Chain/Logistics or related discipline from a four-year college or universityExperience Experience with ETL tools (such as dbt, FiveTran, AWS/Azure products)  Experience with Snowflake as a Data Warehouse  Some experience programming in Java & Python. Big data knowledge is a plus  Experience with Git and/or bitbucket is a plus  Cloud experience (AWS/Azure/Google)  Solid SQL experience  Experience contributing to a Data Dictionary, Canonical Data Model, or Code/Product Documentation  Transportation, Logistics, and/or Tech industry experience a plusSkills & Abilities Proficiency in developing packages, stored procedures, functions, triggers, and complex SQL statements Ability to work with multiple data sources and types (structured/semi-structured/unstructured) Data Visualization with PowerBI / Tableau / QlikView or equivalent is a plus Ability to work in a fast-paced, agile and dynamic environment with both virtual and face-to-face interactions Light knowledge of SQL DBA work or Snowflake DBA work Supply Chain or Logistics knowledge (Transportation a plus) Solid in MS Office Products Adept at assessing organizational dynamics and managing change Works quickly and efficiently. Able to test solutions, learn, and iterate quickly Proactive and pragmatic problem solver Communicates effectively across multiple mediums Licenses & Certifications  Microsoft SQL certifications a plus Snowflake certifications a plus Dbt certified a plus AWS Stack certifications/trainings a plus FiveTran/HVR certified a plus Supervisory Responsibility  N/AWork Environment / Physical Requirements \u2013 Normal Office Settings.This job description indicates the general nature and level of work expected for this position. It is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities. Employees performing this job may be asked to perform other duties as required and the responsibilities of the position may change. Reasonable accommodations will be made to enable individuals with disabilities to perform the essential functions.U.S. Xpress is an Equal Opportunity Employer committed to creating and maintaining a diverse workforce.This role will remain open until it has been filled.\n",
    "location": "Chattanooga, TN"
  },
  "20": {
    "id": "4143045123",
    "Company": "National Audubon Society",
    "title": "Junior Data Engineer",
    "description": "Position SummaryIn pursuit of Audubon's conservation mission, the Data & Technology team partners with programmatic and business teams across the organization to develop and deploy enterprise data systems and data-driven practices at the scale needed to achieve our ambitious goals. The Junior Data Engineer will play a key role in helping maintain and support software applications and data solutions within a modern-day data stack, using enterprise scale data resources to ensure that high quality data is available across the organization to support conservation and operational objectives and advance our mission for hemispheric level bird conservation.A successful candidate will be familiar with a number of data engineering tools and methods, as well as exhibiting strong communication and time management skills. This team member will work closely with other software and data engineers and report to the Director, Data Engineering.This is a hybrid position based in Audubon\u2019s New York City or Washington, DC offices. We will also consider remote candidates within the United States.Working closely with other Data team members and project stakeholders, the Junior Data Engineer will contribute to projects such as:Strengthening an enterprise data architecture and strategy that includes ELT, systems integration, governance, cataloging and quality controlSupporting and improving operational data processes to ensure data quality and reduce technical debt in critical functional business areas such as finance and fundraisingImproving inclusive outreach, membership growth and advocacy engagement among prospective and current Audubon members by bringing together disparate data sets from CRM systems, census, demographic, social media and web analytics data sourcesThis position is funded through December 2026.CompensationSalary range based on geo-differentials:$25.00 - $30.00 / hour = National$30.00 - $35.00 / hour = Alaska, CA (not San Francisco), Connecticut, D.C., Chicago, Oyster Bay, NY$35.00 - $40.00 / hour = NYC (not Oyster Bay), San Francisco, Seattle Additional Job DescriptionEssential FunctionsSupport and maintain data workflows and business applications critical to Audubon\u2019s operational success, leveraging enterprise data architecture tools and applications built in legacy tools and frameworks (such as .NET).Assist in modernizing data integrations between various data systems using a light-weight Python orchestration environment (Apache Airflow).Work closely with the Senior Coordinator of Data & Technology to help grow the maintainability of the technical stack by documenting support processes and systems interactions, as well as to helping to prioritize support needs and communicate solutions to stakeholders.Collaborate with various members of the organization to perform ad-hoc data explorations and analysis to troubleshoot data issues and clarify business requirements.Administer secure access to the data, including implementing data governance practices, user permissioning for systems and data sources, data hiding strategies such as row-level permissions and data masking.Contribute to the development and implementation of risk-mitigation efforts, including a disaster recovery program and job health monitoring.Contribute to initiatives that improve Audubon's data governance, documentation, and data quality standards. Implements that guidance in practice with methodology documentation, metadata, code comments, etc.Convey willingness and customer orientation to train users of varying backgrounds and skillsets to utilize and partner in improving data engineering processes and practices.Collaborate with Audubon staff to ensure that equity, diversity, inclusion and belonging principles are incorporated and followed in all aspects of our work.Travel two or more times a year to attend in person meetings and events.Perform other job-related duties as assigned.Qualifications And Experience0-2 years of experience as a Data Engineer, or in similar roles such as Software Engineer, Data Analyst, or Database Administrator.Bachelor\u2019s Degree in Computer Science, Information Technology, Statistics, Data Science, Mathematics, or related quantitative field preferred. An equivalent combination of education and work experience will also be considered.Proficiency in at least one high-level programming language such as Python, Java, or .NET.Strong SQL coding ability.Familiarity with software version control, such as Git.Experience utilizing modern data infrastructure tools such as cloud data warehouses (Snowflake, BigQuery, etc.), data integration tools (dbt, Fivetran, Airflow, etc.), business intelligence tools (PowerBI, Sigma, etc.), CRM systems (Salesforce, Everyaction, etc.), and other enterprise data tools (such as Airtable) required.Demonstrated experience applying technical coordination and project management best practices using tools such as Asana and Jira required. Demonstrated ability to communicate technical information to non-technical audiences.Ability to collaborate with colleagues with transparency, inclusivity and trust.Self-starter who can work as part of a virtual team and remain motivated in a dynamic environment.Curiosity to stay on the cusp of software and product trends in non-profits and the greater tech industry.Commitment to Audubon\u2019s organizational values of care, collaboration, change, integrity, impact, and innovation.Demonstrated personal and professional commitment to and experience in advancing equity, diversity, inclusion, and belonging.Genuine passion for conservation and the mission of the National Audubon Society.Ability to travel at least two times a year.This position is represented by the Communication Workers of America (CWA).National Audubon Society Competencies: This role will also be accountable to apply and develop the following competencies. Building Relationships: Establish and nurture meaningful connections and trust with others while fostering an environment of inclusivity and respect.Problem Solving: Find effective solutions to challenges and support decision-making by drawing on critical thinking, creativity, and systematic approaches.Accountability: Be reliable and trustworthy in fulfilling commitments while recognizing inequities that may impact the ability of others to fulfill responsibilities effectively.Supporting Change: Adapt to changes and modify behavior in response to new information or unexpected obstacles while considering the diverse needs of others.Critical Thinking: Take an active approach to analyzing, evaluating, and interpreting information or situations objectively and logically to make informed decisions or judgments.\n        ",
    "location": "New York, NY"
  },
  "21": {
    "id": "4187024643",
    "Company": "Techstand LLC",
    "title": "Data Engineer",
    "description": "We are looking for a suitable candidate with 12+ yrs of developing complex database systems along with 8 yrs of Databricks, ElasticSearch, Kibana, Oracle, Python/Scala experience and 5 yrs of ETL,AWS experience.The position is responsible for providing ongoing maintenance and support of the Michigan Disease Surveillance System (MDSS). MDSS is a complex application that supports communicable disease surveillance, registries, and case management systems that are critical to supporting effective responses to public health emergencies and reducing the burden of communicable diseases. MDSS is going through modernization to enhance stability and functionality of the system. With phase 1 already completed. The resource is integral to developing and maintaining and enhancing MDHHS\u2019 MDSS phase 1. Making sure automated processes are funtioning, streamlining critical business processes, data integrity, SEM/SUITE compliance, and securing the application. The resource also performs as a technical lead and provides technical guidance to the other developers in the department. As a technical lead, the resource participates in a variety of analytical assignments that provide for the enhancement, integration, maintenance, and implementation of projects. The resource will also provides technical oversight to other developers in the team that support other critical applications . Not having a resource on staff will lead to MDHSS failing to maintain, enhance, and support the modernized MDSS that can lead to errors causing application outages, data integrity issues and can eventually lead to incorrect information being processed and reporting of the patient information.\u2022 Lead the design and development of scalable and high-performance solutions using AWS services.\u2022 Experience with Databricks, Elastic search, Kibana, S3.\u2022 Experience with Extract, Transform, and Load (ETL) processes and data pipelines.\u2022 Write clean, maintainable, and efficient code in Python/Scala.\u2022 Experience with AWS Cloud-based Application Development\u2022 Experience in Electronic Health Records (EHR) HL7 solutions.\u2022 Implement and manage Elastic Search engine for efficient data retrieval and analysis.\u2022 Experience with data warehousing, data visualization Tools, data integrity\u2022 Execute full software development life cycle (SDLC) including experience in gathering requirements and writing functional/technical specifications for complex projects.\u2022 Excellent knowledge in designing both logical and physical database model\u2022 Develop database objects including stored procedures, functions,\u2022 Extensive knowledge on source control tools such as GIT\u2022 Develop software design documents and work with stakeholders for review and approval.\u2022 Exposure to flowcharts, screen layouts and documentation to ensure logical flow of the system requirements\u2022 Experience working on large agile projects.\u2022 Experience or Knowledge on creating CI/CD pipelines using Azure Devops\n",
    "location": "Lansing, MI"
  },
  "22": {
    "id": "4138869724",
    "Company": "Federal Home Loan Bank of Cincinnati",
    "title": "Data Engineer Internship",
    "description": "General SummaryA member of the Data & Analytics Agile scrum team who will assist in the design, development, and testing of Enterprise Data Warehouse.Principal Duties And ResponsibilitiesAssists in following standard development and enhancement process of ETL pipelines using Azure Data FactoryDocument data engineering processes, as needed.Investigate data related issues by executing SQL queries.Minimum Knowledge, Skills And Abilities RequiredTheoretical knowledge of Azure Data Factory such as Datasets, Pipelines, Activities, Tasks, Data Flows. Hands on experience is preferred.Understanding and writing SQL queries/viewsExperience working in Python and Snowflake preferredConceptual understanding of Data Warehouse design and modelingDesire to learn and teach data engineering techniques in a collaborative environment on an Agile scrum team.Effective verbal and written communication skills.Interpersonal skills necessary to interact effectively with FHLBank personnel.A financial aptitude is helpful to understand the business processes.Applicants must be currently authorized to work in the United States on a full-time basis as our company is not supporting sponsorship for the intern role.Working ConditionsNormal office environment with flexibility.\n        ",
    "location": "Cincinnati, OH"
  },
  "23": {
    "id": "4182807175",
    "Company": "Sayari",
    "title": "Data Engineer Intern - Web Crawling",
    "description": "About Sayari: Sayari is the counterparty and supply chain risk intelligence provider trusted by government agencies, multinational corporations, and financial institutions. Its intuitive network analysis platform surfaces hidden risk through integrated corporate ownership, supply chain, trade transaction and risk intelligence data from over 250 jurisdictions. Sayari is headquartered in Washington, D.C., and its solutions are used by thousands of frontline analysts in over 35 countries.Our company culture is defined by a dedication to our mission of using open data to enhance visibility into global commercial and financial networks, a passion for finding novel approaches to complex problems, and an understanding that diverse perspectives create optimal outcomes. We embrace cross-team collaboration, encourage training and learning opportunities, and reward initiative and innovation. If you like working with supportive, high-performing, and curious teams, Sayari is the place for you.Internship Description:Sayari is looking for a Data Engineer Intern specializing in web crawling to join its Data Engineering team! Sayari has developed a robust web crawling project that collects hundreds of millions of documents every year from a diverse set of sources around the world. These documents serve as source records for Sayari\u2019s flagship graph product, which is a global network of corporate and trade entities and relationships. As a member of Sayari's data team your primary objective will be to work on maintaining and improving Sayari\u2019s web crawling framework, with an emphasis on scalability and reliability. You will work with our Product and Software Engineering teams to ensure our crawling deployment meets product requirements and integrates efficiently with our ETL pipelines.This is a remote paid internship with work expectations being between 20-30 hours a week.Job Responsibilities:Investigate and implement web crawlers for new sourcesMaintain and improve existing crawling infrastructureImprove metrics and reporting for web crawlingHelp improve and maintain ETL processesContribute to development and design of Sayari\u2019s data productRequired Skills & Experience:Experience with PythonExperience managing web crawling at scale, any framework, Scrapy is a plusExperience working with KubernetesExperience working collaboratively with gitExperience working with selectors such as: XPath, CSS, JMESPathExperience with WebDev tools (Chrome/Firefox)Desired Skills & Experience:Experience with Apache projects such as Spark, Avro, Nifi, and AirflowExperience with datastores Postgres and/or RocksDBExperience working on a cloud platform like GCP, AWS, or AzureWorking knowledge of API frameworks, primarily RESTUnderstanding of or interest in knowledge graphsExperience with *nix environmentsExperience with reverse engineeringProficient in bypassing anti-crawling techniquesExperience with Javascript\n",
    "location": "United States"
  },
  "24": {
    "id": "4184921256",
    "Company": "Techstars",
    "title": "Associate Data Engineer",
    "description": "Location: Denver, Colorado - hybridAssociate Data EngineerAs an Associate Data Engineer at Techstars you will be part of a high performing team building innovative software solutions for entrepreneurs worldwide. In this role, you will build backend streaming data pipelines, integrations with third party SaaS applications, complex analytics features and app facing platform data APIs. You will take part in architecture and solution design and help optimize solution performance and reliability. As a result, you will help Techstars serve founders, entrepreneurs, and startup communities better than ever before.Responsibilities And DutiesWork with engineers to implement and maintain data models and pipelinesBuild and deploy high quality data analytics, integrations, and API solutionsEnsure that all data solutions are secure, performant, reliable, observable, and testableContribute in a team environment with a collaborative, open-minded attitudeContinuously improve the quality of the products and solutions deliveredCreate and update technical documentation for data systems and processesMonitor pipelines and ETL processes to ensure data accuracy and completenessTroubleshoot and resolve data-related issues in a timely mannerQualifications And SkillsA passion for building data solutions, and an appreciation for dev/opsExperience working with distributed systems and cloud infrastructureData engineering experience with ETL, streaming, data pipelines, cleansing and masteryExperience with programming and query languages (ie. SQL, Python)Familiarity working with relational database systems (ie. Postgres)Capable of navigating and using command-line interfacesExperience working in an agile development environmentExcellent communication and collaboration skills, a desire to learnAbility to improvise, adapt, and work to deliver on commitments and address issuesExperience with financial/investment data is a plusExperience developing with containerization technologies (Docker, Kubernetes etc.) preferredCompensation: $65,000 - $75,000 with 10% bonusColorado Residents: In any materials you submit, you may redact or remove age-identifying information such as age, date of birth, or dates of school attendance or graduation. You will not be penalized for redacting or removing this information.About TechstarsTechstars is the most active pre-seed investor in the world having invested through its accelerators in more than 3,700 companies. Founded in 2006, Techstars believes that entrepreneurs create a better future for everyone and great ideas can come from anywhere. Now we are on a mission to invest in an unprecedented number of startups per year enabling more capital to flow to more entrepreneurs around the world. We do this by operating accelerator programs and venture capital funds, as well as by connecting startups, investors, corporations, and cities to help build thriving startup communities. www.techstars.comTechstars is an affirmative action, equal opportunity employer and does not discriminate on the basis of race, sex, age, national origin, religion, physical or mental handicaps or disabilities, marital status, Veteran status, sexual orientation, gender identity nor any other basis prohibited by law.AboutTechstars uses E-Verify to check the work authorization of all new hires. For more information about E-Verify, please see the following: E-Verify Participation Poster (English and Spanish)  Right To Work Poster (English and Spanish)\n",
    "location": "Denver, CO"
  },
  "25": {
    "id": "4190014932",
    "Company": "Worldpay",
    "title": "Data Engineer I",
    "description": "Job DescriptionWe are Worldpay. Our technology powers the world\u2019s economy and our teams bring innovation to life. We champion diversity to deliver the best products and solutions for our colleagues, clients and communities. If you\u2019re ready to start learning, growing and making an impact with a career in fintech, we\u2019d like to know: Are you Worldpay?About The RoleWe are seeking talented and motivated Data Engineers to join our dynamic team. The ideal candidate will have a strong background in data engineering, with experience in various technologies and tools. This role involves designing, developing, and maintaining data pipelines, ensuring data quality and integrity, and supporting data-driven decision-making processes.ResponsibilitiesDesign, develop, and maintain scalable data pipelines and ETL processes.Collaborate with data scientists, analysts, and other stakeholders to understand data requirements.Implement data architecture and data modeling solutions.Ensure data security and compliance with industry standards.Optimize and troubleshoot data workflows and processes.Develop and maintain documentation for data processes and systems.Required SkillsProficiency in SQL for querying and managing relational databases.Experience in designing and structuring data systems (Data Architecture).Knowledge of Pyspark for big data processing and analytics.Familiarity with Snowpark for building data pipelines and processing data within Snowflake.Ability to automate data workflows (Data Pipelines).Experience in writing and optimizing stored procedures.Proficiency in using Databricks for big data analytics and machine learning.Expertise in Snowflake for data warehousing and analytics.Knowledge of AWS services (S3, Lambda, IAM, Boto3) for data storage, processing, and security.Understanding of secure methods for transferring data files (Secure File Transfer Processes).Additional SkillsAbility to design and document process flows.Experience with PGP encryption for securing data.Knowledge of Terraform for infrastructure as code.Proficiency in Tableau for data visualization and reporting.Experience with Power BI for creating interactive dashboards and reports.Familiarity with Google Cloud Platform (GCP) services.Understanding of data clean rooms for secure data collaboration.Knowledge of data sharing techniques.Basic understanding of machine learning algorithms.What We Offer YouA career at Worldpay is more than just a job. It\u2019s the change to shape the future of fintech. At Worldpay, we offer you: A voice in the future of fintech Always-on learning and development Collaborative work environment Opportunities to give back Competitive salary and benefitsWorldpay is committed to providing its employees with an exciting career opportunity and competitive compensation. The pay range for this full-time position is $77,100.00 - $125,680.00 and reflects the minimum and maximum target for new hire salaries for this position based on the posted role, level, and location. Within the range, actual individual starting pay is determined by additional factors, including job-related skills, experience, and relevant education or training. Any changes in work location will also impact actual individual starting pay. Please consult with your recruiter about the specific salary range for your preferred location during the hiring process.Privacy StatementWorldpay is committed to protecting the privacy and security of all personal information that we process in order to provide services to our clients. For specific information on how Worldpay protects personal information online, please see the Online Privacy Notice.EEOC StatementWorldpay is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, marital status, genetic information, national origin, disability, veteran status, and other protected characteristics. The EEO is the Law poster is available here supplement document available here.For positions located in the US, the following conditions apply. If you are made a conditional offer of employment, you will be required to undergo a drug test. ADA Disclaimer: In developing this job description care was taken to include all competencies needed to successfully perform in this position. However, for Americans with Disabilities Act (ADA) purposes, the essential functions of the job may or may not have been described for purposes of ADA reasonable accommodation. All reasonable accommodation requests will be reviewed and evaluated on a case-by-case basis.Sourcing ModelRecruitment at Worldpay works primarily on a direct sourcing model; a relatively small portion of our hiring is through recruitment agencies. Worldpay does not accept resumes from recruitment agencies which are not on the preferred supplier list and is not responsible for any related fees for resumes submitted to job postings, our employees, or any other part of our company.\n        ",
    "location": "Cincinnati, OH"
  },
  "26": {
    "id": "4081050485",
    "Company": "Sabre Corporation",
    "title": "Data Science Engineer - 2025 Summer Intern",
    "description": "Sabre is seeking a talented Data Science Engineer to support the Sabre Labs Team. This person will report to Sabre Labs.Sabre Labs is a travel and technology innovation lab exploring capabilities that will impact travel over the next decade. Through research, model development, deployment and communication, Sabre Labs strives to make the future real for our businesses and customers.Airline industry is going through a drastic transformation in the area of retailing and distribution that requires very advance data analytics support to optimize revenue performance and customer experience. Recently introduced concepts of Offer/Order Management and Continuous Dynamic Pricing significantly expand opportunities for engaging with travelers through multiple touch points and creating personalized offers accounting for individual preferences and market context. These practices can substantially benefit from a combination of statistical and machine learning techniques leveraging huge volumes and variety of consumer and competitive data available in airline industry.The Data Science Engineer applies expert level statistical analysis, data modeling, and predictive analysis on strategic and operational problems in airline industry. As an intern in the Sabre Labs Research team, you will leverage your statistical and business expertise to translate business questions into data analysis and models, define suitable KPIs, and graphically present results to a wide range of audiences including internal clients, sales, and development team. In addition, you will source data from multiple different data sources, write high-quality data manipulation scripts in Python, bash, goLang etc, develop and apply data mining and machine learning algorithms for advanced analysis and prediction.Role And ResponsibilitiesWork with subject matter experts to solve prediction/forecasting/optimization problem at hand.Assess the effectiveness and accuracy of data sources, data gathering and forecasting techniques.Develop custom data models and algorithms to apply to data sets and run proof of concept study.Leverage existing Statistical and Machine Learning tools to enhance in-house algorithms.Collaborate with software engineers to implement and test quality code for forecasting and data analytics models.Develop processes and tools to monitor and analyze data accuracy and models\u2019 performance.Conduct logical analysis of moderate to very complex management problems and information systems and formulates mathematical models for resolution of real world problemsPrepare written technical reports to management indicating solution or range of possible alternatives in rank of desirability and probability of success when there is no single resolutionPreferred Qualifications And EducationMS/PhD in (or candidate for) Statistics, Operations Research, Computer Science, Mathematics, Machine Learning or related Quantitative disciplines.Proven ability to apply modeling and analytical skills to real-world problems.Knowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks.Knowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications.Solid programming skills with knowledge of Python. GO or C++.Experience with deployment of machine learning and statistical models on a cloud and leveraging services in GCP.Extremely diverse/complex work is completed with minimal direction, diverse interactions, knowledge resource.Familiarity with airline, hospitality or retailing industries and decision support systems employed there.Experience developing customer choice, recommendation modelsUnderstanding of airline distribution, pricing, revenue management, NDC and Offer/Order Management concepts.\n",
    "location": "Southlake, TX"
  },
  "27": {
    "id": "4189213739",
    "Company": "Empower Pharmacy",
    "title": "Data Engineer",
    "description": "Location: Hybrid - Houston, TexasPosted: March 19, 2025Req# 3364Company OverviewEmpower is a visionary healthcare company committed to providing quality, affordable medication to millions of patients across the nation. We hold the distinguished position of being the largest 503A compounding pharmacy and FDA-registered 503B outsourcing facility in the country serving the functional medicine markets.What sets us apart is our relentless focus on four core values: People, Quality, Service and Innovation. Our dedicated team of professionals collaboratively works to break new ground in the fields of manufacturing, distribution and quality control, providing a uniquely integrated approach to healthcare. Leveraging our vertical integration of the pharmaceutical supply chain, state-of-the-art technology, and a commitment to excellence, we are constantly pushing the boundaries of what is possible in medication accessibility and affordability.You're not just starting a job; you're joining a mission. We believe in empowering our team to innovate, grow, and drive real change in the healthcare industry. If you're a forward-thinker who thrives in a fast-paced, transformative environment, Empower is the place for you. Here, you\u2019ll be encouraged to share your ideas, expand your skill set, and contribute to projects that genuinely make a difference. We prioritize the well-being of our team members, and we offer a supportive and engaging workplace where your contributions are not just recognized but celebrated.Let\u2019s revolutionize healthcare, together. Join us and be a part of something extraordinary.Position SummaryData is the foundation of innovation, and this role ensures that information flows seamlessly, securely, and intelligently across the organization. The Data Engineer designs and deploys modern, cloud-based data infrastructure, optimizing Extract, Load, Transform (ELT) workflows and enabling advanced analytics, artificial intelligence, and machine learning. By driving data strategy and integrating cutting-edge solutions, this position supports our mission of expanding access to quality, affordable medication and empowering teams with the insights needed to transform healthcare.Duties And ResponsibilitiesDevelops and deploys cloud-based data infrastructure solutions, ensuring scalability, reliability, security, and quality while implementing modern data architectures, including star schema and data marts, to optimize storage and accessibility. Designs and implements ELT workflows using both visual ELT tools and manual coding, creating, managing, and optimizing data pipelines to enable analytics, artificial intelligence, and machine learning capabilities. Leverages Representational State Transfer Application Programming Interfaces to integrate external data sources, enhance data ecosystems, and ensure seamless, efficient data flow across platforms. Drives data strategy initiatives, contributing to visualization, insights, and reporting projects while applying analytical expertise to support business growth and inform key decision-making. Ensures the highest standards of data quality, security, and compliance by implementing and overseeing data protection measures, staying current with emerging technologies, and fostering a collaborative team environment. While performing the responsibilities of the job, the employee is required to talk and hear. The employee is often required to remain in a stationary position for a significant amount of the workday and frequently use their hands and fingers to handle or feel in order to access, input, and retrieve information from the computer and other office productivity devices. The employee is regularly required to move about the office and around the corporate campus. The employee is regularly required to stand, walk, reach with arms and hands, climb or balance, and to stoop, kneel, crouch or crawl.Knowledge And SkillsDemonstrated adaptability in a fast-paced environment with strong project management, organizational skills, and the ability to rapidly translate business requirements into technical solutions while maintaining excellent attention to detail. Expertise in database and data warehouse concepts, including relational and dimensional modeling, with deep experience in ETL tools, data ingestion, transformation, and visualization platforms, as well as proficiency in Structured Query Language Python, and Amazon Web Services (AWS) serverless technologies such as Lambda, Step Functions, and DynamoDB. Key CompetenciesCustomer Focus: Ability to build strong customer relationships and deliver customer centric solutions. Optimizes Work Processes: Know the most effective and efficient processes to get things done, with a focus on continuous improvement. Collaborates: Builds partnerships and works collaboratively with others to meet shared objectives. Resourcefulness: Secures and deploys resources effectively and efficiently. Manages Complexity: Makes sense of complex, high quality, and sometimes contradictory information to effectively solve problems. Ensures Accountability: Holds self and others accountable to meet commitments and objectives. Situational Adaptability: Adapts approach and demeanor in real time to match shifting demands of different situations. Communicates Effectively: Develops and delivers multi-mode communications that convey a clear understanding of the unique needs of different audiences. ValuesPeople: Our people define who we are as a company, and we believe that understanding and addressing the needs of our team, clients, and community is fundamental to fostering a culture of support and growth. Quality: Quality stands at the core of our mission, reflecting our commitment to excellence in every medication we produce. Service: We are here to serve others. Every interaction with our patients, providers, employees and other stakeholders comes from a place of service. Innovation: By continuously exploring new methodologies and embracing technology, we ensure that every solution we offer is at the forefront of pharmaceutical care.  Experience and QualificationsMinimum of 3 years of experience in data engineering with expertise in dimensional and relational data modeling. Requires a Bachelor\u2019s or Master\u2019s Degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. Preferred certifications include AWS Data Engineering, AWS Data Analytics, Data Vault 2, Snowflake, Kimball, and Inmon Data Modeling.  Employee Benefits, Health, and WellnessNo-Cost Medication: Get your prescribed compounded medications at no cost, ensuring your health without the financial burden. Onsite Health & Wellness \u2013 IV Therapy Drips: Rejuvenate with complimentary onsite IV Therapy drips, enhancing your well-being and energy levels. Comprehensive Medical, Dental, and Vision Options: Choose from three medical plans tailored to your needs, plus options for dental and vision coverage for you and your family. Telehealth visits: Access board-certified Doctors anytime, anywhere for you and your family. Paid & Volunteer Time Off: Enjoy paid time off for personal pursuits and contribute to causes you care about with volunteer time off. Paid Holidays (8 scheduled; 2 floating): Celebrate with eight scheduled holidays and two floating holidays, giving you flexibility and time for personal traditions. Life & AD&D Coverage: Secure your and your family's financial future with life and accidental death and dismemberment (AD&D) insurance. FSA (Flexible Spending Account): Manage healthcare expenses smartly with pre-tax dollars in a Flexible Spending Account (FSA). 401K Dollar-for-Dollar Up to 4%: Invest in your future with our 401K plan, featuring a dollar-for-dollar match up to 4%. Company Paid Short-Term Disability: Provided at no cost, which replaces 60% of your income if you become disabled for a short period of time. Flexible Schedules: Balance work and life seamlessly with our flexible scheduling options. Rewards & Recognition Program: Your hard work doesn't go unnoticed \u2013 enjoy rewards and recognition beyond your paycheck. Additional Voluntary BenefitsAccident Insurance: Pays a lump sum benefit to help cover expenses following an accidental injury. Hospital Indemnity Insurance: Enhance your peace of mind with supplemental hospital insurance for unexpected stays. Critical Illness: Protect your finances from the expenses of a serious health issue. Long-Term Disability: Protect your income during illness or injury with long-term disability coverage. Supplemental Life & AD&D: Add an extra layer of financial protection for you and your loved ones with supplemental life and AD&D coverage. Legal Services: Access professional legal assistance to address concerns confidently. Identity Theft Protection: Safeguard your identity and finances with our identity theft protection benefit. Pet Insurance: Care for your furry family members with our pet insurance coverage. Employee Assistance Program: Confidential counseling and support services for a holistic approach to your well-being.\n",
    "location": "Houston, TX"
  },
  "28": {
    "id": "4167447758",
    "Company": "Tesla",
    "title": "Internship, Data Engineer, Energy (Summer 2025)",
    "description": "What To ExpectConsider before submitting an application:This position is expected to start May 2025 and continue through the entire Summer term (i.e. through August/September 2025) or into Fall 2025 if available. We ask for a minimum of 12 weeks, full-time and on-site, for most internships.International Students: If your work authorization is through CPT, please consult your school on your ability to work 40 hours per week before applying. You must be able to work 40 hours per week on-site. Many students will be limited to part-time during the academic year.The Internship Recruiting Team is driven by the passion to recognize and develop emerging talent. Our year-round program places the best students in positions where they will grow technically, professionally, and personally through their experience working closely with their Manager, Mentor, and team. We are dedicated to providing an experience that allows the intern to experience life at Tesla by including them in projects that are critical to their team\u2019s success.Locations: Palo Alto, CA or Austin, TXWhat You'll DoApplicants may be reviewed by one or more of the following teams:Energy | AutobidderThe mission of the Autobidder team is to accelerate the world's transition to sustainable energy by maximizing the value of storage and renewable assets. We achieve this by building state-of-the-art software products for monetizing front-of-the-meter and behind-the-meter energy storage systems. Our flagship product, Autobidder, is an end-to-end automation suite for wholesale electricity market participation of grid-connected batteries and renewable resources that maximize revenues by optimally bidding in all available revenue streams in these markets. We are a multidisciplinary algorithmic trading team with expertise in machine learning, numerical optimization, software engineering, distributed systems, electricity markets, and trading. We have a proven track record of operating storage assets and delivering high revenues in both utility-scale and Virtual Power Plant (VPP) settings. Our products currently manage over 7GWh of energy storage worldwide and have returned over $420 million in trading profits, and we're slated for rapid growth on the horizon.Energy | Asset ManagementThe Energy Asset Management team is responsible for preparing data for analytical and operational uses across all of energy products. The team builds and maintains data pipelines that consolidate data from different systems into a structured format that can be consumed by the team to build reports, dashboards and drive decisions.Deploy data pipelines that transforms data into structure output for analytics in a robust, efficient and scalable frameworkBuild tools and frameworks to help users summarize and explore the dataAutomate reporting and data quality checksProvide ongoing support for deployed pipelinesEnergy | Service Transformation & AnalyticsThe Energy Services Transformation & Analytics team is embedded in daily operations, supporting both near-term execution and long-term planning. Team members are expected to partner closely with the business to analyze technical data, provide recommendations, and lead cross-functional decision-making and implementation of the recommendations.Perform Advanced SQL queries on massive datasets to create analytics that provide insight to drive action using various SQL languages and Spark distributed infrastructureCreate and maintain analytics dashboards (using Tableau, PowerBi and similar tools)Create and maintain data ELT pipelines using Python and Airflow schedulingUse Git and Jira for workflow trackingEnergy | Service Engineering Data Analytics and Infrastructure, Query OptimizationThe Tesla Energy Service Engineering Data Analytics and Infrastructure team is responsible for leveraging the vast amount of data from the Energy fleet to ensure the best customer experience and fleet availability across the entire Tesla Energy portfolio; Industrial, Residential, Supercharger, and Solar. In this internship you will have a mandate to identify and improve the underlying algorithms and infrastructure supporting the entire Energy organization.Optimize and perform advanced SQL queries on large-scale datasets across various SQL dialects, leveraging Spark/Trino for distributed computing and actionable analytics.Apply outlier detection and unsupervised learning techniques to uncover patterns and anomalies.Develop high-performance data processing jobs with advanced Python skills, leveraging PySpark for optimized, scalable solutions.Build and maintain data ELT pipelines through Airflow scheduling to ensure efficient data flow.Use Git for version control and Jira for workflow management and tracking.Energy | Product EngineerTesla Energy Products Service Engineering and Fleet Analytic team utilize fleet data and engineering fundamentals to monitor product performance, and root cause design, process, or firmware failures in our fleet of renewable energy hardwareThe person in this role would play a central role in ensuring we have the highest quality products for our customers. This person will work with a cross-functional team of engineers to define and track product KPIs, troubleshoot and diagnose device issues, deploy necessary corrective actions ranging from firmware changes, hardware design changes, manufacturing process or installation process changes, etc.What You'll BringBachelor\u2019s Degree in Computer Science, Computer Engineering or equivalent in experienceStrong knowledge of SQL and PythonDemonstrated experience building data pipelines using AirflowWorking knowledge of Spark and big data processingExperience with Git or other source control softwareScala experience is a plusBenefitsCompensation and BenefitsAs a full-time Tesla Intern, you will be eligible for: Aetna PPO and HSA plans > 2 medical plan options with $0 payroll deduction Family-building, fertility, adoption and surrogacy benefits Dental (including orthodontic coverage) and vision plans. Both have an option with a $0 payroll contribution Company Paid (Health Savings Account) HSA Contribution when enrolled in the High Deductible Medical Plan with HSA Healthcare and Dependent Care Flexible Spending Accounts (FSA) 401(k), Employee Stock Purchase Plans, and other financial benefits Company Paid Basic Life, AD&D, and short-term disability insurance Employee Assistance Program Sick time after 90 days of employment and Paid Holidays Back-up childcare and parenting support resources Voluntary benefits to include: critical illness, hospital indemnity, accident insurance, theft & legal services, and pet insurance Commuter benefits Employee discounts and perks program, Tesla\n        ",
    "location": "Austin, TX"
  },
  "29": {
    "id": "4183585013",
    "Company": "Juniper Networks",
    "title": "Data Science Intern/ AI/ML Engineer Intern - Summer 2025",
    "description": "At Juniper, we believe the network is the single greatest vehicle for knowledge, understanding, and human advancement the world has ever known.To achieve real outcomes, we know that experience is the most important requirement for networking teams and the people they serve. Delivering an experience-first, AI-Native Network pivots on the creativity and commitment of our people. It requires a consistent and committed practice, something we call the Juniper Way.Data Science Intern/ AI/ML EngineerAre you interested in applying your data science and machine learning knowledge to real world applications? This provides opportunity for building end-to-end solution using AI/ML \u2013 data explorations, identify key data correlations, feature engineering & encoding, building model, assess model and deploy the model.We at Juniper Networks are seeking continuing students who are interested in applying their Data Science & AI/ML skills during Summer 2025 Internships.We\u2019re looking for people who are excited about:Solve real-world problems using data science AI/ML.Learn Service Demand Planning processExplore data to gain insightsModels the data, assesses and rebuildArt of possibilitiesResponsibilities include:Reviewing existing data for feature engineering of various demand and supply attributesDeveloping AI/ML model for forecasting service inventory across global service locations based on features selectedValidating the model, assess model for accuracy and confidenceIntegrating defined model with service logistics process to fulfil customer demandSetting up regular intervals to model learning with new increment dataReleasing the solution to pre-production for further quality assuranceEstablishing a near-real time prediction processThe Successful candidate must:Have either a bachelor\u2019s or master\u2019s degree with a major in data science and/or AI/ML. We will also accept candidates who are pursuing a Master\u2019s degree program in data science and/or AI/ML. All candidates must have an anticipated graduation date from their degree program of December 2025 or laterHave college projects supporting developing AI/ML modelsTechnical Skills: Python, SQL, Data Science AI/ML related librariesPreferred qualifications for this role include:Candidates in Master's degree programs for Data Science and/or AI/MLWorking knowledge of data pipeline for continuous model learning and deploymentAdditional qualities of a successful candidate include (feel free to select from these or add your own):Analytical and problem-solving skills Demonstrated track record of taking initiative and being resourceful Ability to work collaboratively and solve problems Strong communication skills, especially written Leadership experience, whether formal or informal Enthusiasm! Please note the following restrictions for this position:Relocation assistance is not offeredCandidates for this role must possess authorization to work in the United States without need for current or future sponsorship.In addition, you will join our comprehensive University Talent Program that fosters personal as well as professional growth, thoroughly preparing interns for the next step in their careers. Juniper internships are project-based, allowing interns to solve important problems, make lasting impacts on their teams, and contribute to the company\u2019s overall objectives. Interns are mentored by some of the most talented and skillful innovators in the industry; you will connect with transformative thought leaders who promote disruptive ideas in their field. You will also participate in constructive professional workshops, company-wide community service initiatives, and an executive speaker series.\u202fYou\u2019ll build lasting friendships, expand your network, and connect with colleagues from around the world.At Juniper Networks, you will not only have opportunities to build real-life, hands-on work experience \u2013 you\u2019ll also have the chance to\u202flearn from the best\u202fand lay the groundwork for a successful career.Minimum Salary: $66,560.00Maximum Salary:$120,750.55The pay range for this position is expected to be between $66,560.00 and $120,750.55/year; however, the base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, and experience. The total compensation package for this position also includes medical benefits, 401(k) eligibility, vacation, sick time, and parental leave. Additional details of participation in these benefit plans will be provided if an employee receives an offer of employment.If hired, employee will be in an \u201cat-will position\u201d and the Company reserves the right to modify base salary (as well as any other payment or compensation program) at any time, including for reasons related to individual performance, Company or individual department/team performance, and market factors.Juniper\u2019s pay range data is provided in accordance with local state pay transparency regulations. Juniper may post different minimum wage ranges for permanent residency petitions pursuant to US Department of Labor requirements.About Juniper NetworksJuniper Networks challenges the inherent complexity that comes with networking and security in the multicloud era. We do this with products, solutions and services that transform the way people connect, work and live. We simplify the process of transitioning to a secure and automated multicloud environment to enable secure, AI-driven networks that connect the world. Additional information can be found at Juniper Networks (www.juniper.net) or connect with Juniper on Twitter, LinkedIn and Facebook.WHERE WILL YOU DO YOUR BEST WORK?Wherever you are in the world, whether it's downtown Sunnyvale or London, Westford or Bangalore, Juniper is a place that was founded on disruptive thinking - where colleague innovation is not only valued, but expected. We believe that the great task of delivering a new network for the next decade is delivered through the creativity and commitment of our people. The Juniper Way is the commitment to all our colleagues that the culture and company inspire their best work-their life's work. At Juniper we believe this is more than a job - it's an opportunity to help change the world...INCLUSION AND DIVERSITY AT JUNIPERAt Juniper Networks, we are committed to elevating talent by creating a trust-based environment where we can all thrive together. We know from experience that people from underrepresented groups often do not apply for roles they do not feel they meet all the criteria for. If you think you have what it takes, but do not necessarily check every single box, please consider applying. We\u2019d love to speak with you.Additional Information for United States jobs:ELIGIBILITY TO WORK AND E-VERIFYIn compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire.Juniper Networks participates in the E-Verify program. E-Verify is an Internet-based system operated by the Department of Homeland Security (DHS) in partnership with the Social Security Administration (SSA) that allows participating employers to electronically verify the employment eligibility of new hires and the validity of their Social Security Numbers.Information for applicants about E-Verify / E-Verify Informaci\u00f3n en espa\u00f1ol: This Company Participates in E-Verify / Este Empleador Participa en E-VerifyImmigrant and Employee Rights Section (IER) - The Right to Work / El Derecho a TrabajarE-Verify\u00ae is a registered trademark of the U.S. Department of Homeland Security.Juniper is an Equal Opportunity workplace and Affirmative Action employer. We do not discriminate in employment decisions on the basis of race, color, religion, gender (including pregnancy), national origin, political affiliation, sexual orientation, gender identity or expression, marital status, disability, genetic information, age, veteran status, or any other applicable legally protected characteristic. All employment decisions are made on the basis of individual qualifications, merit, and business need.\n        ",
    "location": "Sunnyvale, CA"
  },
  "30": {
    "id": "4189195557",
    "Company": "Lensa",
    "title": "Data Engineer",
    "description": "Lensa is the leading career site for job seekers at every stage of their career. Our client, Amazon, is seeking professionals in New York, NY. Apply via Lensa today!DescriptionData EngineerCome build the future as a Data Engineer at Amazon, where you will be inspired working along best-in-class inventors and innovators! You will have the opportunity to create meaningful experiences that deliver on the ever-evolving needs of our customers, and your work will impact millions of people around the world.As an Amazon Data Engineer, you will solve unique and complex problems at a rapid pace, utilizing the latest technologies to create solutions that are highly scalable. You will find that there is an unlimited number of opportunities within Amazon, where developing your career across a wide range of teams is highly supported. We are committed to making your work experience as enjoyable as the experiences you\u2019ll be creating for our customers.Apply now and you will be eligible for Amazon Data Engineer positions that are based on your preferred location, team, and more. We\u2019re hiring across Amazon Stores in the United States and Canada.Teams with available positions include, but are not limited to: Consumer Technology: Build new generation features and products for amazon.com, constantly improving the Customer and Seller experience for billions around the globe. Whether building site wide features such as reviews and recommendations, category specific software for the likes of Pharmacy, Electronics, Digital Software and Video Games or seller infrastructure, there are a variety of complex problems to tackle using a range of technologies in the design of your technical solutions. Operations Technology: Shape the future of transportation planning and execution on a global scale, that impacts hundreds of fulfillment centers, thousands of Amazonians, and millions of customers across the world. Your technology will support thousands of operators worldwide to design, build and run the best-in-class Amazon transportation network. We are building intelligent software to make transportation more reliable, faster, and less costly, providing a better and less expensive experience for our customers. Human Resources Technology: Create a seamless experience for millions of Amazonians and/or candidates. Whether supporting technologies for onboarding, time and attendance, compensation, amazon.jobs, or recruiting, you\u2019ll deliver robust feature sets, elegant designs, intuitive user interfaces and systems that make it easy for Amazonians to excel at performing critical business functions.About UsWork/Life BalanceOur team puts a high value on work-life balance. It isn\u2019t about how many hours you spend at home or at work; it\u2019s about the flow you establish that brings energy to both parts of your life. We believe striking the right balance between your personal and professional life is critical to life-long happiness and fulfillment. We offer flexibility in working hours and encourage you to find your own balance between your work and personal lives.Mentorship & Career GrowthOur team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we\u2019re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign opportunities based on what will help each team member develop into a better-rounded contributor.Inclusive Team CultureHere at Amazon, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon\u2019s culture of inclusion is reinforced within our 14 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.Key job responsibilities Design, implement, and support a platform providing secured access to large datasets. Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation. Tune application and query performance using profiling tools and SQL. Analyze and solve problems at their root, stepping back to understand the broader context. Learn and understand a broad range of Amazon\u2019s data resources and know when, how, and which to use and which not to use. Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS. Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets. Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment.Basic Qualifications3+ years of data engineering experience Proficient in SQL Experience with data modeling, warehousing and building ETL pipelines Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS Preferred QualificationsExperience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases) Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status.Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you\u2019re applying in isn\u2019t listed, please contact your Recruiting Partner.Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits . This position will remain posted until filled. Applicants should apply via our internal or external career site.\n        ",
    "location": "New York, NY"
  },
  "31": {
    "id": "4190633175",
    "Company": "KONE",
    "title": "Data Engineer",
    "description": "Founded in 1910, KONE is a global leader that provides elevators, escalators and automatic building doors, as well as solutions for maintenance and modernization that add value to buildings throughout their life cycle. Our mission is to improve the flow of urban life and make our world\u2019s cities better places to live by providing innovative solutions that help make people\u2019s journeys safe, convenient and reliable.Our operations in over 60 countries around the world has helped us achieve our position as an innovation and sustainability leader with repeated recognitions by Forbes, Corporate Knights for clean capitalism and others.\u202f Job OverviewAre you ready to make your next career move to\u202fjoin our team as our Data Engineer\u202ffor KONE Americas in KONE Lisle, IL?Do you enjoy (ensuring continuous integration and delivery, as well as automate deployment processes for efficient data operations? Does collaborating with data architects, enterprise data owners, and development teams to design data architectures motivate you?Do you thrive in areas where you work closely with Data Engineers, Data Architects, Data Scientists, and Global Business functions to align data architecture with business needs and objectives? Are you skillful with technological tools such as SAP ERP, Salesforce, and MES? If you answered a resounding YES to these questions, then we have an amazing opportunity for you!Other Key Responsibilities:Develop and maintain data pipelines on AWS and Databricks, utilizing a Lake house architecture with Delta Lake and multi-hop medallion architecture for data quality and reusability.Implement and optimize data workflows and ETL jobs using Python, Scala, and DBT for end-to-end data processing.Support DevOps practices by writing clean, maintainable code and using GitLab, AWS CDK, and Terraform for Infrastructure-as-Code (IaC).Take responsibility for ensuring the data flow from the data movement layer to business-optimized data consumption layers on the cloud platform.Design data models, ensuring high-quality data assets with a focus on data modeling and data quality assurance activities.Address technical issues by providing data architecture leadership to engineering teams, analyzing data quality flaws, and developing solutions to mitigate data-related incidents.Contribute to the creation of innovative data products, translating strategic business goals into clear technical data requirements.Provide guidance on existing data products and consumption capabilities, ensuring that teams maximize the use of available data assets.Drive data-related innovation by identifying opportunities for improvement in architecture and integrating those into platform and roadmaps.Participate in the incident resolution process when issues related to data arise, helping to pinpoint root causes in data flow, data quality, and data system integrations.Propose change actions to address long-term solutions and minimize the impact of data-related incidents.Location of position: Lisle, ILJob Duties:As our Data Engineer, you will engage in hands-on data engineering and architectural decision-making to ensure high-quality data products, seamless integration, and optimized data consumption layers for the business. You will also work on various data engineering tasks for delivering solutions on AWS and Databricks, from development to resolving technical issues, maintaining and optimizing workloads.You will bring your strong background in software engineering or data science and are committed to a DevOps mindset to achieve goals collaboratively to KONE. You will use the knowledge gained through your bachelor\u2019s or Master\u2019s degree in Software Engineering, Data Engineering, Data Science, Computer Science (or a related field).Hiring requirements:Previous hands-on professional experience in developing and maintaining data pipelines on AWS and Databricks: Lake house architecture based on Databricks and Delta Lake, multi-hop medallion architecture to divide the data lake into bronze, silver and gold layers based on the quality and reusability of the data stored there, data product publishing in Unity catalog.Proficient in Python, Scala, DBT, and other programming languages for developing ETL solutions.Proven expertise in working with enterprise data landscapes like SAP ERP, Salesforce, and PDM systems, ensuring seamless integration and data flow.Experience in data modeling, data quality assurance, and data governance best practices.Strong familiarity with DevOps methodologies and tools like GitLab, Jira, Confluence, and Terraform for infrastructure and workflow management.Knowledge of data privacy regulations, cybersecurity best practices, and compliance standards for handling sensitive data.Ability to work in a global, multi-cultural team, with excellent communication and problem-solving skills.About KONEAt KONE, we foster an innovative and collaborative culture, valuing each individual's contribution. Employee engagement and sustainability are key focuses, promoting ethical practices and mutual respect. We're proud to offer experiences and opportunities to help you achieve career and personal goals while maintaining a healthy work-life balance.We hire individuals who value culture because we believe culture drives innovationWe value your authentic self. Collaborative, creative, and supportive work environment.Passionate about safety, quality, and innovation We care about the communities where we live and work. Just some of our many benefits include: Competitive salary Flexible work schedule Opportunities to learn and grow 401K Employer Match401k Employer Non-elective Contribution Well-being ProgramMedical, Prescription, Dental and Vision InsuranceDigital Health Solutions & TelehealthHealth Savings Account (HSA)Flexible Spending Accounts (FSAs)Employee Family Assistance Program (EFAP)Family & Medical LeaveParental LeaveLeave to Care for a Domestic PartnerPaid Time Off & HolidaysCompany Paid Life and AD&D InsuranceSupplemental Life and AD&D Insurance Company Paid Short-term and Long-term DisabilityBuy-Up Long-term DisabilityCritical Illness InsuranceHospital Indemnity & Accident InsuranceIdentity Theft ProtectionLegal InsuranceKONE Credit UnionTuition ReimbursementCommuter BenefitsAnnual Base Pay Range:The hiring range for this role is $90,300 \u2013 $124,190. The compensation package offered will depend on their ability to meet the requirements of the role and a range of factors unique to each candidate, including their skill set, years and depth of experience, certifications, and location.Variable Compensation:KONE Annual Bonus plan at 10% Target Incentive (50/50) based on achievement of company goals and individual goals.Come share your passion and energy to make a positive impact at KONE for our customers and your career!*Beware of Recruitment Scams!*We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.Read more on www.kone.com/careerAt KONE, we are focused on creating an innovative and collaborative working culture where we value the contribution of each individual. Employee engagement is a key focus area for us and we encourage participation and the sharing of information and ideas. Sustainability is an integral part of our culture and the daily practice. We follow ethical business practices and we seek to develop a culture of working together where co-workers trust and respect each other and good performance is recognized. In being a great place to work, we are proud to offer a range of experiences and opportunities that will help you to achieve your career and personal goals and enable you to live a healthy and balanced life.Read more on www.kone.com/careers\n",
    "location": "Lisle, IL"
  },
  "32": {
    "id": "4160656913",
    "Company": "Finix",
    "title": "Data Engineer",
    "description": "About UsMove money. Make money. Finix is a full-stack acquirer processor, empowering businesses of all sizes with flexible, modern payment solutions. Processing billions of dollars annually, Finix enables SaaS, marketplace, and e-commerce platforms to accept payments, manage payouts, and onboard merchants seamlessly. With our no-code, low-code, and developer-friendly tools, businesses can get up and running in hours\u2014not months.Finix has raised over $175M, including a $75M Series C led by Acrew Capital, with participation from Lightspeed Venture Partners, Leap Global, American Express Ventures, Bain Capital Ventures, Homebrew, Inspired Capital, Sequoia Capital, Visa, and others.About The RoleFinix\u2019s reporting cluster is the source of truth for operational metrics, customer-facing reports, billing, actionable insights, and a host of business-critical KPIs. As a Data Engineer in the Data Engineering team, you will own the tooling, frameworks, data pipelines, integrations, databases, schemas and the data integrity of this cluster. You are critical to both Finix, and our customers, in supplying the data that underpins key operations decisions, uncovers hidden issues, and powers insight through reports, analytics, and dashboards owned and managed by the Data Engineering team. Our operations teams will depend on you to find, source, prepare and supply the right data allowing them to run Finix. Our product team will depend on you to serve the right data quickly and reliably to our customers so they can make timely business decisions. Data Analysts will rely on you to source and channel data where it needs to be and how it needs to be organized to power daily operations and decisions.You willEnable our operations team to make data-driven decisions based on timely, consistent, complete, and correct data, reports, and analyticsWork closely with engineering team members to build reporting and analytics infrastructure, solving our and our customer\u2019s analytics needsDelight our customers by providing leading reporting and analytics capabilities supporting their operations and accounting teamsBe a go-to resource for all things data pipelines, data lakes and warehouse, tooling & frameworks, integration and sourcing of dataDesign, build and maintain all code, pipelines, schemas, databases and frameworks that power Finix\u2019s data analytics and reporting capabilitiesYou areDetail-oriented, with a healthy dose of mistrust, driving you to investigate even the slightest data discrepancyThrive in a fast-paced environment around common goals and priorities but able to self-direct and problem solve with others to make progressFast learner of new languages, tools & frameworks. You are willing to go wherever necessary to accomplish your goals even if you haven\u2019t been there before. Curious about new approaches and pushing the boundaries of what exists. You have3+ years of experience in Data Engineering, ETL, Data Modeling, Data Science or related roleProficient with SQL, Python, Java and frameworks such as Spark, Storm, Airflow, Glue etc. B.S in computer science, mathematics, statistics, physics, or a related data fieldExperience with BI tools such as Looker, Sisense, Superset, TableauFinix is an equal opportunity employer and values diversity. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or any other protected class.Role: Data Engineer IIILevel: IC3Location: San Francisco, CABase Salary Range: $150,000/yr to $190,000/yr + equity + benefitsOur salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries at our headquarters in San Francisco, California. Individual pay is determined by work location, job related skills, experience, and relevant education or training.\n        ",
    "location": "San Francisco, CA"
  },
  "33": {
    "id": "4188985931",
    "Company": "West Monroe",
    "title": "Data Engineer",
    "description": "Are you ready to make an impact?West Monroe is seeking a talented Data Engineer to join our Data Engineering & Analytics team. In this role, you will collaborate with our clients to address their most complex needs, working alongside a team of highly skilled engineers and architects. You will also be involved in strategic enterprise data projects and large-scale implementations.Responsibilities Support projects leveraging an agile delivery model through requirements gathering and documentation, data ingestion pipeline development, and technical testing. Contribute to project delivery in the following ways:Add configurations for data enrichments such as lookups, type conversions, calculations, and concatenations Add data quality rules Connect to new data sources Map multiple sources into a single output Interpret error messages to troubleshoot data pipelines Create outputs that facilitate reporting, dashboarding, and semantic modeling for tools such as Tableau and Power BI Test data pipelines to ensure accuracy Work closely with our clients as part of the development team to gather technical requirements and develop solutions Coordinate activities with data source application owners to understand the underlying data, ensure integration and data integrity Build relationships with clients through written and verbal communication Qualifications1 - 4 years of hands-on professional development experience Experience writing complex SQL queries related to data manipulation strategies Experience with Spark, Databricks, Kafka, Kinesis, Hadoop, and/or Lambda Exposure using ETL tools (Azure Data Factory, AWS Glue, Informatica, Talend, IBM DataStage, etc.), and/or database platforms (Snowflake, Redshift, Azure Synapse, or Oracle) Exposure to data visualization tools (Tableau, Power BI, Looker) Experience in technical testing, issue identification and resolution Strong verbal and written communication skills Ability to travel 30 to 50% to client sites Hybrid role - 2 days a week in officeBased on pay transparency guidelines, the salary range for this role can vary based on your proximity to one of our West Monroe offices (see table below). Information on our competitive total rewards package, including our bonus structure and benefits is here. Individual salaries are determined by evaluating a variety of factors including geography, experience, skills, education, and internal equity.Employees (and their families) are covered by medical, dental, vision, and basic life insurance. Employees are able to enroll in our company\u2019s 401k plan, purchase shares from our employee stock ownership program and be eligible to receive annual bonuses. Employees will also receive unlimited flexible time off and ten paid holidays throughout the calendar year. Ten weeks of paid parental leave will also be available for use after successful completion of one year of employment.Seattle or Washington, D.C.$92,800\u2014$109,200 USDLos Angeles$97,200\u2014$114,400 USDNew York City or San Francisco$101,700\u2014$119,600 USDA location not listed above$88,400\u2014$104,000 USDOther consultancies talk at you.At West Monroe, we work with you.We\u2019re a global business and technology consulting firm passionate about creating measurable value for our clients, delivering real-world solutions.The combination of business and technology is not new, but how we bring them together is unique. We\u2019re fluent in both. We know that technology alone is not the answer, but how we apply it is. We rely on data to constantly adapt and solve new challenges. Actions that work today with outcomes that generate value for years to come.At West Monroe, we zero in on the heart of the opportunity, getting to results faster and preparing people for what\u2019s next.You\u2019ll feel the difference in how we work. We show up personally. We\u2019re right there in the room with you, co-creating through the challenges. With West Monroe, collaboration isn\u2019t a lofty promise, but a daily action. We work together with you to turn vision into clear action with lasting impact.West Monroe is an Equal Employment Opportunity Employer\u202fWe believe in treating each employee and applicant for employment fairly and with dignity. We base our employment decisions on merit, experience, and potential, without regard to race, color, national origin, sex, sexual orientation, gender identity, marital status, age, religion, disability, veteran status, or any other characteristic prohibited by federal, state or local law. To learn more about diversity, equity and inclusion at West Monroe, visit\u202fwww.westmonroe.com/inclusion.If you are based in California, we encourage you to read West Monroe\u2019s Notice at Collection for California residents, provided pursuant to the California Consumer Privacy Act (CCPA) and linked\u202fhere.\u202f\n        ",
    "location": "Los Angeles Metropolitan Area"
  },
  "34": {
    "id": "4189224254",
    "Company": "Claritev",
    "title": "Data Engineer (hybrid in Boston)",
    "description": "At Claritev, we pride ourselves on being a dynamic team of innovative professionals. Our purpose is simple - we strive to bend the cost curve in healthcare for all. Our dedication to service excellence extends to all of our stakeholders - internal and external - driving us to consistently exceed expectations. We are intentionally bold, we foster innovation, we nurture accountability, we champion diversity, and empower each other to illuminate our collective potential.Be part of our amazing transformational journey as we optimize the opportunity towards becoming a leading technology, data, and innovation voice in healthcare. Onward and upward!!!We are looking for a passionate Data Engineer to join us in building the next generation data analytics platform, creating technical specification documents, and providing support for the data solutions across the enterprise. This is a hybrid role requiring you to be in Boston office twice a week!Job Roles And ResponsibilitiesWork with business users, technology teams, and executives to understand their data needs to create innovative solutionsDesign semantic data layer for self-service and analytics and AIPrepare data for predictive and prescriptive modelingIdentify ways to improve data reliability, efficiency and qualityWork with analytics, data science, and wider engineering teams to help with automating data analysis and visualization needs, advise on transformation processes to populate data models, and explore ways to design and develop data infrastructure.,JOB REQUIREMENTS (Education, Experience, And Training)Must have 2+ years of experience in designing, developing, and implementing large-scale applications or data engineering solutions. Experience in big data technology such as Delta Lake, Apache Hive, Apache Spark, Amazon Redshift, and Hadoop is required.Experience in building semantic layer using tools such as Dremio, Denodo, AtScale, Looker, Cube, and dbt.Experience with data catalog tools such as Informatica, Atlan, Open Metadata.Experience with workflow orchestration tools such as Airflow and Prefect.Experience in BI tools such as Tableau, Power BI, and ThoughtSpot.Exposure to machine learning, data science and artificial intelligenceAbility to collaborate and work effectively on complex software systems in a team settingA growth mindset, a willingness to take ownership of your work, and an ability to adapt to the challenges.CompensationThe salary range for this position is $100K to $120K. Specific offers take into account a candidate\u2019s education, experience and skills, as well as the candidate\u2019s work location and internal equity. This position is also eligible for health insurance, 401k and bonus opportunity.BenefitsWe realize that our employees are instrumental to our success, and we reward them accordingly with very competitive compensation and benefits packages, an incentive bonus program, as well as recognition and awards programs. Our work environment is friendly and supportive, and we offer flexible schedules whenever possible, as well as a wide range of live and web-based professional development and educational programs to prepare you for advancement opportunities.Your Benefits Will Include Medical, dental and vision coverage with low deductible & copayLife insuranceShort and long-term disabilityPaid Parental Leave401(k) + matchEmployee Stock Purchase PlanGenerous Paid Time Off - accrued based on years of serviceWA Candidates: the accrual rate is 4.61 hours every other week for the first two years of tenure before increasing with additional years of service10 paid company holidaysTuition reimbursementFlexible Spending AccountEmployee Assistance ProgramSummer HoursSick time benefits - for eligible employees, one hour of sick time for every 30 hours worked, up to a maximum accrual of 40 hours per calendar year, unless the laws of the state in which the employee is located provide for more generous sick time benefitsEEO STATEMENTClaritev is an Equal Opportunity Employer and complies with all applicable laws and regulations. Qualified applicants will receive consideration for employment without regard to age, race, color, religion, gender, sexual orientation, gender identity, national origin, disability or protected veteran status. If you would like more information on your EEO rights under the law, please click here.APPLICATION DEADLINEWe will generally accept applications for at least 15 calendar days from the posting date or as long as the job remains posted.\n",
    "location": "Boston, MA"
  },
  "35": {
    "id": "4180955080",
    "Company": "CAVA",
    "title": "Data Engineer",
    "description": "Company Profile:At CAVA we make it deliciously simple to eat well and feel good every day. We are guided by a Mediterranean heritage that\u2019s been perfecting how to eat and live for four thousand years. We prioritize authenticity, curiosity and the pursuit of excellence in everything we do. We are working towards something big, together.We foster a culture built on five core values: Generosity First, Always: We lead with kindness. Our best work happens when we act in service of others.Constant Curiosity: We are eager to learn, grow, and explore beyond the obvious.Act with Agility: We welcome change; it\u2019s the only constant. We embrace, adjust, adapt.Passion for Positivity: We greet each day with warmth and possibility.Collective Ambition: We have high aspirations that are achieved when we work together with a shared purpose.Data Engineer, Big Data EngineeringData Engineer position is a key engineering role helping us to design, build, and support our future state data platform here at CAVA. In this role you will be expected to develop, maintain and implement our next-generation data architecture working with other team members and product owners. The ideal candidate will combine technical expertise with strong analytical thinking andcollaborative skills to drive our data initiatives forward.What You\u2019ll Do: Designing and implementing end-to-end data pipelines using modern cloud technologies and best practicesOptimizing data engineering pipelinesCollaborating with data scientists and analysts to improve data quality and accessibilityImplementing data governance and security best practicesCreate design documents for data integration or data reporting projectsAnalyze data integration problems, provide solutions and recommend corrective actions.Analyze source system data structures and map them to target data warehouse schemas.Must have excellent skills in requirements analysis, logical/physical modeling, data transformation and data modeling and technical governance design concepts.Stay updated with the latest industry trends and technologies, apply them to improve software development processes.Ensure security and data protection measures are implemented within software applications.The Qualifications:Bachelor's degree in computer science, Engineering, or related field2+ years of experience in data engineering rolesExperience with cloud platforms (AWS, Azure, or GCP) and their data servicesExperience with Apache Spark and distributed computingExperience in Python and SQL for data processingStrong understanding of data warehousing concepts and dimensional modelingKnowledge of devops processesStrong problem-solving abilities and analytical thinkingExperience in performance tuning and optimizationAbility to work with and geographically disperse teams, both on and offshoreWillingness to work in 24x7 model when required. Expectation is that you support the code and products yourself and your team createPhysical Requirements: Ability to maintain stationary position to be able to operate a computer and other office equipmentMust be able to identify, analyze and assess detailsFor certain positions, must be able to occasionally move or transport items up to 50 poundsAbility to communicate with others and exchange information accurately and effectivelyConstantly positions self and move about to support ordinary restaurant or food production support or office operations, as applicableAbility to work in a constant state of alertness and in a safe mannerWhat we offer:Competitive salary, plus bonus and long-term incentives*Early Wage Access!*Unlimited PTO, paid parental leave, plus paid opportunities to give back to the communityHealth, Dental, Vision, Telemedicine, Pet Insurance plus more!401k enrollment with CAVA contributionCompany-paid STD, LTD, Life and AD&D coverage for salaried positions*Free CAVA foodCasual work environmentThe opportunity to be on the ground floor of a rapidly growing brandAll exempt and nonexempt employees are eligible for benefits. Benefits are effective the 1st of the month following 30 days of service and you have until the day before the effective date to enroll. A new hire can enroll in our benefit program by selecting a link that is emailed directly to the new hire at their personal email address once hired.Please note that visa sponsorship is not available. The compensation range posted includes total cash.*Indicates qualifying eligible positions onlyCAVA \u2013 Joining \u201cA culture, not a concept\u201dThis job description is not intended to be a comprehensive list of all the duties and responsibilities of the position, and such duties and responsibilities may change without notice. As an equal opportunity employer, CAVA considers applicants for all positions without regard to race, color, sex, religion, national origin, disability, age, height, weight, marital status, sexual orientation, familial status, genetic information or any other characteristic or protected classes as defined by federal, state, or local law.\n",
    "location": "Washington DC-Baltimore Area"
  },
  "36": {
    "id": "4184891397",
    "Company": "Salesforce",
    "title": "Summer 2025 Intern - Data Engineer",
    "description": "To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.About Futureforce University RecruitingOur Futureforce University Recruiting program is dedicated to attracting, retaining and cultivating talent. Our interns and new graduates work on real projects that affect how our business runs, giving them the opportunity to make a tangible impact on the future of our company. With offices all over the world, our recruits have the chance to collaborate and connect with fellow employees on a global scale. We offer job shadowing, mentorship programs, talent development courses, and much more.Job CategoryFixed Term & TemporaryJob DetailsAbout SalesforceWe\u2019re Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too \u2014 driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing well and doing good \u2013 you\u2019ve come to the right place.START DATE: Summer 2025Internship DatesGroup 1 May 19th - August 8thGroup 2 June 2nd - August 22ndGroup 3 June 16th - September 5thOur headcount demand is always changing as we grow; start dates listed here may or may not have an immediate opening at the time of your application.LOCATIONS WE HIRE FOR: Seattle WA, San Francisco CAOur headcount demand is always changing as we grow; some of the locations listed here may or may not have an immediate opening at the time of your application. Salesforce\u2019s Employee Success (ES) Product Management team is focused on advancing AI-driven data initiatives to transform and expand our data capabilities across all ES functions. A key priority in this effort is developing a strong data and AI foundation to support innovation, automation, and data-driven decision-making across the ES landscape.Job SummaryWe are seeking a motivated and enthusiastic Data Engineering Intern to join our team. This internship offers a unique opportunity to gain hands-on experience in building and maintaining data driven products in a fast-paced environment. The ideal candidate will have a strong foundation in Python and SQL, a solid understanding of data engineering concepts, and a passion for learning.ResponsibilitiesAssist in the design, development, and maintenance of data pipelines and ETL processes.Work with large datasets to build and optimize data models and data warehouses.Collaborate with data engineers and analysts to integrate data from various sources.Develop and maintain SQL queries and scripts for data extraction, transformation, and loading.Collaborate with team members to troubleshoot and resolve data-related issues.Document data engineering processes and procedures.Potentially assist in the exploration and implementation of AI related data projects.Required Skills And QualificationsCurrently pursuing a Bachelor's degree in Computer Science, Data Science, Engineering, or a related field (Spring 2026 Grad)Strong proficiency in Python and SQL.Proficiency using GIT for code standardization and versioning.Strong understanding of data warehouse concepts(e.g., star schema, snowflake schema), data models, and data pipelines.Familiarity with ETL (Extract, Transform, Load) principles.Basic knowledge of data integration and data modeling.Ability to write clean, efficient, and well-documented code.Strong problem-solving and analytical skills.Excellent communication and teamwork skills.Ability to learn quickly and adapt to new technologies.Preferred Skills And QualificationsExperience with data science or machine learning concepts.Experience with AWS technologies like ECS, Aurora, Lambda, S3 preferred.Experience with data pipeline tools (e.g., Apache Airflow).Knowledge of building AI agents.Experience with data visualization tools (e.g., Tableau, Power BI).AccommodationsIf you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form.Posting StatementAt Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at www.equality.com and explore our company benefits at www.salesforcebenefits.com.Salesforce is an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce does not accept unsolicited headhunter and agency resumes. Salesforce will not pay any third-party agency or company that does not have a signed agreement with Salesforce.\ufeffSalesforce welcomes all.Pursuant to the San Francisco Fair Chance Ordinance and the Los Angeles Fair Chance Initiative for Hiring, Salesforce will consider for employment qualified applicants with arrest and conviction records.For Washington-based roles, the base salary hiring range for this position is $50 to $50.For California-based roles, the base salary hiring range for this position is $54 to $54.Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Certain roles may be eligible for incentive compensation, equity, benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com.\n        ",
    "location": "San Francisco, CA"
  },
  "37": {
    "id": "4176383214",
    "Company": "Unum",
    "title": "Data Engineer I - Marketing Analytics (Hybrid)",
    "description": "When you join the team at Unum, you become part of an organization committed to helping you thrive.BenefitsHere, we work to provide the employee benefits and service solutions that enable employees at our client companies to thrive throughout life\u2019s moments. And this starts with ensuring that every one of our team members enjoys opportunities to succeed both professionally and personally. To enable this, we provide:Award-winning culture Inclusion and diversity as a priority Performance Based Incentive PlansCompetitive benefits package that includes: Health, Vision, Dental, Short & Long-Term Disability Generous PTO (including paid time to volunteer!) Up to 9.5% 401(k) employer contribution Mental health support Career advancement opportunities Student loan repayment options Tuition reimbursementFlexible work environments All the benefits listed above are subject to the terms of their individual Plans.And that\u2019s just the beginning\u2026With 10,000 employees helping more than 39 million people worldwide, every role at Unum is meaningful and impacts the lives of our customers. Whether you\u2019re directly supporting a growing family, or developing online tools to help navigate a difficult loss, customers are counting on the combined talents of our entire team. Help us help others, and join Team Unum today!General SummaryThe Data Engineer I will design, build and maintain data systems and pipelines to allow data to be accessed and utilized optimally for analytics or visualization/insight generation purposes. The role requires technical expertise in the fields of computer programming, database management and data architecture; and relies on increasing business knowledge.This is a hybrid opportunity available in one of the following locations: Atlanta, GA; Portland, ME; Chattanooga, TN; or Columbia, SC.Job SpecificationsBachelor\u2019s degree in quantitative field is required, Master\u2019s is preferred2 years of professional experience or equivalent relevant work experience preferredCore Data Engineer Capabilities: Knowledge in all of the following skillsets and deep expertise in at least two: Software Engineering: Experience in at least one relevant object-oriented programing language (i.e. Java/Scala, Python). Exposure to DevOps best practice including CI/CD, process automation and optimization. Data Architecture and Infrastructure: Understanding of data architecture principles and related infrastructure requirements, covering on-prem and Cloud platforms. Holistic Data Preparation: The ability to understand and present data, including a basic understanding of how the data will build towards a business solution.Data Extraction, Transform & Load: Preferred skills include: Experience in writing complex SQL queries that join multiple tables/databases. Experience exploring databases/tables or other legacy data content to identify best data sources to solve business problems. Demonstrates ability to troubleshoot complex SQL queries with little guidance. Demonstrates ability to create logical data models by combining data from multiple sources including internal and external dataCore business capabilities: Demonstrated communication skills, experience in financial services, exposure to working with senior management and executive leadership, and attention to detail while effectively prioritizing work and managing multiple projects simultaneously. Ability to understand and explain a problem and identify and communicate an appropriate solution.Leadership Capabilities: Developing ability to coach or mentor team members, ability to commit quickly and positively to change. Viewed as embracing change and leading proof of concept work and prototyping when necessaryPreferred characteristics: Entrepreneurial self-starter, a thorough, results-oriented problem-solver, and a lifelong learner with voracious curiosity AND basic understanding of their organization.Job DescriptionPrincipal Duties and ResponsibilitiesEmploy a variety of languages and tools to develop, construct, test and maintain data pipelines, ensuring they support the requirements of the business.Integrate varying sizes of data from different sources (including DB2, SQL Server, Web API and Teradata). Apply validation, aggregation, and reconciliation techniques to create a rich data framework.Work closely with the data scientists and business partners to understand the business problem they are trying to solve and the analytics solutions they plan to apply. Use this understanding to create appropriate data structures tailored for the specific problem.Create data assets that conform to scalability, extensibility, performance and maintainability requirements for the problem at hand. Promotes development of solutions following appropriate engineering process that is fit to purpose for different use scenarios. Understand and contribute to the evolution of the enterprise data architecture including the application of current and emerging data frameworks and tools (eg hosting data in Cloud).Efficiently prepares results for interpretation and/or visualization and communicates findings and potential value to manager.Support integration of solutions within existing business processes using automation techniques. Understand theory and application of current and emerging software engineering practices.Provides support to lower level Data Engineer peers.Perform other related duties as assigned~IN1Unum and Colonial Life are part of Unum Group, a Fortune 500 company and leading provider of employee benefits to companies worldwide. Headquartered in Chattanooga, TN, with international offices in Ireland, Poland and the UK, Unum also has significant operations in Portland, ME, and Baton Rouge, LA - plus over 35 US field offices. Colonial Life is headquartered in Columbia, SC, with over 40 field offices nationwide.The base salary range for applicants for this position is listed below. Unless actual salary is indicated above in the job description, actual pay will be based on skill, geographical location and experience.$60,500.00-$123,400.00Additionally, Unum offers a portfolio of benefits and rewards that are competitive and comprehensive including healthcare benefits (health, vision, dental), insurance benefits (short & long-term disability), performance-based incentive plans, paid time off, and a 401(k) retirement plan with an employer match up to 5% and an additional 4.5% contribution whether you contribute to the plan or not. All benefits are subject to the terms and conditions of individual Plans.CompanyUnum\n",
    "location": "Atlanta, GA"
  },
  "38": {
    "id": "4093873307",
    "Company": "ChenMed",
    "title": "Data Engineer",
    "description": "We\u2019re unique. You should be, too.We\u2019re changing lives every day. For both our patients and our team members. Are you innovative and entrepreneurial minded? Is your work ethic and ambition off the charts? Do you inspire others with your kindness and joy?We\u2019re different than most primary care providers. We\u2019re rapidly expanding and we need great people to join our team.The Business Intelligence Developer (Data Engineer) will work closely with business units and technical teams to understand, develop, improve, and maintain BI systems. The Business Intelligence Developer-DE will use their skills and expertise to transform data into metrics, automated reports, and dashboards providing actionable insights to improve decision-making at all levels of the business. The Business Intelligence Developer-DE will have strong data management skills and communicate directly with internal clients.Essential Job Duties/ResponsibilitiesDevelops a deep familiarity with a variety of data sources including transactional databases, data warehouses, internal tools, and external integrations.Develops data programs that extract, transform, and load (ETL) data from the data warehouse, assures that architecture and development follow data warehouse best practices. This individual works with DW architects, data architects, hardware/software engineers, DBAs, application vendors and managed services to ensure enterprise DW solutions are reliable, scalable, flexible and low maintenance.Develops reports, dashboards, and tools using SQL, Python, SSRS, and QlikView, as well as other tools.Takes ownership of our data and constantly improve its quality and integrity; proactively identifying issues at every step and coordinating the implementation of required fixes.Partners with business stakeholders to understand their requirements and assist in the development of dashboards, visualizations and solutions tailored to their needs.Documents existing processes, architectures and code.Assists, mentors, and instructs lower-level developers and analysts as needed.Performs other duties as assigned and modified at manager\u2019s discretion.Education And Experience CriteriaBA/BS degree in Computer Science, MIS, Business Administration or related field required OR additional experience above the minimum may be considered in lieu of the required education on a year-for-year basisMaster\u2019s level-education with a combination of technical and business background a plusA minimum of 1 year of ETL and/or SQL development experience requiredA minimum of 1 year of RDBMS experience (MySQL, SQL Server, PostgreSQL, etc.) strongly preferredA minimum of 1 year of healthcare industry experience (or equivalent educational background) preferredExperience programming with a high-level scripting language (Python, Perl, C#, etc.)Experience working with a variety of reporting and analytic tools (QlikView and SSRS strongly preferred)Proven experience working with business units; gathering their needs and creating solutionsExperience in query tuning and database engine optimization desiredExperience in database programming, stored procedures, data imports and exports desiredFamiliar with Medicare Advantage industry desiredWe\u2019re ChenMed and we\u2019re transforming healthcare for seniors and changing America\u2019s healthcare for the better. Family-owned and physician-led, our unique approach allows us to improve the health and well-being of the populations we serve. We\u2019re growing rapidly as we seek to rescue more and more seniors from inadequate health care.ChenMed is changing lives for the people we serve and the people we hire. With great compensation, comprehensive benefits, career development and advancement opportunities and so much more, our employees enjoy great work-life balance and opportunities to grow. Join our team who make a difference in people\u2019s lives every single day.Current Employee apply HERECurrent Contingent Worker please see job aid HERE to apply\n        ",
    "location": "Miami, FL"
  },
  "39": {
    "id": "4021589719",
    "Company": "CCC Intelligent Solutions",
    "title": "Data Science Engineer Internship Summer 2025",
    "description": "Pay Rate/Hourly Range:The hourly range is $20.00 - $43.00 per hour.Pay is based on factors including school year, program of study, role responsibilities, etc.CCC Intelligent Solutions Inc. is a leading cloud platform for the multi-trillion-dollar P&C insurance economy creating intelligent experiences for insurers, repairers, automakers, part suppliers, lenders, and more.The CCC Intelligent Experience Cloud, powered by broad AI and an innovative event-based architecture, connects more than 35,000 businesses to power customized applications and platforms for optimal outcomes, and personalized experiences that just work. Through purposeful innovation and the strength of our connections, our technologies empower the people and industry relied upon to keep lives moving forward when it matters most. Learn more about CCC at www.cccis.com.The RoleOur program is designed to #CCCJumpstart your career! At CCC, you will work and learn alongside innovative and inspiring leaders and gain valuable technical experience while working on real business solutions in a corporate setting.Key Responsibilities:Technology: Computer Vision Deep Learning and Machine Learning Data Exploration, Visualization, Storytelling ETLBuilding machine learning models and deploying them as products for the auto-insurance industry. This can be done using traditional machine learning or techniques specific to massive unstructured data such as deep learning.Machine learning on structured datasets \u2013 starting with data exploration and feasibility tests, design and perform experiments to compare modeling approaches, generate predictions and ensure the solution meets the target metrics specified by business requirements, as well as ensuring scalability of algorithms. Interpret, visualize and communicate results to a general audience.Applying deep learning to claim image data - using photos of damaged cars to predict what parts that need to be replaced, if a car is a total loss or repairable, finding the severity of the damage using AI and data insights. The amount of property, casualty, car photos, and other data that CCC can harness is one of our biggest competitive advantages.Requirements:In order to be considered for this role you are required to be in pursuit of an Associate, Bachelor's, Master's, or PhD degree throughout your internship.If you have a Masters, and/or PhD, and have demonstrated excellence in your academic studies, this opportunity is right for you. We are looking for candidates who offer strong collaborative skills and work well with a team. A strong interest in computer science and/or related fields is essential to the success of each intern.If you have knowledge, through coursework or project work, of Python, Machine Learning, Deep Learning, Mathematics, or Statistics/ Modeling, this opportunity is right for you.About CCC's Commitment to Employees:CCC Intelligent Solutions understands that our employees play an integral role in our vision to shape a world where life just works. Our team is defined by our values of Integrity, Customer-Focus, Innovation, Inclusion & Diversity, Tenacity, and Connection. Through diverse perspectives, purposeful innovation, and the strength of connections, our technologies empower the people and industry relied upon to keep lives moving forward when it matters most.At CCC, together everyone can thrive as we innovate and collaborate, creating employee experiences that just work. We are committed to providing opportunities for our people to make real-life impacts, advance in their careers, and contribute to CCC\u2019s success.CCC offers competitive compensation and benefits to support you and your families, including:401K MatchPaid time offAnnual Incentive Plan Performance BonusComprehensive health insuranceAdoption AssistanceTuition ReimbursementWellness ProgramsStock Purchase Plan optionsEmployee Resource GroupsFor more information about our benefits, please check out our careers site.Here, you belong. You are seen, valued, and respected. We celebrate you for who you are and all you bring. Every voice is heard and is important to our success. You can hear what employees have to say about our culture here.\n        ",
    "location": "Chicago, IL"
  },
  "40": {
    "id": "4187046973",
    "Company": "Los Angeles Dodgers",
    "title": "Data Engineer",
    "description": "Job DetailsDescriptionTitle: Data EngineerDepartment: Baseball Operations Status: Full-TimePay Rate: $120,000 - $130,000/year*Reports to: Senior Director, Baseball Systems PlatformsCompensation rates vary based on job-related factors, including experience, job skills, education, and training.The Los Angeles Dodgers Baseball Systems team is committed to building and maintaining the technological platforms for baseball data, analysis, and decision-making for the Dodgers, and to providing technical expertise and advice across Dodger Baseball Operations. We focus on both the tools needed to put a winning team on the field today and those we need to ensure a winning future. The data engineering group within Baseball Systems operates the data platform used throughout Baseball Operations. We design, implement, and maintain the processes that bring in game, tracking, and scouting data, make them available to the rest of the Dodgers, and in turn re-ingest output from analytics to make that available as well.The Data Engineer works within the data engineering group within Baseball Systems to support our data operations. You will be responsible for implementing new ETL services and working with combining baseball data sources to create a cohesive view of games and plays. You will support the health of our data platform across relational databases, non-relational stores, and cloud file storage. You will collaborate with the rest of Baseball Operations to make sure that the data we provide them is complete, accurate, and meets their needs.We work together to build and maintain a winning, industry-leading team. If you are also enthusiastic about baseball and want to see your work reflected on the field and in the box score, please contact us!Essential Duties & ResponsibilitiesDesign, build, enhance, and support operations for the Dodgers R&D database schema and ETL layerDesign, implement, review, and test data collection, storage, and mapping proceduresBuild and maintain computational environments to support analytical modeling (statistics, machine learning, and optimization)Apply statistical models for data quality testing and missing data imputationWrite, optimize, and automate data processing tasksBuild or integrate system and database monitoring tools and maintain themPrioritize tasks within a projectMentor junior data engineers in implementation, software engineering processes, and baseball domain knowledgePerform other related duties as assignedBasic Requirements/QualificationsB.S. or M.S. in Computer Science, Computer Engineering, or a related field1+ years of data engineering experienceWilling and able to commute or travel for on-site work in Los Angeles as neededSQL development skills and an understanding of database technologies (PostgreSQL preferred)Experience with Python, Bash, and other scripting languagesExperience using Linux servers in a virtualized environmentFamiliarity with cloud-based and distributed computing concepts (AWS and Kubernetes preferred)Excellent analytical and problem-solving skillsYou Should Definitely Contact Us IfYou want to work together to build and maintain an industry-leading teamYou are enthusiastic about baseball, and you want to see your work reflected on the field and in the box scoreCurrent Los Angeles Dodgers employees should apply via the internal job board in UltiPro by following these prompts:MENU > MYSELF > MY COMPANY > VIEW OPPORTUNITIES > select the position > CONSENT > APPLY NOWLOS ANGELES DODGERS LLC is firmly committed to providing equal opportunity for all qualified applicants from every race, creed, and background. The Organization is also firmly committed to complying with all applicable laws and governmental regulations at the state and local levels which prohibit discrimination.LOS ANGELES DODGERS LLC considers all applicants without regard to national origin, race, color, religion, age, sex, sexual orientation, disability, military status, citizenship status, pregnancy or related medical conditions, marital status, ancestry-ethnicity, or any other characteristic protected by applicable state or federal civil rights law. The Immigration Reform and Control Act requires that the Organization obtain documentation from every individual who is employed, which verifies their identity and authorizes their right to work in the United States.LOS ANGELES DODGERS LLC is committed to the full inclusion of all qualified individuals. As part of this commitment, LOS ANGELES DODGERS LLC will ensure that persons with disabilities are provided reasonable accommodations for the hiring process. If reasonable accommodation is needed, please contact talentrelations@ladodgers.com.\n        ",
    "location": "Los Angeles, CA"
  },
  "41": {
    "id": "4115312625",
    "Company": "Georgia-Pacific LLC",
    "title": "Data Engineer",
    "description": "Georgia Pacific\u2019s BI and Data Analytics group supporting key segments including Packaging & Cellulose and Building Products is looking for a Data Engineer to join our team. We are looking for entrepreneurial minded innovators and leaders who can help us further develop this service of exceptionally high value to our business, improve the capabilities and delivery of the team.LOCATION: ATLANTA, GAOPEN TO H1B CANDIDATESWhat You Will DoA successful candidate will bring knowledge of best-in-class BI development standards, practices. You must be enthusiastically collaborative, passion for working with people, value seeking, open to challenge and be challenged with new ideas and established approaches with an appetite for learning and innovation. Ideal candidate would have strong technical experience to design and develop BI Solutions and partner across cross functional business and technology teams across the globe to co deliver impactful solutions.Design, Develop, Test, and deploy Pipelines from a wide variety of sources with Batch, Real-time and Near Real-time capabilities across our Segments.Demonstrate strong conceptual, analytical, and problem-solving skills and ability to articulate ideas and technical solutions effectively to external IT partners as well as internal data team members.Work with cross-functional teams, on-shore/off-shore, development/QA teams/Vendors in a matrixed environment for data delivery.Maintain, monitor, troubleshoot, fix, and explain failures/errors in existing ETL jobs, BI models, dashboards, and reports.Work with users and BI team to develop business requirements and documentation.Update and maintain key data cloud solution deliverables and diagrams.Ensure conformance and compliance using Georgia-Pacific data architecture guidelines and enterprise data strategic vision.Who You Are (Basic Qualifications)Experience with overall all IT experience.Experience in PySpark, Glue, Lambda, S3 and other AWS tool sets.Develop data pipeline from Enterprise data lake using PySpark, Glue and other AWS tools.Hands-on experience in designing, implementing, managing large-scale data and ETL solutions with AWS IaaS and PaaS Compute, Storage, and Database services (S3, RDS, Lambda, IAM, RedShift, Glue, EMR, Kinesis, Athena).Hands on experience in Cloud monitoring stack like CloudTrail, CloudWatch and AWS Event-bridge trigger service.Working experience in building Data Marts and Data Modeling.Good understanding of Data warehouse, ETL, AWS architecture and Redshift.What Will Put You AheadAWS certifications: Solution Architect (SAA/SAP) or Data Analytics Specialty (DAS).Full life cycle project experience in building a data solution in AWSUnderstanding of common DevSecOps/DataOps and CICD processes, methodologies, and technologies.Strong knowledge in PySpark, Redshift store procedures, Kinesis and AWS Glue service.Experience in working with Infor and SAP S/4 Hana.At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate's knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.Hiring PhilosophyAll Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here.Who We AreAs a Koch company and a leading manufacturer of bath tissue, paper towels, paper-basedpackaging, cellulose, specialty fibers, building products and much more, Georgia-Pacific works to meet evolving needs of customers worldwide with quality products. In addition to the products we make, we operate one of the largest recycling businesses. Our more than 30,000 employees in over 150 locations are empowered to innovate every day \u2013 to make everyday products even better.At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.Our BenefitsOur goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength - focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes - medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.Additionally, everyone has individual work and personal needs. We seek to enable the best work environment that helps you and the business work together to produce superior results.Equal OpportunitiesEqual Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, some offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please click here for additional information.\n        ",
    "location": "Atlanta, GA"
  },
  "42": {
    "id": "4188601406",
    "Company": "Russell Tobin",
    "title": "Data Engineer",
    "description": "What are we looking for in our Data Engineer?Contract \u2013 6 monthsLocation \u2013 Cupertino, CA (Hybrid)Pay-range - $50-55/hour on W2Duties:Maintain and improve reliability of existing data science pipelines and tooling. This may include better scheduling, building better tools to build and test workflows, or updating and adding features to internal libraries.Debug and optimize data transformations, storage, and other issues with existing and new pipelines that arise.Assist with infrastructure modernization, moving to newer storage formats, internal tooling, and compute platforms.Build and maintain features in support libraries to enable faster and reproducible prototyping across the Eval Data Science organizationMaintaining internal Python libraries with best practices and teaching those practices to data scientistsServe as an internal engineering consultant for data science projects in areas like computation, data access, and data storage considerationsSkills:Experience with distributed computational framework, such as Spark (Scala or Python)Data ETL tasks like cleaning, transforming, processing and storage optimizationCI/CD experience with tools such as Github Actions, Travis CIData Orchestration Systems like Airflow, Dagster, PrefectCloud storage and data processing platformsLanguages: Python, Scala, SQLEducation:BS/BA in quantitative field such as engineering, mathematics, computer science, or equivalent experience such as 3+ years working on data engineering tasks like: data transformations, cleaning, orchestration, and data processing using Python and/or Scala.Benefits InfoRussellTobin offers eligible employee\u2019s comprehensive healthcare coverage (medical, dental, and vision plans), supplemental coverage (accident insurance, critical illness insurance and hospital indemnity), 401(k)-retirement savings, life & disability insurance, an employee assistance program, legal support, auto, home insurance, pet insurance and employee discounts with preferred vendors.#CBRate/Salary: $50-55/hr on W2\n",
    "location": "Cupertino, CA"
  },
  "43": {
    "id": "4184877679",
    "Company": "SonicWall",
    "title": "Data Plane Engineer Intern",
    "description": "SonicWall is a cybersecurity forerunner with more than 30 years of expertise and is recognized as a leading partner-first company, ensuring our partners and their customers are never alone in the fight against cybercrime. With the ability to build, scale and manage security across the cloud, hybrid and traditional environments in real-time, SonicWall provides relentless security against the most evasive cyberattacks across endless exposure points for increasingly remote, mobile and cloud-enabled users. With its own threat research center, SonicWall can quickly and economically provide purpose-built security solutions to enable any organization\u2014enterprise, government agencies and SMBs\u2014around the world. For more information, visit www.sonicwall.com or follow us on Twitter, LinkedIn, Facebook and Instagram.About YouYou have a passion for software engineering, network software, and building scalable distributed systems. You enjoy working with network protocols, sockets programming, network traffic reporting, policy enforcement, and working with authorization and authentication protocols like OAuth2, OpenID Connect, and SAML.ResponsibilitiesAs a Software Engineer Intern, you will get a chance to develop the mission-critical distributed subsystem in a rapidly scaling platform that handles millions of requests and massive amounts of events and other data. It is real-time, scalable, and highly available, and is the source of data for security monitoring, alerting, access control, and data visualization. You understand the importance of data collected from every application and component in a software-defined business environment - web, mobile, server, infrastructure, hardware - in enabling the most advanced and effective security controls and insights for business and IT decision-making.RequirementsExperience developing high performance network softwareExperience with networking and sockets programmingExperience with OS kernel network subsystems in Linux and/or WindowsStrong foundation in a programming language such as Go, C/C++, Python, a data structures, algorithms and a strong understanding of threads, synchronization, and concurrent programmingUnderstanding of design and implementation principles to build secure, scalable, resilient systemsEducation And ExperienceCurrently enrolled in a Degree program in Computer Science, Engineering, or related field preferred.shipSonicWall is an equal opportunity employer.We are committed to creating a diverse environment and are an equal opportunity employer. All qualified applicants receive consideration for employment without regard to race, color, ethnicity, religion, sex, gender, gender identity and expression, sexual orientation, national origin, disability, age, marital status, veteran status, pregnancy, or any other basis prohibited by applicable law.At SonicWall, we pride ourselves on recruiting a diverse mix of talented people and providing active security solutions in 100+ countries.Applicant Privacy Notice\n",
    "location": "United States"
  },
  "44": {
    "id": "4187286232",
    "Company": "Encore Talent Solutions",
    "title": "Junior Data Engineer ",
    "description": "Data Engineer + ACHTitle: Junior Data EngineerJob Description:A junior data engineer is an entry-level data professional who is responsible for designing, building, and maintaining data pipelines and systems. They work closely with Product Owners and Analysts to ensure that data is easily accessible and of high quality. Junior data engineers typically have less experience than data engineers and are expected to work under the guidance of experienced engineers.Job SummaryWe are seeking a highly motivated and detail-oriented Entry Level Data Engineer to join our dynamic team. The successful candidate will assist in gathering requirements, curating data elements and documenting data lineage in specified data repositories within the bank. This entry-level position offers an excellent opportunity to gain hands-on experience in the field of data engineering and work closely with experienced professionals.Core Skills1. Programming LanguagesProficiency in programming languages such as Python, SQL.2. Big Data TechnologiesExperience with big data technologies such as Hadoop, Spark, Kafka, and Flink.Familiarity with data warehousing solutions like Amazon Redshift, Google BigQuery, and Snowflake3. Database ManagementStrong understanding of relational databases (e.g., MySQL, PostgreSQL) and NoSQL databases (e.g., MongoDB, Cassandra).Skills in database design, indexing, and query optimization4. Data ModelingAbility to design and build data models to support business requirements.Experience with data modeling tools and techniques5. ETL ProcessesExpertise in Extract and Load (EL) processes and transform tool DBT Ability to develop and maintain ETL workflows6. Cloud PlatformsKnowledge of cloud platforms such as Snowflake Experience with cloud-based data services and infrastructure7. Data GovernanceUnderstanding of data governance principles and best practices.Ability to implement data governance frameworks to ensure data quality and compliance8. Problem-SolvingStrong analytical and problem-solving skills.Ability to troubleshoot and resolve data-related issues9. CommunicationExcellent communication skills to collaborate with technical and non-technical stakeholders.Ability to translate complex technical concepts into understandable terms10. Continuous LearningCommitment to staying updated with the latest trends and technologies in data engineering.Willingness to continuously improve skills and knowledgeDuties and Responsibilities\u2022 Curating data elements and documenting data lineage\u2022 Managing data storage and ensuring data quality\u2022 Building and maintaining data pipelines and ETL processes \u2013 moving data between different cloud-based platforms and on-premises data sources\u2022 Developing and maintaining databases and data warehouses\u2022 Working with Product Owners and Analysts to understand business requirements and provide data solutions\u2022 Writing scripts and code to automate data processesRequirements and Qualifications\u2022 Strong problem-solving and communication skills\u2022 Proficiency in at least one programming language (Python, Java, etc.)\u2022 Proficiency in DB2 and/or DataStage\u2022 Knowledge of SQL and relational databases\u2022 Understanding of big data technologies (Curation, Data Lineage, Data Mesh, etc.)\u2022 Experience with cloud platforms (AWS, Snowflake, etc.)\n",
    "location": "Cincinnati, OH"
  },
  "45": {
    "id": "4191394793",
    "Company": "TieTalent",
    "title": "Junior Data Engineer",
    "description": "AboutJunior Data EngineerPhiladelphia, PAContract-to-HireA forward-thinking company specializing in data-driven solutions is looking to add a Junior Data Engineer to their growing team. We are seeking a motivated and skilled individual to take on new challenges. This role offers an exciting opportunity to work with cutting-edge technologies and contribute to impactful data projects.Responsibilities Develop and maintain data pipelines using Python and Spark. Perform big data analysis to extract meaningful insights. Create and maintain data visualizations to support decision-making. Process and clean raw data to ensure data quality and integrity. Collaborate with data scientists and analysts to understand data requirements. Optimize data workflows for performance and scalability. Document data processes and workflows for future reference. Troubleshoot and resolve data-related issues.Required Skills Proficiency in Python and Spark for data engineering tasks. Experience with big data analysis techniques and tools. Strong skills in data visualization using tools like Tableau, Power BI, or similar. Ability to handle and process raw data from various sources. Excellent problem-solving abilities and attention to detail. Strong communication and teamwork skills. Willingness to learn and adapt to new technologies and methodologies.Preferred Qualifications Bachelor's degree in Computer Science, Data Science, Engineering, or a related field. Familiarity with cloud platforms such as AWS, Azure, or Google Cloud. Experience with SQL and database management. Knowledge of ETL (Extract, Transform, Load) processes. Understanding of data warehousing concepts.Job Type: ContractPay: Up to $29.00 per hourSchedule 8 hour shift Monday to FridayWork Location: Hybrid remote in Philadelphia, PA 19103Nice-to-have skillsPythonSparkTableauPower BIAWSAzureSQLETLData WarehousingPhiladelphia, PennsylvaniaWork experienceData EngineerApplicationsData InfrastructureLanguagesEnglish\n",
    "location": "Philadelphia, PA"
  },
  "46": {
    "id": "4190735832",
    "Company": "Costco Wholesale",
    "title": "Data Engineer",
    "description": "Costco IT is responsible for the technical future of Costco Wholesale, the third largest retailer in the world with wholesale operations in fourteen countries. Despite our size and explosive international expansion, we continue to provide a family, employee centric atmosphere in which our employees thrive and succeed.This is an environment unlike anything in the high-tech world and the secret of Costco\u2019s success is its culture. The value Costco puts on its employees is well documented in articles from a variety of publishers including Bloomberg and Forbes. Our employees and our members come FIRST. Costco is well known for its generosity and community service and has won many awards for its philanthropy. The company joins with its employees to take an active role in volunteering by sponsoring many opportunities to help others.Come join the Costco Wholesale IT family. Costco IT is a dynamic, fast-paced environment, working through exciting transformation efforts. We are building the next generation retail environment where you will be surrounded by dedicated and highly professional employees.Data Engineers are responsible for developing and operationalizing data pipelines to make data available for consumption (i.e. reports and advanced analytics). This includes data ingestion, data transformation, data validation/quality, data visualization, data pipeline optimization, orchestration; and engaging with DevOps Engineers during CI/CD. The Data Engineer role requires a grounding in programming and SQL, followed by expertise in data storage, modeling, cloud, data warehousing, and data lakes. This role also will partner closely with Product Owners, Data Architects, Platform/DevOps Engineers, etc. to design, build, test, and automate data pipelines that are relied upon across the company as the single source of truth.The Data Engineer is responsible for data across the IS Sustainability stack. This is a new team at Costco and will be fast paced, highly visible, supporting the business goals by being an industry leader in this space. This role is focused on data engineering to build and deliver automated data pipelines from a variety of internal and external data sources.If you want to be a part of one of the worldwide BEST companies \u201cto work for\u201d, simply apply and let your career be reimagined.ROLEWorks in tandem with Data Architects to align on data architecture requirements provided by the requestor.Defines and maintains optimal data pipeline architecture.Analyzes data to spot anomalies, trends and correlate data to ensure Data Quality.Develops data pipelines to store data in defined data models / structures.to improve data reliability, efficiency and quality of data managementPerforms peer review for another Data Engineer\u2019s workDevelops and operationalizes data pipelines to create enterprise certified data sets that are made available for consumption (BI, Advanced analytics, APIs/Services).Identifies ways to improve data reliability, efficiency, and quality of data management.Works with area SMEs to design data pipelines and recommends ongoing optimization of data storage, data ingestion, data quality, and orchestration.Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery.Conducts ad-hoc data retrieval for business reports and dashboards.Assesses the integrity of data from multiple sources.Designs, develops, and implements ETL/ELT processes using Informatica Intelligent Cloud Services (IICS) and Azure Data Factory (ADF).Uses Azure services, such as Databricks, Azure SQL DW (Synapse), Data Lake Storage, Azure Event Hub, Cosmos, Delta-Lake to improve and speed up delivery of our data products and services.Develops and implements PowerBI reports and applications.Implements big data and NoSQL solutions by developing scalable data processing platforms to drive high-value insights to the organization.Leads the analysis by applying statistics, machine learning, and analytic approaches to predict and optimize business outcomes.Designs and builds ML/DL models to solve business problems.Frames a problem correctly and comes up with a hypothesis.Communicates technical concepts to non-technical audiences both in written and verbal form.RequiredExperience engineering and operationalizing data pipelines with large and complex datasets.Hands-on experience with Informatica PowerCenter and/or IICS.Experience with Cribl, Confluent/Kafka, Big Panda.Experience working with Cloud technologies, such as ADLS, Azure Databricks, Spark, Azure Synapse, Cosmos DB, and other big data technologies.Extensive experience working with various data sources (DB2, SQL, Oracle, flat files (csv, delimited), APIs, XML, JSON).Advanced SQL skills. Solid understanding of relational databases and business data; ability to write complex SQL queries against a variety of data sources.Experience with Data Modeling, ETL, and Data Warehousing.Strong understanding of database storage concepts (data lake, relational databases, NoSQL, Graph, data warehousing).Experience in delivering business insights using advanced statistical and machine learning models and visualization.Proficiency in working with diverse databases and other data sources.Experience with Git / Azure DevOps.Experience delivering data solutions through agile software development methodologies.Demonstrates ability to communicate technical concepts to non-technical audiences both in written and verbal form.Demonstrates strong understanding with coding and programming concepts to build data pipelines (e.g. data transformation, data quality, data integration, etc.).Demonstrates strong understanding of database storage concepts (data lake, relational databases, NoSQL, Graph, data warehousing).RecommendedGraduate degree in Computer Science, Data Science, and Statistics/Mathematics or related field.Azure, GCP Certifications.Experience implementing data integration techniques, such as event/message based integration (Kafka, Azure Event Hub), ETL.Exposure to the retail industry.Experience with UC4 Job Scheduler.Strong proficiency in Machine Learning, Statistical and Reporting tools (Python, R, SQL, PowerBI).Knowledge of Deep Learning and Neural Networks, and its applications.Strong experience working in Cloud (Azure, GCP) based analytics platform.Knowledge of Agile software development.Experience in software development.Excellent verbal and written communication skills.Proficient in Google Workspace applications, including Sheets, Docs, Slides, and Gmail.Required DocumentsCover LetterResumeCalifornia applicants, please click here to review the Costco Applicant Privacy Notice.Pay RangesLevel SR - $150,000 - $190,000, Bonus and Restricted Stock Unit (RSU) eligibleWe offer a comprehensive package of benefits including paid time off, health benefits - medical/dental/vision/hearing aid/pharmacy/behavioral health/employee assistance, health care reimbursement account, dependent care assistance plan, short-term disability and long-term disability insurance, AD&D insurance, life insurance, 401(k), stock purchase plan to eligible employees.Costco is committed to a diverse and inclusive workplace. Costco is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or any other legally protected status. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to IT-Recruiting@costco.comIf hired, you will be required to provide proof of authorization to work in the United States.\n        ",
    "location": "Seattle, WA"
  },
  "47": {
    "id": "4182142135",
    "Company": "Coalition, Inc.",
    "title": "Data Engineer I",
    "description": "About usCoalition is the world's first Active Insurance provider designed to help prevent digital risk before it strikes. Founded in 2017, Coalition combines broad insurance coverage with a digital risk assessment and continuous security monitoring to help organizations protect themselves in today\u2019s hyper-connected world.Opportunities to make an impact with bold thinking are real - and happening daily. ResponsibilitiesOwn and maintain robust data models that serve all verticals and areas of Coalition\u2019s businessSet the standard for fidelity and metrics consistency across all data models, ensuring all team members have access to the latest and most accurate data, via query or our business intelligence platformCollaborate with Product, Analytics, Engineering, and Business team members to help report on KPIs, OKR performance monitoring, and metrics anomaliesImprove our analytics data stack\u2019s efficiency and deliver timely data even as the business scalesMake new data sources available and query-able via a ETL pipeline or other data processing jobsSkills and QualificationsA Bachelor's degree in Statistics, Economics, Mathematics or related technical field1-2 years of experience in related analytical, data science or consulting backgroundSQL proficiency with proven competencies in designing well-architected data models, optimizing for query performance, and clearly documenting codeStrong communication skills and ability to convey complex topics to non-technical audiencesPassion, drive, and a deep sense of responsibility. We build the systems that keep us and our customers safe!Bonus PointsWork in Tableau, Looker, or similar visualization / business intelligence platformExperience with dbt, Airflow, dagster, Prefect, or similar analytics engineering toolsExperience with Python, R, or similar programming languageUnderstanding of and experience with experimental design, statistical methods and machine learningExperience in insurance, insurtech and fintech industriesExperience using data science tools like scikit-learn, pandas, numpy, XGBoost, TensorFlowExposure to causal inference and other advanced statistical methodsCompensationOur compensation reflects the cost of labor across several US geographic markets. The US base salary for this position ranges from $88,000/year in our lowest geographic market up to $134,300/year in our highest geographic market. Consistent with applicable laws, an employee's pay within this range is based on a number of factors, which include but are not limited to relevant education, skills, job-related knowledge, qualifications, work experience, credentials, and/or geographic location. Your recruiter can share more on target salary for your location during the interview process. Coalition, Inc. reserves the right to modify this range as needed.Perks100% medical, dental and vision coverageFlexible PTO policyAnnual home office stipend and WeWork accessMental & physical health wellness programs (One Medical, Headspace, Wellhub, and more)!Competitive compensation and opportunity for advancement \u00a0Why Coalition?\u00a0We\u2019re a remote-first, mission-driven team committed to building a more inclusive culture with people of all different backgrounds. We trust our team members to take responsibility, share ownership, and put in the work to help us in our pursuit to solve digital risk.Coalition\u2019s exceptional growth stems from its ability to address real-world problems for organizations of all sizes and remain true to our founding values of character, humility, responsibility, purpose, authenticity, and inclusion.\u00a0We\u2019re always looking for collaborative, inquisitive individuals to join #OurCoalition.Visit our Newsroom >\u00a0Privacy NoticeCoalition is committed to protecting your privacy and handling your personal information responsibly. We collect, use, and store personal information as necessary for the recruitment process and in compliance with applicable privacy laws and regulations in all regions where we operate. We want you to understand what personal information we collect, how we use it, and your rights regarding access, correction, and deletion of your data where applicable. Information submitted, collected, and processed as part of your application is subject to Coalition's Privacy Policy. For further details, please review our full Privacy Policy or contact us with any questions regarding how your information is handled.Our Privacy Policy >\u00a0Anti-Discrimination NoticeCoalition is proud to be an Equal Opportunity employer. Our policy is to provide equal employment opportunities to all individuals, without discrimination or harassment on the basis of any characteristic protected by applicable laws in each country where we operate. This commitment includes, but is not limited to, ensuring equal treatment in recruitment, selection, training, promotion, transfer, compensation, and all other aspects of employment. Coalition does not tolerate discrimination or harassment of any kind, and we are dedicated to fostering an inclusive and supportive workplace.AccommodationsCoalition is committed to providing reasonable accommodations to qualified individuals with disabilities, including applicants and employees, in accordance with applicable laws and regulations in each country where we operate. Our policy is to support equal opportunity in the hiring process by considering qualified applicants regardless of disability or other protected characteristics, unless providing accommodation would impose an undue hardship or disproportionate burden. If you require accommodation to complete an application, interview, pre-employment testing, or participate in the selection process, please contact us at candidateaccommodations@coalitioninc.com. We also consider all qualified applicants, including those with criminal histories, in line with applicable laws and regulations in each jurisdiction.To all potential candidates: Coalition primarily does not use third-party recruiting services. Potential candidates will only be contacted by Coalition, Inc. during the recruitment process. You can always verify any opportunity on our official careers page http://www.coalitioninc.com/careers.To all recruitment agencies: Coalition does not accept unsolicited agency resumes. Do not forward resumes to our email alias, employees, or other physical or virtual organization locations. Coalition is not responsible for any fees related to unsolicited resumes.\n",
    "location": "United States"
  },
  "48": {
    "id": "4169966868",
    "Company": "Agility Partners",
    "title": "Jr. Data Engineer",
    "description": "Agility Partners is seeking a qualified Junior Data Engineer to fill an open position with one of our clients. This is an exciting opportunity to gain hands-on experience in data engineering, working closely with Product Owners and Analysts to curate data elements, document data lineage, and ensure high-quality data management. The ideal candidate will have a strong foundation in data engineering principles and be eager to learn from experienced professionals.Responsibilities:Curate data elements and document data lineage.Manage data storage and ensure data quality.Build and maintain data pipelines and ETL processes to move data between cloud-based and on-premises sources.Develop and maintain databases and data warehouses.The Ideal Candidate:Strong problem-solving and communication skills.Proficiency in at least one programming language (Python, Java, etc.).Experience with DB2 and/or DataStage.Knowledge of SQL and relational databases.Understanding of COBOL.Experience with cloud platforms such as AWS or Snowflake is a plus.One to two years of experience in data engineering or a similar role.Reasons to Love It:Opportunity to gain hands-on experience in a growing field.Work closely with experienced professionals in a collaborative environment.Exposure to cutting-edge cloud and data technologies.\n",
    "location": "Cincinnati, OH"
  },
  "49": {
    "id": "4080016680",
    "Company": "Peloton Interactive",
    "title": "Data Engineer",
    "description": "ABOUT THE ROLEPeloton is looking for a talented Data Engineer to join the Data Science team. In this role, you will help scale Peloton\u2019s data infrastructure for business needs by building batch and streaming data pipelines that process terabytes of data daily. You will have the opportunity to work with multiple teams of engineers and business partners to support analytics and reporting data needs across the organization.\u00a0This role will be hybrid, not remote.YOUR DAILY IMPACT AT PELOTONBuild and maintain data pipelines to support the data and analytics needs of different stakeholders across multiple business verticals including Supply Chain, Logistics, Finance, Marketing, Product, etc.\u00a0Understand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions.Collaborate with the team to develop best practices in building our next generation data platform.Implement data lake ingestion pipelines using Apache Spark and Apache Hudi.YOU BRING TO PELOTON3+ years in data engineering roles or 4+ years in data-intensive engineering or software development roles.Bachelor\u2019s degree in a STEM related field. Advanced degree preferred.Strong knowledge of at least one programming language: Python (preferred) or Java.Working knowledge of developing ETL/ELT pipelines, including data processing and transforming data to meet business goals.1+ years of experience with big data architectures and tools to efficiently process large volumes of data.Familiar with REST for accessing cloud based services.Excellent knowledge about databases or data warehouses, such as PostgreSQL or Redshift.Strong understanding and working knowledge of using SQL (PostgreSQL or Redshift preferred) for various reporting and transformation needs.Comfortable with Linux operating system and command line tools such as Bash.Team player with excellent communication, adaptability and collaboration skills.Experience running Agile methodology and applying SCRUM to data engineering.Experience with GIT and Github.BONUS POINTS IF YOU HAVEExperience working in a cloud-based ecosystem, preferably AWS (including RDS, Redshift, Athena, Glue, etc.).Experience with Apache Hadoop, Hive, Spark or PySpark.Experience with orchestration tools such as Apache Airflow.Experience with data integration tools such as Airbyte. The base salary range represents the low and high end of the anticipated salary range for this position based at our New York City headquarters. The actual base salary offered for this position will depend on numerous factors including individual performance, business objectives, and if the location for the job changes. Our base salary is just one component of Peloton\u2019s competitive total rewards strategy that also includes annual equity awards and an Employee Stock Purchase Plan as well as other region-specific health and welfare benefits.As an organization, one of our top priorities is to maintain the health and wellbeing for our employees and their family. To achieve this goal, we offer robust and comprehensive benefits including:- Medical, dental and vision insurance- Generous paid time off policy- Short-term and long-term disability- Access to mental health services- 401k, tuition reimbursement and student loan paydown plans- Employee Stock Purchase Plan- Fertility and adoption support and up to 18 weeks of paid parental leave\u00a0- Child care and family care discounts- Free access to Peloton Digital App and apparel and product discounts- Commuter benefits and Citi Bike Discount- Pet insurance and so much more!\u00a0Base Salary Range: $140,389 USD - $182,506 USD \u00a0ABOUT PELOTON:Peloton (NASDAQ: PTON) provides Members with expert instruction, and world class content to create impactful and entertaining workout experiences for anyone, anywhere and at any stage in their fitness journey. At home, outdoors, traveling, or at the gym, Peloton brings together innovative hardware, distinctive software, and exclusive content. Founded in 2012 and headquartered in New York City, Peloton has millions of Members across the US, UK, Canada, Germany, Australia, and Austria. For more information, visit www.onepeloton.com.Peloton is an equal opportunity employer and complies with all applicable federal, state, and local fair employment practices laws. Equal employment opportunity has been, and will continue to be, a fundamental principle at Peloton, where all team members, applicants, and other covered persons are considered on the basis of their personal capabilities and qualifications without discrimination because of race, color, religion, sex, age, national origin, disability, pregnancy, genetic information, military or veteran status, sexual orientation, gender identity or expression, marital and civil partnership/union status, alienage or citizenship status, creed, genetic predisposition or carrier status, unemployment status, familial status, domestic violence, sexual violence or stalking victim status, caregiver status, or any other protected characteristic as established by applicable law. This policy of equal employment opportunity applies to all practices and procedures relating to recruitment and hiring, compensation, benefits, termination, and all other terms and conditions of employment.\u00a0 If you would like to request any accommodations from application through to interview, please email: applicantaccommodations@onepeloton.com.Qualified applicants with arrest or conviction records will be considered for employment in accordance with the Los Angeles County Fair Chance Ordinance for Employers and the California Fair Chance Act, the City of Los Angeles Fair Chance Initiative for Hiring Ordinance and the San Francisco Fair Chance Ordinance, as applicable to applicants applying for positions in these jurisdictions.Please be aware that fictitious job openings, consulting engagements, solicitations, or employment offers may be circulated on the Internet in an attempt to obtain privileged information, or to induce you to pay a fee for services related to recruitment or training. Peloton does NOT charge any application, processing, or training fee at any stage of the recruitment or hiring process. All genuine job openings will be posted here on our careers page and all communications from the Peloton recruiting team and/or hiring managers will be from an @onepeloton.com email address.\u00a0If you have any doubts about the authenticity of an email, letter or telephone communication purportedly from, for, or on behalf of Peloton, please email applicantaccommodations@onepeloton.com before taking any further action in relation to the correspondence.Peloton does not accept unsolicited agency resumes. Agencies should not forward resumes to our jobs alias, Peloton employees or any other organization location. Peloton is not responsible for any agency fees related to unsolicited resumes.\n",
    "location": "New York, NY"
  },
  "50": {
    "id": "4178571603",
    "Company": "WeVision LLC",
    "title": "Big Data Engineer",
    "description": "Job Summary: We are seeking a highly skilled and motivated Big Data Engineer to join our client's Big Data Platform team within the streaming domain. This role offers an exciting opportunity to work with large-scale data solutions that drive critical business decisions, including product design, advertising effectiveness, content discovery, and new market expansions. Our team values data as a core asset, leveraging it to enhance user experience, support advertisers, and empower content partners. We are looking for a technically proficient candidate who is eager to innovate and build cutting-edge data solutions.Key Responsibilities:Design, develop, and optimize scalable data solutions.Collaborate with cross-functional teams to drive data-driven decision-making.Implement and maintain batch and real-time data processing frameworks.Ensure data integrity, security, and compliance with industry best practices.Enhance self-service data capabilities for internal stakeholders.Basic Qualifications:Bachelor's degree in Computer Science, Information Systems, Software Engineering, or a related field, or equivalent work experience.Minimum of 3 years of experience in big data engineering, including modeling and developing large data pipelines.Proficiency in programming languages such as Scala, Java, and Python.Strong expertise in Hadoop ecosystem components, including Yarn and Resource Manager.Hands-on experience with distributed systems and batch processing technologies such as Apache Spark.Strong SQL skills for data exploration and analysis.Understanding of Software Development Lifecycle (SDLC) and best practices.Preferred Qualifications:Experience with real-time data processing technologies like Apache Flink.Familiarity with CI/CD pipelines for streamlined development and deployment.Ability to design and develop user interfaces for self-service data access.Strong problem-solving skills and ability to bridge technical and non-technical teams.Experience in integrating complex systems and enhancing software development practices.\n",
    "location": "Irvine, CA"
  },
  "51": {
    "id": "4168724538",
    "Company": "Anthrologic",
    "title": "Data Engineer",
    "description": "Company DescriptionAnthrologic is a team of seasoned marketing and data experts on a mission to help global brands unlock the true potential of AI. We\u2019ve spent decades working at the intersection of technology, data, and marketing\u2014seeing firsthand how clunky systems, underpowered experiments, and overwhelming complexity can stall progress and stifle innovation. Our answer? We blend deep operational know-how with cutting-edge AI strategies that actually deliver value. From advisory services and custom model development to full-solution design and deployment, Anthrologic provides end-to-end support that streamlines repetitive tasks, empowers teams, and elevates strategic thinking.Guided by our mantra\u2014\u201cModernize Data. Humanize Technology. Revolutionize Marketing.\u201c\u2014Anthrologic partners with brands looking to break free from the hype and achieve tangible results. We\u2019re already collaborating with industry-leading platforms like GrowthLoop and NinjaCat to develop enterprise-ready AI agents that handle the nuts and bolts of marketing, freeing your people to focus on creativity and big-picture strategy. Whether you\u2019re seeking to refine your approach or launch transformative AI initiatives, Anthrologic is here to help you succeed in today\u2019s fast-paced digital landscape.Role DescriptionThis is a full-time remote role for a Data Engineer at Anthrologic. The Data Engineer will be responsible for tasks related to data engineering, data modeling, Extract Transform Load (ETL) processes, data warehousing, and data analytics on a daily basis.QualificationsData Engineering and Data Modeling skillsExperience with Extract Transform Load (ETL) processesData Warehousing and Data Analytics skillsProficiency in programming languages such as Python or SQLExperience with platforms like AWS, GCP, Azure Snowflake, Growthloop, HightouchExperience with AI engineering is a must\n",
    "location": "Austin, Texas Metropolitan Area"
  },
  "52": {
    "id": "4169817204",
    "Company": "CEEUS, Inc.",
    "title": "Data Engineer",
    "description": "ORGANIZATIONAL RELATIONSHIPSDepartment | Business SystemsReports To | Director of Business Data & ApplicationsPURPOSEOur business is growing, as is the role we play in the electric utility supply chain. As a key contributor to the organization's data initiatives, the Data Engineer will define and implement strategies for Data Governance, Master Data Management, Data Warehousing, and Data Analytics. This role will build and optimize scalable data pipelines and ensure robust data quality frameworks are in place to empower data-informed decision-making across the organization. The position operates at the intersection of technical expertise and strategic thinking to ensure the company fully leverages its data assets.NATURE AND SCOPEThe Data Engineer will play a critical role in the development and implementation of data disciplines to drive governance, warehousing, and analytics. The responsibilities focus on building infrastructure that supports data stewardship and analytics excellence. The role has three primary objectives to advance CEEUS toward mature and dynamic data management and analytics capabilities:Establishing and managing data governance practices.Developing and maintaining a data warehouse for efficient data storage and retrieval.Implementing analytics focused solutions to extract meaningful insights.QualitiesThe candidates we are looking for will be:Experienced in data architecture, modeling, governance, ETL, and analytics.Self-motivated and passionate about learning, innovation, and continuous improvement.Detailed, analytical, and committed to becoming better.Skilled in providing agile solutions in a dynamic work environment.Proficient in communication and strategic planning skills.JOB RESPONSIBILITIES Data Governance:Support the establishment of a Data Governance framework to ensure consistent data standards and quality.Collaborate with cross-functional teams to implement data policies and maintain compliance.Audit and maintain data accuracy, completeness, and reliability across the organization.Conduct training sessions to enhance data literacy across the organization.Master Data Management:Facilitate the development of clear master data definitions.Design and implement processes for creating, updating, and maintaining master data records across systems.Define and implement master data management strategies to ensure data integrity and alignment across systems.Build pipelines to integrate and standardize data from multiple sources.Data Warehousing:Design, develop, and maintain scalable data warehouse solutions to support analytical workloads.Optimize data pipelines for efficient ingestion, transformation, and storage of structured and unstructured data.Develop scalable data models that support evolving business requirements and large datasets.Automate ETL (Extract, Transform, Load) workflows to improve efficiency and reduce manual effort.Optimize data storage, retrieval, and integration processes.Implement security best practices to protect sensitive data and ensure compliance with data privacy regulations.Data Analytics:Partner with business stakeholders to identify analytical needs and provide actionable insights.Assist with Implementing visualization tools such as Power BI to develop intuitive dashboards and reports.Provide ad hoc reporting by extracting data that can provide insights quickly and efficiently.Collaboration and Communication:Work closely with departments to understand their data needs and provide tailored solutions.Ensure alignment between business stakeholders and technical teams.Working Location and ConditionsLocated at CEEUS Headquarters office in West Columbia, SCNormal office and warehouse environmentStandard working hours of 8:00 AM to 5:00 PM, Monday through Friday with 1 hour for lunchRequires availability outside of standard hours as needed to support critical issues or projectsRequires travel as needed for industry conferences, meetings, and onsite work at our VA facilityEssential Job FunctionsAbility to sit or stand at a workstation for up to six hours at a time.Must possess fine motor skills and finger dexterity necessary for precise and efficient use of a keyboard, mouse, and trackpad in performing job-related tasks for extended periods.Ability to communicate frequently and clearly in both verbal and written formats.Must have the ability to see and interpret detailed information, including small text and images, on screens, paper, or other materials, with or without corrective lenses.Must be able to process complex information, evaluate multiple variables, and apply logical reasoning to solve problems and make informed decisions.\n",
    "location": "West Columbia, SC"
  },
  "53": {
    "id": "4138835331",
    "Company": "Macquarie Group",
    "title": "Data Engineer",
    "description": "The Data Analytics team within our Technology group is building a leading data and analytics platform. We partner and build batch and real-time analytics solutions for traders, quants, desk analysts and central data teams across the Commodities and Global Markets business.At Macquarie, our advantage is bringing together diverse people and empowering them to shape all kinds of possibilities. We are a global financial services group operating in 34 markets and with 55 years of unbroken profitability. You\u2019ll be part of a friendly and supportive team where everyone - no matter what role - contributes ideas and drives outcomes. What role will you play? You will work directly with our business, developing data analytics capabilities and solutions, such as the data catalog, data science workbench, visualisation tools, and real-time analytics. You\u2019ll be working with our public-cloud based data lake, data pipelines for internal operational and external fundamental and alternative market data. You will join us on our journey to continually improve the robustness of our solutions and the speed with which we can respond to our business\u2019s needs.What You OfferInterest in Data and Automation Ability to code and debug in Python Knowledge of GIT and CI/CD Awareness of Cloud (eg. AWS)We love hearing from anyone inspired to build a better future with us, if you're excited about the role or working at Macquarie we encourage you to apply.About Technology Technology enables every aspect of Macquarie, for our people, our customers and our communities. We\u2019re a global team that is passionate about accelerating the digital enterprise, connecting people and data, building platforms and applications and designing tomorrow\u2019s technology solutions.BenefitsMacquarie employees can access a wide range of benefits which, depending on eligibility criteria, include:Hybrid and flexible working arrangements One wellbeing leave day per year and minimum 25 days of annual leavePrimary caregivers are eligible for 20 weeks paid leave along with 12 days of transition leave upon return to work and 6 weeks paid leave for secondary caregiversPaid volunteer leave and donation matchingRange of benefits to support your physical, psychological and financial wellbeing Employee Assistance Program, a robust behavioural health network with counselling and coaching servicesRecognition and service awards Our commitment to diversity, equity and inclusion We are committed to providing a working environment that embraces diversity, equity and inclusion. As an inclusive employer, Macquarie provides equal opportunities to all individuals regardless of race, color, religion, sex, sexual orientation, national origin, age, disability, protected veteran status, genetic information, marital status, gender identity or any other impermissible criterion or circumstance.Our aim is to provide reasonable accommodations to individuals who may need support during the recruitment process and through working arrangements. If you require additional assistance, please let us know during the application process.\n        ",
    "location": "Houston, TX"
  },
  "54": {
    "id": "4172538727",
    "Company": "Definitive Healthcare",
    "title": "Data Engineer",
    "description": "At Definitive Healthcare, our passion is to transform data, analytics and expertise into healthcare commercial intelligence. We help clients uncover the right markets, opportunities and people, so they can shape tomorrow\u2019s healthcare industry. Our SaaS platform creates new paths to commercial success in the healthcare market, so companies can identify where to go next.Our employees are kind, collaborative, energetic, approachable and driven. On top of that, we value the unique perspectives, backgrounds and voices of our employees. Why? Because their diverse experiences drive new ideas and help us build a better community.For over 10 years, we\u2019ve built a collaborative culture driven by employees who share a passion for improving the healthcare ecosystem, enjoy giving back to the local community and value diversity and inclusion.One of the hallmarks of our culture is our commitment to community service. Through the DefinitiveCares program, employees can work with their choice of more than 40 charitable organizations, supporting causes from hunger and homelessness to healthcare, LGBTQ+ issues, racial justice, women\u2019s initiatives and more. 2021 marked the sixth year that we had 100% employee participation in DefinitiveCares.We also provide a range of opportunities for employees to connect with each other. Employees can join any of our employee run affinity groups supporting causes such as women\u2019s empowerment, LGBTQ+, Black, indigenous and people of color (BIPOC), disabilities and working parents and potential for many more. Affinity groups often enable greater education companywide through training, events and speaker series.We\u2019re also a great place to work. For five years in a row, we\u2019ve been recognized by the Boston Business Journal and the Boston Globe as a best place to work in Massachusetts. In 2022, Energage recognized us for Culture Excellence in Compensation & Benefits, Innovation, Great Leadership, Purpose & Value and Work-Life Flexibility!Think you\u2019d be a good addition to our team? Explore our available positions here. We\u2019d love the chance to get to know you.We are looking for a talented Data Engineer to join our team and help us build and maintain robust data infrastructure and pipelines. If you are passionate about data and have a strong background in Python, Spark, AWS, SQL, SSIS, and related technologies, we want to hear from you!Responsibilities:  Design and Develop Data Pipelines: Build and maintain scalable data pipelines using Python, Spark, Databricks , SQL, and SSIS.Implement data workflows and ETL processes using Apache Airflow and SSIS. Data Integration and Management: Integrate data from various sources (AWS, on-premises) into a unified data warehouse.Handle variety of data formats such as csv, text, xml, parquet, delta etc.,Ensure data quality and integrity through effective data cleansing and curation practices.Manage and optimize data storage solutions, ensuring high availability and performance.Automate observability of data and workloads Metadata Management and Governance: Implement and manage Unity Catalog for metadata management.Ensure data governance policies are followed, including data security, privacy, and compliance.Develop and maintain data documentation and data dictionaries.Automate data observability across pipelines Performance Tuning and Troubleshooting: Optimize Spark jobs for performance and efficiency.Investigate and resolve performance bottlenecks in Spark applications.Utilize JVM tuning techniques to improve application performance. Data Maturity Lifecycle: Implement and manage the Medallion architecture for data maturity lifecycle.Ensure data is appropriately processed and categorized at different stages (bronze, silver, gold) to maximize its usability and value. Collaboration and Continuous Improvement: Work closely with data scientists, analysts, and other stakeholders to understand data needs and deliver solutions.Implement CI/CD pipelines to automate deployment and testing of data infrastructure.Stay up-to-date with the latest industry trends and technologies to continuously improve data engineering practices.Required Skills and Qualifications:  Technical Skills: Hands-on SQL, Python or Scala programming.Strong experience with SSIS, Apache Spark and Databricks.Hands-on experience with Apache Airflow or similar workflow orchestration tools.Knowledge of data cleansing and curation techniques.Familiarity with Unity Catalog or other metadata management tools.Understanding of data governance principles and best practices.Experience with cloud platforms (AWS).Proficiency in CI/CD tools and practices (e.g., Jenkins, GitLab CI, etc.).Experience with JVM tuning and Spark job performance investigation.Experience with Medallion architecture for data maturity lifecycle. Soft Skills: Excellent problem-solving and analytical skills.Strong communication and collaboration skills.Ability to work independently and as part of a team.Detail-oriented with a focus on delivering high-quality work.Preferred Qualifications: Certification in cloud platforms (AWS Certified Data Analytics, etc.). Familiarity with SQL and NoSQL databases. Experience in a similar role within a fast-paced, data-driven environment. Why we love Definitive, and why you will too!Industry leading productsWork hard, and have fun doing itIncredibly fast growth means limitless opportunityFlexible and dynamic cultureWork alongside some of the most talented and dedicated teammatesDefinitive Cares, our community service group, gives all of us a chance to give backCompetitive benefits package including great healthcare benefits and a 401(k) matchWhat our Employees are saying about us on Glassdoor: \u201cGreat Work atmosphere, great work life balance, excellent company to work for, amazing top notch product, incredible customer service, lots of tools to help you succeed.\u201dBusiness Development Manager\u201cGreat team. Amazing growth. Employees are treated very well.\u201dResearch Analyst\u201cI have waited 36 years to work at a dream job for a dream company and I am so happy to have finally got there.\u201dProfile AnalystIf you don\u2019t fit all of these qualifications, but believe you\u2019re still a great fit, feel free to apply and tell us why in your cover letter.If you are a California, Colorado, New York City or Washington resident and this role is a remote role, you can receive additional information about the compensation and benefits for this role, which we will provide upon request.Definitive Hiring PhilosophyDefinitive Healthcare is an equal opportunity employer that celebrates diversity and is committed to creating an inclusive workplace with equal opportunity for all applicants and teammates. Our goal is to recruit the most talented people from a diverse candidate pool regardless of race, color, religion, age, gender, gender identity, sexual orientation or any other status. If you\u2019re interested in working in a fast growing, exciting working environment \u2013 we encourage you to apply!Privacy Your privacy is important to us. Please review our Candidate Privacy Notice which tells you how we use and process your personal informationPlease note: All communications regarding the hiring process at Definitive Healthcare will come directly from one of our corporate recruiters or coordinators with an @definitivehc.com email address. We will never request any money transfer or purchase of equipment with a promise of reimbursement. If you receive any suspicious communications, please reach out to careers@definfitivehc.com to confirm your status in the application process.\n",
    "location": "Framingham, MA"
  },
  "55": {
    "id": "4140108850",
    "Company": "Sabre Corporation",
    "title": "Data Science Engineer - 2025 Summer Intern",
    "description": "Sabre is seeking a talented Data Science Engineer Intern to support the Data and Analytics Team. We design and develop analytics for internal stakeholders and customers who use our SynXis Insights products. This internship provides a fantastic opportunity to gain hands-on experience in data science and engineering, work on impactful projects, and contribute to our company\u2019s success.Role And ResponsibilitiesAssist in the collection, processing, and analysis of large datasetsDevelop and implement machine learning models and algorithmsCreate data visualizations and dashboards to communicate findingsPerform exploratory data analysis to uncover insights and trendsPreferred Qualifications And EducationCurrently pursuing a Bachelor's or Master's degree in Data Science, Computer Science, Statistics, or a related field with graduation after May 2025Knowledge of SQL and experience with databases is desirableFamiliarity with PythonExperience with data visualization tools (Tableau, Power BI, Google Looker)\n",
    "location": "Southlake, TX"
  },
  "56": {
    "id": "4173328622",
    "Company": "CoBank",
    "title": "Data Engineer",
    "description": "Benefits OverviewA career at CoBank can offer you the opportunity to make a personal impact on the people and communities where we do business. When you choose a career with CoBank, you make a difference by standing for something that matters. In order to be the best, we hire the best!Remarkable Benefits Offered By CoBankCareers with a purpose. Stand for something!Time-Off Packages, boasting over 15 days of vacation, 10 paid sick days and 11 paid holidaysCompetitive Compensation & IncentiveHybrid work model: flexible arrangements for most positionsBenefits Packages, including Medical, Dental and Vision coverage, Disability, AD&D, and Life InsuranceRobust associate training and development with CoBank UniversityTuition reimbursement for higher education up to $10,000 per yearOutstanding 401k: up to 6% matching and additional 3% non-elective contributionCommunity Impact: United Way Angel Day, Volunteer Day and Associate Directed ContributionAssociate Resource Groups: creating a culture of respect and inclusionRecognize a fellow associate through our GEM awardsJob DescriptionCoBank's Data Engineering (DE) team is looking for a Data Engineer to design, build, and optimize data pipelines that power our data-driven strategies. You will work closely with cross-functional teams to ensure the efficient and secure movement of data across the organization. This role requires strong technical skills and a passion for building scalable data solutions that align with business objectives.Essential Functions Design and implement ETL processes for complex data integration tasks. Develop and optimize data models for cloud and on-premises environments. Collaborate with business and engineering teams to gather and understand data requirements. Contribute to the maintenance and enhancement of data warehouses and data lakes. Education Bachelor's Degree in computer science, information systems, or a relevant discipline. preferredWork Experience3 years - 5 years of relevant experience in data engineering or a related field, required Proficiency in ETL processes and data modeling techniques. Strong knowledge of database design, optimization, and cloud platforms (especially AWS). Excellent collaboration and communication skills. Core Competencies: advanced problem-solving and analytical skills; agile project management and teamwork; leadership potential and initiative. Experience with AWS services and other cloud environments (preferred). Involvement in open-source projects related to data engineering (preferred).About CoBankThe typical base pay range for this role is between $89,600 - $110,160. Compensation may vary based on individual job-related knowledge, skills, expertise, and experience. This position is eligible for a discretionary annual incentive program driven by organization and individual performance.The listed salary, other compensation and benefits information is accurate as of the date of this posting. This job will be posted for a minimum of five (5) business days or until the position is filled. CoBank reserves the right to adjust compensation for all positions and to modify or discontinue benefits programs at any time in its sole discretion, subject to applicable law.CoBank is a cooperative bank serving vital industries across rural America. The bank provides loans, leases, export financing and other financial services to agribusinesses and rural power, water and communications providers in all 50 states. The bank also provides wholesale loans and other financial services to affiliated Farm Credit associations serving more than 76,000 farmers, ranchers and other rural borrowers in 23 states around the country. CoBank is a member of the Farm Credit System, a nationwide network of banks and retail lending associations chartered to support the borrowing needs of U.S. agriculture, rural infrastructure and rural communities. Headquartered outside Denver, Colorado, CoBank serves customers from regional banking centers across the U.S. and also maintains an international representative office in Singapore.REASONABLE ACCOMMODATIONWe are committed to ensuring that our online application process provides an equal employment opportunity to all applicants, including qualified individuals with disabilities. If you are an applicant with a disability, or are assisting an applicant with a disability, and require accessibility assistance or would like to request a reasonable accommodation for any aspect of the application process, including completing an application, interviewing, or otherwise participating in the employee selection process, please submit a request by emailing recruiting@cobank.com. Include your contact information and specific details about your requested accommodation.Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment visa at this time.CoBank is an Equal Opportunity Employer.All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, national origin, disability, or status as a protected veteran.\n        ",
    "location": "Greenwood Village, CO"
  },
  "57": {
    "id": "4176115198",
    "Company": "Preferred Travel Group",
    "title": "Data Engineer I",
    "description": "General SummaryThe Data Engineer I assists in building and maintaining the company\u2019s data systems, including writing queries, automating data workflows, and creating integrations to support various business functions. This role provides an opportunity to learn under the guidance of senior team members.ORGANIZATIONAL RELATIONSHIPUnder the direct supervision of the Director of Data, AI, and Integrations, the Data Engineer works closely with other engineers in the Data and QA teams.Duties & ResponsibilitiesData EngineeringWrite SQL queries and Python scripts to manipulate and transform data.Create and maintain automated workflows using tools like Azure Data Factory, Jupyter Notebooks, or SQL Agent.Integration DevelopmentSupport the development of web APIs (SOAP, REST) for data sharing between systems.Reporting and VisualizationBuild reports, dashboards, and data visualizations using SSRS, Power BI, and Excel.Operational SupportFollow procedural checklists for recurring processes.Troubleshoot issues reported by users and implement fixes.CollaborationManage tasks in the ticketing system and provide updates during stand-ups.Commit work to shared Git repositories with proper version control practices.Participate in team code reviews.QualificationsBachelor\u2019s degree in Computer Science, Information Systems, or equivalent experience (e.g., bootcamps, certifications, internships).Familiar with at least one relevant language, such as SQL, Python, or C#Strong problem-solving skillsAbility to learn new tools quicklyWORKING CONDITIONSThis is a work from home position, can be hybrid if desired and near our locations in Chicago, IL or Newport Beach, CA. All technology required will be provided.Required TrainingOrientation via some live remote and some pre-recorded video sessionsIT security trainingInternal development process and proceduresDISCLAIMER: The above information on this description has been designed to indicate the general nature and level of work performed by associates within this classification. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job.Salary range: $55,000-$70,000 annually; actual compensation within this range will be determined by multiple factors including candidate location, experience and expertise.\n        ",
    "location": "Newport Beach, CA"
  },
  "58": {
    "id": "4184545931",
    "Company": "Amount",
    "title": "Data Engineer",
    "description": "Amount provides a unified digital origination and decisioning platform that helps financial institutions meet the moment. Designed to scale with banks and credit unions at any stage of their digital journey, Amount delivers a seamless, digital-first experience\u2014streamlining everything from loan origination to deposit account opening. With built-in fraud orchestration and risk management, Amount enables financial institutions to control risk across any product while optimizing performance and enhancing security. Our flexible, modular platform is backed by enterprise-grade infrastructure and compliance, allowing institutions to launch new offerings in months, not years. Amount\u2019s clients include financial institutions collectively managing over $3.1T in assets and serving more than 50 million U.S. consumers. Learn more at www.amount.com.Inclusion, diversity, and belonging are core to Amount's values, and we believe they are more than words, they are actions. We support our commitment to these ideas by empowering intrepid engagement and learning, increasing diverse representation, and fostering a culture where everyone can bring their full self to work without regard to differences. We look for people who embrace this culture.A DAY IN THE LIFE:As a Data Engineer, you will be enhancing, hardening, and supporting Amount\u2019s Data-related systems such as data pipelines, OLAP data stores, reporting dashboards, and client-facing Data API.Amount has multiple client-facing platforms including our brand-new, 3rd generation lending platform and the Data Engineering team is responsible for keeping data flowing consistently, accurately, and reliably from these platforms to our clients and internal stakeholders. Scaling the data pipelines of the new Amount platform is an especially interesting challenge.You will use tools such as Python, Airflow, Argo, Spark, Trino, and Looker as well as AWS products including Redshift, S3, EMR, and Aurora to build a world-class data platform for internal and external customers.Salary: $84,000 - $95,000WHAT WE\u2019LL TRUST YOU TO DELIVER:Improvements to existing data pipelines eg. new fields, performance improvements, new data source integrations, etc.Design new features for the Data Platform as well as improve scalability and reliability,Operational support for the data platform to ensure all data is available within defined SLAsCollaboration with the Product team and other engineering teams to plan and execute projectsQuality code, robust designs, and innovative ideasWHAT YOU LIKELY BRING TO THE TABLE: Bachelor\u2019s degree in Computer Engineering, Information Technology, Information Systems, Computer Science, or a related discipline or equivalent workStrong software engineering skillsAt least 1 year of experience building data pipelines using Python, SQL, and other common data platform technologiesAptitude for learning new technologies and analytics techniquesExperience with Java and/or Ruby is a plusABOUT AMOUNT (TL;DR)Founded: 2020Employees: 150+Locations: Chicago (HQ) and US RemoteFunding: Amount has raised $281M in total equity capital since inception, including most recently at a valuation of $1B. Investors include WestCap, Hanaco Ventures, Goldman Sachs, Invus Opportunities, Mastercard, and PSCUPress: Amount Blog - Equity Raise, Business Wire, PR Newswire - Comerica, PYMNTS - PSCU, A Year in Review - Amount\u2019s Blog, Builtin, The Financial BrandKey Customers: TD Bank, Velera, Guaranteed Rate, American Express, Citizens, Fifth Third Bank, Bank United, FIS, Associated Bank, ComericaSocial Media: LinkedIn, Builtin, Twitter, Amount BlogTech Stack: Greenhouse, LinkedIn Recruiter, Lattice, G Suite, Atlassian, AWS, Python, Java, Ruby, GO, node.js, Temporal, Scala, Apache NiFi, Talend, Informatica, Hadoop, Hive, Spark, Pandas, Looker, Argo, Airflow Luigi, Kubernetes, C#, JavaScript (for advanced concepts), ASP.NET MVC, .NET Core, Microsoft SQL Server, Entity Framework (ORM for database interaction)\n        ",
    "location": "Chicago, IL"
  },
  "59": {
    "id": "4168663534",
    "Company": "Anheuser-Busch",
    "title": "Data Engineer",
    "description": "Dreaming big is in our DNA. It\u2019s who we are as a company. It\u2019s our culture. It\u2019s our heritage. And more than ever, it\u2019s our future. A future where we\u2019re always looking forward. Always serving up new ways to meet life\u2019s moments. A future where we keep dreaming bigger. We look for people with passion, talent, and curiosity, and provide them with the teammates, resources and opportunities to unleash their full potential. The power we create together \u2013 when we combine your strengths with ours \u2013 is unstoppable. Are you ready to join a team that dreams as big as you do?SALARY: $78,200 - $108,900, bonus eligibleCOMPANY:Michelob ULTRA. Cutwater Spirits. Budweiser. Kona Brewing Co. Stella Artois. Bud Light. That\u2019s right, over 100 of America\u2019s most loved brands, to be exact. But there\u2019s so much more to us than our top-notch portfolio of beers, seltzers, and more. We are powered by a 19,000-strong team that shares our passion to create a future with more cheers. We look for people with talent, curiosity, and commitment and provide the teammates, resources and opportunities to unleash their full potential. The power we create together \u2013 when we combine your strengths with ours \u2013 is unstoppable. Are you ready to join a team that dreams as big as you do?ROLE SUMMARY: In this role you will work at the intersection of understanding business problems so that you can build data assets to support products that invoke people to take data driven decisions. This will entail developing a deep understanding of existing processes, data that supports the processes and high frequency iterations to ship aesthetically compelling visualizations and reporting products. You will also have the opportunity to influence architecture and technology to promote the quality and innovation vital for industry changing data driven decisions at all levels of the company.JOB RESPONSIBILITES: Meaningfully relate data for key products across the companyDaily interactions with Product, Data Science and Software Engineering teams to iterate quickly and constantly change / update / add visualizations to our reporting products. Build data models using the latest cloud technologies that support fast, low maintenance and scalable problem solving. Build stable pipelines and support Data Science and the Front End team on all data needs. JOB QUALIFICATIONS: SQL, Python, PysparkDatabricks, Unity CatalogSnowflakeAzure DevOpsDatabricks workflows, AirflowDatadogLucidchartExtra Credit:TypescriptKafka protocolAzure functions, Azure logic appsAzure DatafactoryPowerAppsDocker, Kubernetes#AC-1\n        ",
    "location": "St Louis, MO"
  },
  "60": {
    "id": "3998841067",
    "Company": "PepsiCo",
    "title": "Data Engineer",
    "description": "OverviewPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo\u2019s global business scale to enable business insights, advanced analytics and new product development. PepsiCo\u2019s Enterprise Data Operations (EDO) team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.What PepsiCo Enterprise Data Operations (EDO) does:Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the companyResponsible for day-to-day data collection, transportation, maintenance/curation and access to the PepsiCo corporate data asset Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholdersIncrease awareness about available data and democratize access to it across the company As a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premise data sources as well as cloud and remote systems.ResponsibilitiesActive contributor to code development in projects and services.Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.Responsible for implementing best practices around systems integration, security, performance and data management.Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.Develop and optimize procedures to \u201cproductionalize\u201d data science models.Define and manage SLA\u2019s for data products and processes running in production.Support large-scale experimentation done by data scientists.Prototype new approaches and build solutions at scale.Research in state-of-the-art methodologies.Create documentation for learnings and knowledge transfer.Create and audit reusable packages or libraries.Compensation & Benefits:The expected compensation range for this position is between $100,000 - $167,500 based on a full-time schedule.Location, confirmed job-related skills and experience will be considered in setting actual starting salary.Bonus based on performance and eligibility; target payout is 8% of annual salary paid out annually.Paid time off subject to eligibility, including paid parental leave, vacation, sick, and bereavement.In addition to salary, PepsiCo offers a comprehensive benefits package to support our employees and their families, subject to elections and eligibility: Medical, Dental, Vision, Disability, Health and Dependent Care Reimbursement Accounts, Employee Assistance Program (EAP), Insurance (Accident, Group Legal, Life), Defined Contribution Retirement Plan.Qualifications4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture. 3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).2+ years in cloud data engineering experience in AzureExperience with Azure Data Factory, Azure Databricks and Azure Machine learning tools is a plus.Fluent with Azure cloud services. Azure Certification is a plus.Experience with integration of multi cloud services with on-premises technologies.Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.Experience with at least one MPP database technology such as Redshift, Synapse or SnowFlake.Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.Experience with version control systems like Github and deployment & CI tools.Experience with Statistical/ML techniques is a plus.Experience with building solutions in the retail or in the supply chain space is a plusUnderstanding of metadata management, data lineage, and data glossaries is a plus.Working knowledge of agile development, including DevOps and DataOps concepts.Familiarity with business intelligence tools (such as PowerBI). EducationBA/BS in Computer Science, Math, Physics, or other technical fields.Skills, Abilities, KnowledgeExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.Proven track record of leading, mentoring data teams.Strong change manager. Comfortable with change, especially that which arises through company growth. Able to lead a team effectively through times of change.Ability to understand and translate business requirements into data and technical requirements.High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment. Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.Foster a team culture of accountability, communication, and self-management.Proactively drives impact and engagement while bringing others along.Consistently attain/exceed individual and team goalsAbility to lead others without direct authority in a matrixed environment. CompetenciesHighly influential and having the ability to educate challenging stakeholders on the role of data and its purpose in the business. Understands both the engineering and business side of the Data Products released. Places the user in the center of decision making. Teams up and collaborates for speed, agility, and innovation. Experience with and embraces agile methodologies. Strong negotiation and decision-making skill. Experience managing and working with globally distributed teams.EEO StatementAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender IdentityIf you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy.Please view our Pay Transparency Statement\n        ",
    "location": "Purchase, NY"
  },
  "61": {
    "id": "4180448942",
    "Company": "TieTalent",
    "title": "AWS Python Data Engineer",
    "description": "AboutAWS Python Data EngineerCategoryBusiness Consulting, Strategy and Digital TransformationMain location:, Virginia, RestonAlternate Location(s):United States, Texas, PlanoPosition IDJ0225-1808Employment TypeFull TimeU.S. - Finding purpose at CGI (https://youtu.be/7gAvvUSJyJA)By playing this video you consent to Google/YouTube processing your data and using cookies \u2013Learn more (xweb.asp?clid=21001&page=cookiespolicy#integrationofyoutube) .Position DescriptionCGI has an immediate need for a AWS Python Data Engineer to join our team. This is an exciting opportunity to work in a fast-paced team environment supporting one of the largest leaders in the secondary mortgage industry. We take an innovative approach to supporting our client, working side-by-side in an agile environment using emerging technologies. This position is located in Reston, VA or Plano, TX. A hybrid working model is acceptable. We partner with 15 of the top 20 banks globally, and our top 10 banking clients have worked with us for an average of 26 years! We have over 92,000+ CGI Members in 40 countries and over 5k+ loyal Clients who are leveraging our end-to-end services across the globeYour Future Duties And Responsibilities Data Architecture & Engineering Design, implement, and maintain scalable data pipelines and data lakes using AWS services such as Glue, Lambda, Step Functions, S3, Redshift, and DynamoDB. Develop real-time and batch data processing solutions using Apache Spark, Kinesis, and Kafka. Optimize data storage, retrieval, and compute efficiency across AWS resources. ETL & Data Integration Build and manage ETL/ELT pipelines using AWS Glue, Step Functions, or other orchestration tools. Integrate data from various sources (APIs, databases, third-party platforms) into centralized storage (e.g., S3, Redshift, or Snowflake). Implement data transformation and quality checks to ensure accuracy, consistency, and compliance. Data Governance & Security Enforce data security and compliance standards using AWS IAM, KMS, and Lake Formation. Implement data cataloging, lineage tracking, and metadata management with AWS Glue Data Catalog. Monitor, tune, and optimize query performance on Redshift, Athena, and other databases. Collaboration with stakeholdersRequired Qualifications To Be Successful In This Role 8+ years of experience in data engineering, with at least 5+ years of hands-on AWS experience. Expertise in AWS services such as Glue, Redshift, Athena, S3, Lambda, Step Functions, EMR, and Kinesis. Strong proficiency in SQL, Python, and Spark for data processing and transformations. Experience with data modeling, warehousing (Redshift, Snowflake), and distributed computing frameworks. Familiarity with IaC (Terraform, CloudFormation) Experience with CI/CD pipelines for data workflows (e.g., GitHub Actions, CodePipeline, or Airflow). Knowledge of data governance, security best practices, and compliance frameworks. Strong analytical and problem-solving skills with the ability to optimize complex queries and large-scale data processing. AWS Data Engineer or related certification is a plusEducationBachelor's degree in computer science, Information Systems or related fieldCGI is required by law in some jurisdictions to include a reasonable estimate of the compensation range for this role. The determination of this range includes various factors not limited to skill set, level, experience, relevant training, and licensure and certifications. To support the ability to reward for merit-based performance, CGI typically does not hire individuals at or near the top of the range for their role. Compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range for this role in the U.S. is $66,300.00 - $146,900.00.BenefitsCGI\u2019s benefits are offered to eligible professionals on their first day of employment to include:Competitive compensationComprehensive insurance optionsMatching contributions through the 401(k) plan and the share purchase planPaid time off for vacation, holidays, and sick timePaid parental leaveLearning opportunities and tuition assistanceWellness and Well-being programsSkillsContinuous IntegrationGitHubPythonSQLTerraformWhat You Can Expect From UsTogether, as owners, let\u2019s turn meaningful insights into action.Life at CGI is rooted in ownership, teamwork, respect and belonging. Here, you\u2019ll reach your full potential because\u2026You are invited to be an owner from day 1 as we work together to bring our Dream to life. That\u2019s why we call ourselves CGI Partners rather than employees. We benefit from our collective success and actively shape our company\u2019s strategy and direction.Your work creates value. You\u2019ll develop innovative solutions and build relationships with teammates and clients while accessing global capabilities to scale your ideas, embrace new opportunities, and benefit from expansive industry and technology expertise.You\u2019ll shape your career by joining a company built to grow and last. You\u2019ll be supported by leaders who care about your health and well-being and provide you with opportunities to deepen your skills and broaden your horizons.Come join our team\u2014one of the largest IT and business consulting services firms in the world.Qualified applicants will receive consideration for employment without regard to their race, ethnicity, ancestry, color, sex, religion, creed, age, national origin, citizenship status, disability, pregnancy, medical condition, military and veteran status, marital status, sexual orientation or perceived sexual orientation, gender, gender identity, and gender expression, familial status or responsibilities, reproductive health decisions, political affiliation, genetic information, height, weight, or any other legally protected status or characteristics.CGI provides reasonable accommodations to qualified individuals with disabilities. If you need an accommodation to apply for a job in the U.S., please email the CGI U.S. Employment Compliance mailbox at US_Employment_Compliance@cgi.com . You will need to reference the Position ID of the position in which you are interested. Your message will be routed to the appropriate recruiter who will assist you. Please note, this email address is only to be used for those individuals who need an accommodation to apply for a job. Emails for any other reason or those that do not include a Position ID will not be returned.We make it easy to translate military experience and skills! Clickhere (https://cgi-veterans.jobs/) to be directed to our site that is dedicated to veterans and transitioning service members.All CGI offers of employment in the U.S. are contingent upon the ability to successfully complete a background investigation. Background investigation components can vary dependent upon specific assignment and/or level of US government security clearance held. Dependent upon role and/or federal government security clearance requirements, and in accordance with applicable laws, some background investigations may include a credit check. CGI will consider for employment qualified applicants with arrests and conviction records in accordance with all local regulations and ordinances.CGI will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with CGI\u2019s legal duty to furnish information.Nice-to-have skillsAWS LambdaKafkaSQLPythonTerraformTexas, United StatesWork experienceData EngineerData InfrastructureLanguagesEnglish\n",
    "location": "Texas, United States"
  },
  "62": {
    "id": "4184889506",
    "Company": "Salesforce",
    "title": "Summer 2025 Intern - Data Engineer",
    "description": "To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.About Futureforce University RecruitingOur Futureforce University Recruiting program is dedicated to attracting, retaining and cultivating talent. Our interns and new graduates work on real projects that affect how our business runs, giving them the opportunity to make a tangible impact on the future of our company. With offices all over the world, our recruits have the chance to collaborate and connect with fellow employees on a global scale. We offer job shadowing, mentorship programs, talent development courses, and much more.Job CategoryFixed Term & TemporaryJob DetailsAbout SalesforceWe\u2019re Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too \u2014 driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing well and doing good \u2013 you\u2019ve come to the right place.START DATE: Summer 2025Internship DatesGroup 1 May 19th - August 8thGroup 2 June 2nd - August 22ndGroup 3 June 16th - September 5thOur headcount demand is always changing as we grow; start dates listed here may or may not have an immediate opening at the time of your application.LOCATIONS WE HIRE FOR: Seattle WA, San Francisco CAOur headcount demand is always changing as we grow; some of the locations listed here may or may not have an immediate opening at the time of your application. Salesforce\u2019s Employee Success (ES) Product Management team is focused on advancing AI-driven data initiatives to transform and expand our data capabilities across all ES functions. A key priority in this effort is developing a strong data and AI foundation to support innovation, automation, and data-driven decision-making across the ES landscape.Job SummaryWe are seeking a motivated and enthusiastic Data Engineering Intern to join our team. This internship offers a unique opportunity to gain hands-on experience in building and maintaining data driven products in a fast-paced environment. The ideal candidate will have a strong foundation in Python and SQL, a solid understanding of data engineering concepts, and a passion for learning.ResponsibilitiesAssist in the design, development, and maintenance of data pipelines and ETL processes.Work with large datasets to build and optimize data models and data warehouses.Collaborate with data engineers and analysts to integrate data from various sources.Develop and maintain SQL queries and scripts for data extraction, transformation, and loading.Collaborate with team members to troubleshoot and resolve data-related issues.Document data engineering processes and procedures.Potentially assist in the exploration and implementation of AI related data projects.Required Skills And QualificationsCurrently pursuing a Bachelor's degree in Computer Science, Data Science, Engineering, or a related field (Spring 2026 Grad)Strong proficiency in Python and SQL.Proficiency using GIT for code standardization and versioning.Strong understanding of data warehouse concepts(e.g., star schema, snowflake schema), data models, and data pipelines.Familiarity with ETL (Extract, Transform, Load) principles.Basic knowledge of data integration and data modeling.Ability to write clean, efficient, and well-documented code.Strong problem-solving and analytical skills.Excellent communication and teamwork skills.Ability to learn quickly and adapt to new technologies.Preferred Skills And QualificationsExperience with data science or machine learning concepts.Experience with AWS technologies like ECS, Aurora, Lambda, S3 preferred.Experience with data pipeline tools (e.g., Apache Airflow).Knowledge of building AI agents.Experience with data visualization tools (e.g., Tableau, Power BI).AccommodationsIf you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form.Posting StatementAt Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at www.equality.com and explore our company benefits at www.salesforcebenefits.com.Salesforce is an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce does not accept unsolicited headhunter and agency resumes. Salesforce will not pay any third-party agency or company that does not have a signed agreement with Salesforce.\ufeffSalesforce welcomes all.Pursuant to the San Francisco Fair Chance Ordinance and the Los Angeles Fair Chance Initiative for Hiring, Salesforce will consider for employment qualified applicants with arrest and conviction records.For Washington-based roles, the base salary hiring range for this position is $50 to $50.For California-based roles, the base salary hiring range for this position is $54 to $54.Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Certain roles may be eligible for incentive compensation, equity, benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com.\n        ",
    "location": "Seattle, WA"
  },
  "63": {
    "id": "4177769620",
    "Company": "Double Good",
    "title": "Data Engineer",
    "description": "Double Good's mission is to create joy. We create joy with our delectable and award-winning popcorn. We create joy with our easy-to-use fundraising platform that raises a meaningful amount of money for youth sports and activities, empowering kids to pursue their dreams. We create joy through our Kids Foundation which hosts Double Good Days events across the country to bring all-ability fun to children with special needs and their families. As featured on the Today Show, Double Good is not just about the product; we have a strong social mission.In recent years, Double Good has seen 40% year over year growth, and we're excited about our future! We're excited about the possibility of you joining our mission. We are looking for a Data Engineer to join our growing Data team.In this pivotal role, you will be part of a world-class data team that is focused on empowering decision-makers and enhancing operations. We thrive on collaboration, creativity, problem-solving, and a drive to add value.Location - This role will be hybrid out of our downtown Chicago officeWhy Join Us: Join a rockstar team and use leading technologies to make a huge impact on critical company initiatives.Enjoy a stellar company culture, competitive salary and great benefits.What the data team does: We focus on building data assets that are impactful to the business. We select and configure tools to support reliable data operations and availability of data assets to business users. We collaborate with each other and across the whole organization to connect systems, connect dots and share knowledge. We build data stores, pipelines, databases, data models, metrics, visualizations, and all the things that make it a reliable, secure, well-managed data platform.What you'd focus on: Your role will be to help design, build and manage the data platform, pipelines and models that support strategic analysis, BI, ML and AI. As you build, you will also help make it more reliable, secure and well-managed by proactively identifying and implementing improvements in the platform.Experience & Skills we value:A solid track record with at least 5 years in data engineering.Proven experience and self-motivation in taking initiative to identify needs, moving projects forward and delivering results.Exceptional communication skills that promote effective teamwork.Deep understanding of Kimball and OBT-style data modeling, with a complimentary drive to understand the business and create optimal models for BI and ML use cases. Expertise in dbt and Airflow, with deep experience in SQL, Python, pipeline development, orchestration and deployment. Experienced in Snowflake administration, with a solid understanding of performance optimization and RBAC.Excellent problem-solving capabilities with a flair for analyzing the business and systems impact.Nice to have and/or you'll learn: Being comfortable developing in IaC tools, like TerraformBeing comfortable working with offshore DevOps teams to make requests for networking changes, AWS services, and permissionsEqual Pay Disclosure(s):We're on a mission to create more joy in people's lives, and that includes our internal employees. We create a place people love to be a part of, where people can discover and practice their unique skill sets, a place where they can contribute and do their best work. We do this by offering our employees a competitive compensation & benefits plan.Base Pay range for this position:$90,000 - $130,000Target Annual Bonus: 15% of base salaryThe final discretionary compensation that will be offered for this role depends on a variety of factors, including job-related knowledge, skills, experience, and market location.Tool Appendix:Our strategy for data platform tool selection is to use hosted/SaaS tools when possible. For us, this means that the data engineer role does less with infrastructure but more with data pipelines and data models.We work with a diverse set of tools, as shown below. While familiarity with all is beneficial, we value strong technical skills and a willingness to learn new technologies.CategoryTool/TopicIngestionFivetran, but looking at other solutionsDataSnowflake, Snowpark, AWSOrchestrationAirflowTransformationDbt, SQL, PythonData modeling:Kimball, metrics, OBTSemantic/metrics layer:Cube.devBIDomo, but looking at other solutionsData catalogLooking at solutionsObservabilityElementary, Slack Notifications, ReportsCI/CDGithub ActionsIaCTerraform, Titan for SnowflakeOtherSecurity, Users, Roles and Permissions in SnowflakeTeamGoogle Workspaces, Microsoft coming soon, Jira, Confluence, Azure DevOps, Slack for company commsSourcesMicrosoft D365 F&O, Postgres, APIs, third-partiesBenefits:Double Good offers competitive benefits including medical, dental and vision coverage with plans that can fit each teammate's needs. We offer immediate vesting in our 401k plan, paid time off, company-paid leaves and other perks including a Popcorn Allowance (yup, free popcorn!).Visit the Careers page on our website for more information at https://www.doublegood.com/careers.Double Good is an Equal Opportunity and Affirmative Action employer, working in compliance with both federal and state laws. We are committed to the concept of Equal Employment opportunity. Qualified candidates will be considered for employment regardless of race, color, religion, age, sex, national origin, marital status, medical condition, or disability.\u202f The EEO is the law and is available here. Right to Work Statement (English and Spanish).\n",
    "location": "Chicago, IL"
  },
  "64": {
    "id": "4181990464",
    "Company": "Concentrix Catalyst",
    "title": "Data Engineer",
    "description": "Job Title:Data EngineerJob DescriptionData Engineer Job Ref #: 006081Concentrix CVG Customer Management Group Inc. has multiple openings for the position of Data Engineer. Work may be performed in various unanticipated locations throughout the U.S. depending on specific project needs. Travel and/or relocation may be required. Telecommuting may be permitted.The Data Engineer will write, update, and maintain software applications; perform production maintenance of code; gather solutions requirements. Own technical commitments to clients and work with the team to successful delivery of solutions. Analyze, design, and code for complex requirements as well as write programs of complexity. Responsible for defining problems, collecting data, establishing facts, drawing valid conclusions, and preparing appropriate reports.The position requires a Master\u2019s degree in Computer Science, Data Science, Math, or any technical/analytical field that is closely related to the specialty. Must have knowledge of (may be gained concurrently with or as part of required education): Network architectures and protocols, including TCP/IP protocol architecture; database security; Hadoop; cloud computing; and cryptography. Permanent U.S. work authorization is required for this position (no visa sponsorship available).The base salary range for this position is $95,139 - $137,186, plus incentives that align with individual and company performance. Actual salaries will vary based on work location, qualifications, skills, education, experience, and competencies. Benefits available to eligible employees in this role include medical, dental, and vision insurance, comprehensive employee assistance program, 401(k) retirement plan, paid time off and holidays, and paid learning days. The position will be posted for two weeks from its posting date of March 14, 2025.To apply, send resume to ctlyst_postings@concentrix.com with Job Ref# 006081 in the subject line of the email.#ConcentrixCatalystLocation:USA, Cincinnati, OHLanguage Requirements:Time Type:Physical & Mental Requirements: While performing the duties of this job, the employee is regularly required to operate a computer, keyboard, telephone, headset, and other office equipment. Work is generally sedentary in nature.If you are a California resident, by submitting your information, you acknowledge that you have read and have access to the Job Applicant Privacy Notice for California ResidentsConcentrix is an Equal Opportunity/Affirmative Action Employer including Disabled/Vets.For more information regarding your EEO rights as an applicant, please visit the following websites:EnglishSpanishTo request a reasonable accommodation please click here.If you wish to review the Affirmative Action Plan, please click here.R1596668\n",
    "location": "Cincinnati, OH"
  },
  "65": {
    "id": "4179714527",
    "Company": "CubeSmart",
    "title": "Data Engineer",
    "description": "OverviewThis is a hybrid role based out of Malvern, PA (3 days in office)CubeSmart is currently seeking a Data Engineer to join the Information Technology team at our corporate office in Malvern, PA. The Data Engineer will be responsible for the implementation, development, and optimization of database, data objects, and database scripting with the core database platforms utilized by the core CubeSmart business applications.Who We Are:At CubeSmart, we\u2019re intentional about culture. You can experience it everywhere from our mission statement of \u201cgenuine care\u201d to our \u201cIt\u2019s What\u2019s Inside That Counts\u201d tagline to calling each other \u201cteammates\u201d rather than employees. This spirit fosters a fun and collaborative environment that has resulted in our rapid growth and being recognized amongst the top in our industry.CubeSmart\u2019s award-winning team is made up of people who genuinely care. Teammates care about our customers and the life events and/or business needs they are facing. Teammates are passionate, responsible and understanding. The CubeSmart team is made up of people who have a can-do attitude, are committed to their own success and the success of the company, and lead by example.If this sounds like a team and culture that matches your personal values and motivations, we want to hear from you.ResponsibilitiesThis role will also be responsible for the development and maintenance of ETL processes that manage the consumption of data by CubeSmart\u2019s core systems. The role will serve as a technical expert to analyze the needs of CubeSmart\u2019s applications and business data requirements and produce an optimal solution according to their needs and specifications, and in alignment with CubeSmart\u2019s data design standards. This individual will be part of the team building a data and analytics platform in the Microsoft Azure cloud to support the CubeSmart business. This will include designing and building the data ingestion and ETL pipelines to create the cloud Lakehouse environment. In addition, this person will be engaged to develop features in the legacy SQL Server Data Warehouse platform.ResponsibilitiesCollaborate with program manager(s), application development, and data consumers to develop data ingestion and ETL pipelines required to enable CubeSmart data consumers to onboard processes to the Data Lakehouse platformCreate database designs, stored procedures, views and other associated database objects required to implement processes in support of application development efforts Design and development of data pipelines and ETL processes required to support system integration and application development needs in the legacy SQL data warehouse environmentIdentification and implementation of database performance tuning required to validate that all systems are performing to an optimal levelCollaborate with 3rd party vendors to understand externally sourced data and incorporate into CubeSmart\u2019s data environment, in adherence with CubeSmart\u2019s data design standardsBachelor\u2019s degree in Computer Science, Information Technology or a related disciplineStrong skills in SQL, Python, and PySparkExperience with Microsoft Azure data services (Azure Data Factory, Azure Databricks, Azure SQL, and Databricks ML Flow) or equivalent is preferredFull understanding of the Software Development Life Cycle (SDLC) and experience with Agile software development processesSolid understanding of cloud DevOps capabilities, preferably Azure DevOpsExperience with MongoDB (or similar non-relational database platform) or PostgreSQL is a plusExperience supporting databases and data models for ERP or financial systems in a plusQualificationsKnowledge, Skills, Abilities and Personal CharacteristicsService orientation towards business-focused 24\u00d77 support and service mentalityAbility to work within a team environment, showing an openness to collaborate with technical and business peer, with the ability to step up and take on new challenges in response to changing business conditionsAbility to learn quickly, adapt work processes to adhere with best practicesAbility to clearly convey technical information to both technical and non-technical audiencesCollaborative individual who creates open channels of communications and encourages technical dialogue across the department.Well-developed analytical and problem-solving abilitiesAbility to work on multiple tasks and projects at once, with the ability to properly prioritize one\u2019s own work and the work of othersWe are an Equal Opportunity Employer, Minority/Female/Veteran/Individuals with Disabilities/Sexual Orientation/Gender Identity.\n",
    "location": "Malvern, PA"
  },
  "66": {
    "id": "4153178988",
    "Company": "SMX",
    "title": "Data Engineer Python (4144)",
    "description": "SMX is seeking a talented Data Engineer (Python) with expertise in ETL (Extract, Transform, Load) processes and Apache Airflow. The candidate will be responsible for designing and implementing robust and efficient data pipelines, ensuring high data quality, and contributing to the continuous improvement of our data management practices. This is a remote position supporting a Washington, DC based team.Essential Duties & Responsibilities:Design, develop, and maintain ETL processes using Python and Apache Airflow. Collaborate with data analysts and other stakeholders to understand and meet their data requirements. Develop and implement data validation processes to ensure high data quality. Troubleshoot and resolve issues related to data pipelines. Optimize data extraction, transformation, and loading (ETL) processes to improve efficiency and performance. Document and maintain the design and details of data processes and schemas. Stay updated with the latest industry trends and technologies to ensure data practices remain current. Required Skills & Experience: Proficiency in Python:Strong understanding of Python programming language. Experience with Python libraries and frameworks like Pandas, NumPy, and Django. Expertise in Apache Airflow:Experience in designing, building, and maintaining data pipelines using Apache Airflow. Knowledge of Airflow's architecture, including DAGs and Operators. ETL Processes:Proficiency in data extraction, transformation, and loading processes. Experience with data extraction from various sources, data transformation (cleaning, validating, aggregating, joining, etc.), and loading data into databases or data warehouses. Database Knowledge:Strong understanding of SQL and NoSQL databases. Proficiency in writing complex queries and applying database optimization techniques. Data Warehousing:Experience with data warehousing solutions like Amazon Redshift, Google BigQuery, or Microsoft Azure SQL Data Warehouse. Soft Skills:Strong communication and collaboration skills. Excellent problem-solving skills. US Citizenship is required to obtain a federal clearance. Desired Skills & Experience Knowledge of data modeling and data warehousing. Application Deadline: 4-4-2025#cjpostThe SMX salary determination process takes into account a number of factors, including but not limited to, geographic location, Federal Government contract labor categories, relevant prior work experience, specific skills, education and certifications. At SMX, one of our Core Values is to Invest in Our People so we offer a competitive mix of compensation, learning & development opportunities, and benefits. Some key components of our robust benefits include health insurance, paid leave, and retirement.The proposed salary for this position is:$86,000\u2014$143,200 USDAt SMX\u00ae, we are a team of technical and domain experts dedicated to enabling your mission. From priority national security initiatives for the DoD to highly assured and compliant solutions for healthcare, we understand that digital transformation is key to your future success.We share your vision for the future and strive to accelerate your impact on the world. We bring both cutting edge technology and an expansive view of what\u2019s possible to every engagement. Our delivery model and unique approaches harness our deep technical and domain knowledge, providing forward-looking insights and practical solutions to power secure mission acceleration.All qualified candidates will receive consideration for employment without regard to disability status, protected veteran status, race, color, age, religion, national origin, citizenship, marital status, sex, sexual orientation, gender identity or expression, pregnancy or genetic information.Selected applicant may be subject to a background investigation and/or education verification.\n",
    "location": "Washington, DC"
  },
  "67": {
    "id": "4137705228",
    "Company": "Agero, Inc.",
    "title": "Data Engineer",
    "description": "About Agero:Wherever drivers go, we\u2019re leading the way. Agero\u2019s mission is to rethink the vehicle ownership experience through a powerful combination of passionate people and data-driven technology, strengthening our clients\u2019 relationships with their customers. As the #1 B2B, white-label provider of digital driver assistance services, we\u2019re pushing the industry in a new direction, taking manual processes, and redefining them as digital, transparent, and connected. This includes: an industry-leading dispatch management platform powered by Swoop; comprehensive accident management services; knowledgeable consumer affairs and connected vehicle capabilities; and a growing marketplace of services, discounts and support enabled by a robust partner ecosystem. The company has over 150 million vehicle coverage points in partnership with leading automobile manufacturers, insurance carriers and many others. Managing one of the largest national networks of service providers, Agero responds to approximately 12 million service events annually. Agero, a member company of The Cross Country Group, is headquartered in Medford, Mass., with operations throughout North America. To learn more, visit https://www.agero.com/.Role Description & Mission:We are seeking a Data Engineer who is passionate about data and eager to make a meaningful impact. In this role, you will design, build, and maintain the core data infrastructure that powers our analytics, machine learning, and data science initiatives. Your responsibilities will include optimizing data management processes, ensuring data quality and reliability, and developing scalable, efficient data models to support advanced analytics and data-driven decision-making. Success in this role requires a strong technical foundation, a collaborative mindset, and a drive to deliver innovative and impactful solutions.Key Outcomes:Data Pipelines:Develop and maintain robust ETL/ELT pipelines to ingest data from diverse sources (relational and NoSQL databases, APIs, etc.), including implementing best practices for real-time and batch data ingestion. Create and optimize data workflows using modern orchestration tools (e.g., Apache Airflow, Snowflake Tasks, Dagster, Mage). Cloud Cost Optimization:Monitor and optimize cloud costs (e.g., AWS, Snowflake) by analyzing resource usage and implementing cost-saving strategies. Perform query optimization in Snowflake to reduce compute costs and improve performance. Data Foundations:Develop and maintain modern data architectures, including data lakes and data warehouses (e.g., Snowflake, Databricks, Redshift), considering trade-offs of different data storage solutions and ensuring alignment with business requirements and SLAs. Data Modeling & Transformation:Apply dimensional modeling techniques (Kimball), star and snowflake schemas, and normalization vs. denormalization strategies based on use cases. Develop transformations using DBT (Core or Cloud), Spark (PySpark), or other frameworks. Collaborate on emerging approaches such as data mesh or specialized templates (e.g., Jina) to handle complex data needs. Coding:Write reusable, efficient, and scalable code in Python, PySpark, and SQL. Integrate serverless computing frameworks or modern API frameworks to support data-driven applications (FastAPI, Flask). Develop and maintain data-intensive UIs and dashboards using tools like Streamlit, Dash, Plotly, or React. Data Quality Control:Establish data governance and data quality frameworks, using either custom solutions or popular open-source/commercial tools (e.g., DBT tests, Great Expectations, Soda). Implement data observability solutions to monitor and alert on data integrity and reliability (e.g., Monte Carlo, Alation, or Elementary). Define SLAs, SLOs, and processes to identify, troubleshoot, and resolve data issues. Teamwork:Work cross-functionally with data scientists, analysts, and business stakeholders to translate requirements into robust data solutions. Follow and advocate for best practices in version control, CI/CD. Document data flows, processes, and architecture to facilitate knowledge sharing and maintainability. Skills, Education & Experience:Education & Experience: Bachelor's degree in a technical field and 3+ years of industry experience or Master's degree in a technical field and 3+ years of industry experience. (2-5+ years of experience)Technical Skills (Essential):Extensive experience with Snowflake (preferred) or other cloud-based data warehousing solutions like Redshift or BigQuery. Expertise in building and maintaining ETL/ELT pipelines using tools like Airflow, DBT, Fivetran, or similar frameworks. Proficiency in Python (e.g., Pandas, PySpark) for data processing and transformation. Advanced SQL skills for querying and managing relational and NoSQL databases (e.g., DynamoDB, MongoDB). Solid understanding of data modeling techniques, including dimensional modeling (e.g., star schema, snowflake schema). Knowledge of query optimization and cost management strategies for platforms like Snowflake and cloud environments. Experience with data quality and observability frameworks (e.g., Great Expectations, Soda). Proven expertise in designing and deploying data solutions in the cloud, with a focus on AWS services (e.g., EC2, S3, RDS, Lambda, IAM). Experience in building and consuming data-intensive APIs using frameworks like FastAPI or Flask. Familiarity with version control systems (e.g., Git) and implementing CI/CD pipelines. Soft Skills:Strong communication and collaboration skills with the ability to explain technical concepts to both technical and non-technical audiences. Ability to manage multiple priorities and work independentlyBonus Skills:Experience with data streaming platforms such as Kafka or Kinesis. Familiarity with Agile methodologies (Scrum, Kanban) and IaC tools like Terraform or CloudFormation. Knowledge of emerging technologies or frameworks in the data engineering ecosystem, such as Delta Lake, Iceberg, or Hudi. Hiring In:United States: AZ, FL, IL, KY, MA, MI, NM, NH, TN, GA, NC, VA, CACanada: Province of OntarioThe base salary range presented represents the anticipated low and high end salary range for new hires in this position. Your final base salary will be determined based on factors such as work location, experience, job related skills, and relevant training and education. The range listed is just one component of the total compensation package provided by Agero to employees.National Pay Range$97,482\u2014$140,000 USDLife at Agero:At Agero, you'll find a workplace where your unique perspective is not just welcomed, it's celebrated. We believe that our differences make us stronger, and we're committed to creating an environment where every employee feels a sense of belonging. If you're looking for a company that values your individuality, provides opportunities for growth, and champions open communication, Agero is the place for you. Join our team and help us drive the future of driver assistance, while experiencing a workplace where you can truly thrive.Benefits Built for Well-being: Agero\u2019s innovation is driven by a workforce where all associates feel like they can truly thrive. Agero offers a wide range of benefits to promote well-being, encourage personal development, and ensure financial stability. Our benefits include:Health and Wellness: Healthcare, dental, vision, disability, life insurance, and mental health benefits for associates and their families. Financial Security: 401(k) plan with company match and tuition assistance to support your future goals. Work-Life Balance: Flexible time off, paid sick leave, and ten paid holidays annually. For Contact Center Roles: Accrual of up to 3 weeks Paid Time Off per year, paid sick leave, and ten paid holidays annually. Family Support: Parental planning benefits to assist associates through life\u2019s milestones. Bonus/Incentive ProgramsJoin Agero and experience a workplace that invests in your success both personally and professionally.\n",
    "location": "United States"
  },
  "68": {
    "id": "4179405141",
    "Company": "JOBS by allUP",
    "title": "Lead Data Engineer",
    "description": "Make a Real Impact in Cancer CareOur client, Elekta, is on a mission to help physicians spend more time fighting cancer and less time fighting software. Their engineering team connects thousands of patients to care each day through MOSAIQ, their certified health information system. As their Lead Data Engineer, you'll play a crucial role in shaping the future of cancer treatment technology.About the RoleElekta is seeking an experienced Lead Data Engineer to join their Oncology Information Systems (OIS) engineering team. You'll lead the data architecture and management strategy for MOSAIQ, working at the intersection of healthcare technology and database engineering. This is a hands-on technical leadership role focused on OLTP systems that are deployed both on-premises and in the cloud.What You'll DoDrive the evolution of our data management and database infrastructure strategy to enhance system performance and reduce latencyCollaborate with engineers and oncology domain experts to develop and implement innovative data solutionsDesign and implement advanced interface strategies for external system integrationLead the evaluation and implementation of performance optimization initiativesConduct system analysis and testing to ensure scalability, reliability, and securityWork closely with development teams to resolve complex technical challengesReview and improve our API and system integration approachesWhat You BringBachelor's degree in Engineering, Computer Science, or related field (Master's preferred)Deep experience with data management and database technologies from major cloud providers (AWS, Azure, or Google Cloud)Strong background in on-premises database deployment and managementExperience with version control systems (Git) and CI/CD pipelinesTrack record of building enterprise software products deployed at customer sites or in the cloudExcellence in problem-solving, analysis, and technical leadershipStrong communication skills and ability to collaborate effectively with cross-functional teamsExperience with agile development methodologiesLocation & Work EnvironmentThis is a hybrid position based in the San Francisco Bay Area (San Jose) requiring 3 days in office. You'll work in a collaborative, fast-paced environment where creativity and innovation are encouraged.What We OfferBase salary $175,000 - $195,000 depending on experienceOpportunity to work with a proactive and supportive teamHybrid work option (you are required to work on location at least 3 days/week)Excellent Medical, Dental and Vision coverage401k, paid Vacation and HolidayA health of additional benefits including wellness reimbursement, tuition reimbursement and flexible spending accountOpportunity to work on cutting edge in medical advancement.Close-knit company cultureUpward mobilityWhy Elekta?Join a mission-driven team making a real difference in healthcareWork with cutting-edge technology in a scaled agile environmentCollaborate with a diverse team of architects, product owners, designers, and developersHelp shape the future of cancer care technologyOpportunity to solve complex technical challenges at scaleWe're looking for self-motivated individuals who thrive in fast-paced environments and are always searching for better ways to solve problems. If you're creative, smart, and work best in teams, we'd love to talk to you.Elekta is an equal opportunity employer. We value diversity and are committed to creating an inclusive environment. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of this role.\n",
    "location": "San Francisco Bay Area"
  },
  "69": {
    "id": "4182992800",
    "Company": "High Trail",
    "title": "Data Engineer",
    "description": "We've been retained to find an Analytics Engineer, where you'll be responsible for building and refining an end-to-end data transformation layer\u2014from real-time data ingestion and model development to production deployment and monitoring in AWS. You'll design scalable business solutions that drive data-driven decision-making and create measurable impact.Key ResponsibilitiesDevelop and refine dimensional data models using dbt and ClickhouseBuild proprietary metrics and KPIs in collaboration with sales and customer successImprove data quality with automated testing and observabilityAutomate core business processes for efficiency gainsCreate internal Hex applications and Preset visualizations to democratize data accessEnhance modeling infrastructure for scalability and performanceImplement statistical methods for anomaly detection (Prophet, ARIMA, Tukey\u2019s Fences)Drive quantifiable business value through data operationalizationStay ahead of industry trends and introduce cutting-edge tools and techniquesPresent insights and recommendations to senior leadershipQualifications & SkillsBachelor's degree in a technical field (Economics, Information Systems, or related)Strong experience with dbt, SQL, and data warehousing (Snowflake, BigQuery, Clickhouse preferred)Proficiency in Python, Docker, and KubernetesHands-on experience with ETL tools, Spark, and CI/CD pipelines (Git, Jenkins)Experience with data visualization tools like Tableau or HexStrong analytical, problem-solving, and communication skillsAbility to work cross-functionally in a fast-paced environmentThis role is ideal for someone with an entrepreneurial mindset, who thrives on scaling data solutions, driving automation, and delivering clear ROI for customers. If you're passionate about defining the next generation of data-powered products, we\u2019d love to hear from you!\n",
    "location": "New York City Metropolitan Area"
  },
  "70": {
    "id": "4181938646",
    "Company": "mthree",
    "title": "Python Developer/Data Engineer",
    "description": "**Looking for local candidates**Want to work in technology in the financial industry?We are looking for someone to be a part of a dynamic team for one of our clients as a Python Developer in Data Engineering. Our client is growing Data Engineering capabilities within our Reliability & Production Engineering (RPE) organization as part of the transformation of their Technology.About mthree:Since 2010, mthree has been helping clients solve their business and technological challenges. We are a technology and business consultancy with a global workforce delivering significant business and IT projects in some of the largest financial services organizations worldwide.Core ServicesConsulting and AdvisoryManaged ServicesAlumni Graduate ProgramAlumni Pro ProgramWe have a global presence and are experts in delivering exceptional quality to our client base, providing consulting services across Risk, Regulation & Compliance; Vendor Products; Application Support; Application Development; Cyber & Information Security; Data Science and DevOps areas.Our Expert program offers experienced professionals access to top roles in tech, finance, aviation and insurance. Join us to work on groundbreaking technology projects, from international trading platforms to critical applications for leading airlines. We recruit professionals who are eager to fast-track their careers in technology or operations within prestigious global organizations.We would like to talk to you if you:Are interested in distributed systems and working with highly scalable and reliable services.Like to work in a fast-moving environment and you aren't afraid to change things to make them better.Enjoy new technological challenges and solving hard problems.Are keen to learn from exposure to a fluid environment while helping clients with their challenges.Believe a team working well together is smarter than the single smartest person on that team.Aspire to grow as a person and as a teammate.Have grit, drive, and a deep sense of ownership.As an intermediate-level Python developer in data engineering, you'll design, build, and maintain data pipelines to extract, transform, and load data, ensuring data quality and scalability while collaborating with cross-functional, cross-regional teams.Your responsibilities will include, but not be limited to Software & Data pipeline Development:Designing, developing, and implementing software applications, scalable data pipelines using Python and related technologies.Optimize data workflows to handle large datasets efficiently.Writing clean, efficient, and maintainable code that meets industry standards.Developing and managing databases and applications.Implement data extraction, transformation, and loading (ETL) processes.Writing and documenting effective APIs.Testing software at scale to ensure responsiveness and efficiency.Ensure data accuracy, consistency, and completeness.Identify and resolve data issues or anomalies.Collaboration and Communication:Collaborating with team members, stakeholders, and other developers to define project requirements and develop technical solutions.Participating in code reviews to ensure code quality and best practices.Problem Solving and Debugging:Troubleshooting, debugging, and upgrading software.Identifying and resolving performance issues and bugs.Documentation:Contributing to the documentation of code and projects.Skills and Qualifications:Proficiency in Python, with a strong understanding of object-oriented programming principles.Data Structures and Algorithms: Familiarity with common data structures and algorithms and corresponding usage in practical solutions.Data : Experience with data libraries(e.g., Django, Flask, NumPy, Pandas).Familiarity with(e.g., PostgreSQL, MySQL) and/or NoSQL databases (e.g., MongoDB).Familiarity with data storage technologies (e.g., data lakes, distributed file systems).Knowledge of ETL tools (e.g., Apache Airflow, Apache Kafka).Experience with orchestration tools & cloud computing platforms (e.g., Ansible, Terraform/Kubernetes, AWS, Azure, GCP).Familiarity with modern CI/CD frameworks and tools (e.g., Git, Jenkins, Github)Hands on experience with LinuxDemonstrated ability to troubleshoot problems and debug to identify root cause.Demonstrate ability to think outside of the box.Experience: 2-4 years of experience in software development with a focus on Python & dataAt mthree, our values support courageous teammates, needle movers, and learning champions all while striving to support the health and well-being of all employees. We take great pride in celebrating the diversity of each individual who contributes to making mthree the company it is today and will be in the future. We value diversity both within mthree and with our partner companies, and we're proud to provide an environment where all our colleagues can flourish. That means promoting a strong culture of equality but, most importantly, inclusion.\n",
    "location": "New York City Metropolitan Area"
  },
  "71": {
    "id": "4168179310",
    "Company": "Paylocity",
    "title": "Associate Data Engineer",
    "description": "DescriptionPaylocity is an award-winning provider of cloud-based HR and payroll software solutions, offering the most complete platform for the modern workforce. The company has become one of the fastest-growing HCM software providers worldwide by offering an intuitive, easy-to-use product suite that helps businesses automate and streamline HR and payroll processes, attract and retain talent, and build a strong workplace culture.While traditional HR and payroll providers automate basic HR processes such as payroll and benefits administration, Paylocity goes further by developing tools that HR and businesses need to compete for talent and deliver against the expectations of the modern workforce.Help Paylocity enhance communication and enable employees to connect, collaborate, and create from anywhere with a position in Product & Technology!Want to develop the strategies and principles needed to deliver compelling software? Join our team and help us enhance our all-in-one software platform, elevate our one-of-a-kind technology, and improve the employee experience.Take your career to the next level at one of G2's Top 100 Software Companies. Explore our Product & Technology positions to see where you fit!We are committed to building a world-class team by hiring top talent globally. To ensure compliance, we use an Employer of Record model. Our ideal candidates would be based in Guadalajara, Mexico, or the surrounding areas, with the right to work there, and be able to work within four hours of Eastern Standard Time.As an Associate Data Engineer, you will help design, create, deploy and maintain Paylocity\u2019s data architecture. This role will influence how the data will be stored, consumed, integrated and managed by different systems. The role will be pivotal and enabling insights, analytics and reporting across the entire Paylocity data space.Are you the teammate we are looking for?Who You AreA Data Engineer within Paylocity works closely with multiple Data Engineering teams across the globe. You\u2019ll work closely with consuming Reporting, Analytics and Data Science teams to deliver positive customer impacts through various data pipelines.Focused on data driven solutions to improve customers success through reports and analyticsInvested in staying current in data engineering by applying new technologies and practicesAble to work in a collaborative environment with a desire to share your ideasAble to work independently on modules and complete tasks with high quality, but unafraid to seek out suggestions from other team membersExcited to work on cutting-edge technologyDuring The First Six Months, You WillWork on clearly defined individual tasks within a project and receive active guidance.Regularly write SQL Statements for selecting, filtering and joining different entitiesCreate accurate and performant Tables and Views given a data modelCreate data pipelines utilizing internal data platform tooling based on SQL, DBT and SnowflakeDemonstrate basic understanding of one or more RDBMS systemsRegularly connect and query from one or more RDBMS systemDemonstrate basic understanding of Object Stores (partitions, how to access, how to monitor)Demonstrate understanding of Organizational-wide Data Standards (including Data Modeling, Documentation, Governance and Security)Demonstrate understanding of team-wide Support model including monitoring dashboardsRegularly model a given business domain utilizing Kimball methodologiesReview PRs and asks questions or suggests changesDocument logic and pipelines in a concise and accurate way Regularly participate in story planning and refinement with story definition, AC and story pointsOccasionally review design documents, provide feedback and ask clarifying questionsUpdate and curate tests when introducing new functionalityEffectively deliver well-specced individual tasks.Required ExperienceBachelor's degree in a computer science, engineering, technology related field or equivalent experience Organizational experience writing and refactoring performant SQL queriesExperience of data modeling tools and standards, ETL tools, and big data technologies (e.g., Hadoop, Spark, DBT, Pig, NoSQL). Working experience with data lakes and data warehouses Should be familiar with Systems Architecture and developer tools Data driven personality Must be able to work and communicate effectively in an agile environment Must be effective and creative in problem solving Must have experience working within a teamPaylocity is an equal-opportunity employer. Paylocity is committed to the full inclusion of all individuals. We recruit, train, compensate, and promote regardless of race, religion, color, national origin, sex, disability, age, veteran status, and other protected status as required by applicable law. At Paylocity, we believe diversity makes us better.We embrace and encourage our employees\u2019 differences in age, culture, ethnicity, family or marital status, gender identity or expression, language, national origin, physical and mental ability, political affiliation, race, religion or spiritual belief, sexual orientation, socio-economic status, veteran status, and other characteristics that make our employees unique. We actively cultivate these differences through our employee resource groups (ERGs), employee experiences, perspectives, talents, and approaches to drive innovation in the software and services we provide our customers.We comply with federal and state disability laws and make reasonable accommodations for applicants and employees with disabilities. To request reasonable accommodation in the job application or interview process, please contact leavebenefits@paylocity.com. This email address is exclusively designated for such requests, aligning with federal and state disability laws. Please do not send resumes to this email address, as they will be removed.\n        ",
    "location": "United States"
  },
  "72": {
    "id": "4159689704",
    "Company": "Takeda",
    "title": "Data Engineer",
    "description": "By clicking the \u201cApply\u201d button, I understand that my employment application process with Takeda will commence and that the information I provide in my application will be processed in line with Takeda\u2019s Privacy Notice and Terms of Use. I further attest that all information I submit in my employment application is true to the best of my knowledge.Job Description:Pursuant to Section 20 C.F.R. 656.10, you are hereby notified that an application for Alien Employment Certification will be filed by Takeda Development Center Americas, Inc. for the following job opportunityJOB LOCATION: Cambridge, MAPOSITION: Data EngineerPOSITION DESCRIPTION: Takeda Development Center America, Inc. is seeking a Data Engineer with the following duties Identify various methods for analyzing data by utilizing diverse technologies, preparing and managing big data, programming, and data loading, as well as conducting initial exploration in the process of searching and identifying patterns; maintain end-to-end data pipelines, internalize numerous systems and processes, broad enablement of automation and real-time data accessibility, efficient data review and query, facilitation of disruptive technologies for next-generation trial designs and insight derivation; review data science inputs and requirements, converting them from initial data exploration into practical applications for extensive datasets with billions of records or unstructured data. This process may involve programming, mathematical algorithms and potentially newer tools like Machine learning or Deep learning to construct models and extract meaningful insights from the data; introduce novel and state-of-the-art computational techniques to other teams and scientists to improve capabilities for data analysis with the purpose of deriving high accuracy insight from available datasets more efficiently; utilize advanced methods on structured, partly structured, and unstructured data across various organizations; gather and prepare data for utilization or training purposes, enhance the implementation of algorithms to evaluate, expand and deploy future models; independently design, develop and deploy data models related to R&D questions; create technical strategies and roadmaps for data analysis, uncovering patterns, formulating data models, and scaling these models for controlled production environments or to design and implement a future state environment for data processing; recognize prospects and foresee shifts in the technological landscape by comprehending and continuously evaluating the external factors that influence the business.REQUIREMENTS :Master\u2019s degree in Computer Science, Information Systems or related field plus 3 years of related experience. Prior experience must include Identify various methods for analyzing data, utilize diverse technologies, prepare, and manage big data, programming and data loading as well as initial exploration in the process of searching and finding patterns; maintain end-to-end data pipelines, internalize numerous systems and processes, broad enablement of automation and real time data accessibility, efficient data review and query, facilitation of disruptive technologies for next-generation trial designs and insights derivation; review data science inputs and requirements, converting them from initial data exploration into practical applications for extensive datasets with billions of records or unstructured data; utilize advanced methods on structured, partly structured, and unstructured data across various organizations; gather and prepare data for utilization or training purposes, enhance the implementation of algorithms to evaluate, expand and deploy future models; create technical strategies and roadmaps for data analysis, uncovering patterns, formulating data models, and scaling these models for controlled production environments or to design and implement a future state environment for data processing; recognize prospects and foresee shifts in the technological landscape by comprehending and continuously evaluating the external factors that influence the business.Full time. $150,000 - $215,600.00 per year. Competitive compensation and benefits.Qualified applicants can apply at https //jobs.takeda.com. Please reference job #R0145002. EOEAny individual may provide documentary evidence bearing on this application, with information on available workers and information on wages and working conditions, to the Certifying Officer, U.S. Department of Labor, Employment and Training Administration, Office of Foreign Labor Certification, 200 Constitution Avenue, NW, Room N-5311, Washington, DC 20210.Locations:Boston, MAWorker Type:EmployeeWorker Sub-Type:RegularTime Type:Full time\n        ",
    "location": "Boston, MA"
  },
  "73": {
    "id": "4168750922",
    "Company": "PTR Global",
    "title": "Data Engineer",
    "description": "Position: Data EngineerWork Location: Irving, TX/ Charlotte, NC- HybridDuration: 12+ months contract with potential to extendOnly on W2Pay range: $55-$60/HR on W2About The RoleOur Fortune 500 client, a leader in the financial services industry, is looking for Data Engineer to join their customer support team in Irving, TX/ Charlotte, NCMust have:SQL- Python Pyspark Google cloud Private cloud Configuration based understand how the framework works Building ETL pipelines Some data modeling experience Job Description:Analyze and understand the existing data processes, rationalize dataset, and build operational data model. Design & Build data pipelines to integrate the data from System of records to operational and analytical data store. Build framework for the ETL processes related to L2 data products ( originations, servicing, hardship, Capital markets, Sub domain aggregation). Understand the UCDP framework(homegrown) and build the data pipeline adopting that framework. Work with Product owners, L1 team members, reporting , LOB stakeholders from requirement through deployment process. Follow SDLC process , compnay standards and governance for all the work. Perform Proof of concept in cloud platform to migrate workloads from on prem to cloud. Collaborate with other team members and guide the team wherever applicable. Perform proof of concepts for the new cloud platform wherever applicable. What qualifications, experience or skills are required:Proficiency in Python/Spark , Data Pipelining Tools Expertise in designing, architecting data pipelines for reporting and other downstream applications using cloud and opensource software. Expertise in CI/CD pipeline tools like Jenkins, GitHub Actions, or Gitlab. Expertise in cloud platforms \u2013 GCP, Azure Understand Database, Deltalake (Operational and Analytical Data store), reporting concepts \u2013 Use the same in designing cloudbased solutions. Familiarity with tools like SSIS, SAS et. To understand the code for migrating to new platform. Critical Thinking, Ability to adapt to change, Problem solving. Pay Range: $55 - $60The specific compensation for this position will be determined by a number of factors, including the scope, complexity and location of the role as well as the cost of labor in the market; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. Our full-time consultants have access to benefits including medical, dental, vision and 401K contributions as well as any other PTO, sick leave, and other benefits mandated by appliable state or localities where you reside or work.\n        ",
    "location": "Irving, TX"
  },
  "74": {
    "id": "4170309291",
    "Company": "TAG - The Aspen Group",
    "title": "Data Engineer",
    "description": "The Aspen Group (TAG) is one of the largest and most trusted retail healthcare business support organizations in the U.S. and has supported over 20,000 healthcare professionals and team members with close to 1,500 health and wellness offices across 48 states in four distinct categories: dental care, urgent care, medical aesthetics, and animal health. Working in partnership with independent practice owners and clinicians, the team is united by a single purpose: to prove that healthcare can be better and smarter for everyone. TAG provides a comprehensive suite of centralized business support services that power the impact of five consumer-facing businesses: Aspen Dental, ClearChoice Dental Implant Centers, WellNow Urgent Care, Chapter Aesthetic Studio, and AZPetVet. Each brand has access to a deep community of experts, tools and resources to grow their practices, and an unwavering commitment to delivering high-quality consumer healthcare experiences at scale.\u200bAs a reflection of our current needs and planned growth we are very pleased to offer a new opportunity to join our dedicated team as a Data Engineer.Job Summary: We are looking for a Data Engineer to build and maintain scalable, cloud-native data pipelines using DBT Cloud, Python, Airflow, and Google BigQuery. You will be responsible for developing efficient ELT workflows, optimizing data models, and ensuring high-performance data processing to support analytics and business intelligence initiatives.  This role is ideal for someone with strong technical expertise in modern data engineering practices, a passion for cloud technologies, and a desire to work on scalable, high-impact data solutions. Key Responsibilities: Data Pipeline Development & Optimization: Build and maintain scalable ELT pipelines using DBT Cloud, Python, and Airflow. Optimize BigQuery data models for performance, cost-efficiency, and scalability. Develop data transformations and ensure data quality across all pipelines. Automate and monitor data workflows to ensure reliability and observability. Platform & Infrastructure: Work with Google Cloud services (BigQuery, Cloud Storage, Pub/Sub) to build highly available data solutions. Implement best practices for data security, governance, and compliance. Collaborate with engineering and DevOps teams to establish CI/CD pipelines for data workflows.  Collaboration & Continuous Improvement: Work closely with data analysts, scientists, and business teams to support reporting and analytics needs. Identify performance bottlenecks and propose innovative solutions to improve data processing efficiency. Stay up to date with the latest data engineering trends and cloud technologies.  Qualifications & Experience: 2+ years of experience in data engineering or a related field. Strong proficiency in DBT Cloud, Python, Airflow, and Google BigQuery. Experience designing scalable ELT data pipelines in cloud environments. Familiarity with SQL performance tuning and data warehousing best practices. Hands-on experience with orchestration tools (Airflow, Prefect, etc.). Knowledge of data governance, security, and compliance best practices.  Preferred Qualifications: Experience with streaming architectures (Kafka, Pub/Sub, Dataflow, etc.). Exposure to infrastructure as code (Terraform, etc.). Knowledge of data observability and monitoring tools. Google Cloud Certified Data Professional This role is onsite 4 days/week in our Fulton Market officeA generous benefits package that includes paid time off, health, dental, vision, and 401(k) savings plan with match\n",
    "location": "Chicago, IL"
  },
  "75": {
    "id": "4138466991",
    "Company": "Exusia",
    "title": "Lead Data Engineer - Snowflake / Databricks / Azure",
    "description": "Lead Data Engineers \u2013 Azure/ Databricks/ SnowflakeDepartment:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Delivery Team - EmpowerIndustry:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Information Technology & Services, Computer Software, Management ConsultingLocation:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0US Remote (US citizens, Candidates based in US with Valid Visa will be preferred)Experience Range:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a06 - 14 yearsBasic Qualification:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Bachelor of Engineering or EquivalentTravel Requirements:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Not requiredWebsite:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0www.exusia.com\u00a0\u00a0Exusia, a cutting-edge digital transformation consultancy, and is looking for top Talent in Azure, Databricks Snowflake Data Warehouse Application Developers to join our global delivery team's Industry Analytics practice in the United States or India.What\u2019s the Role?Full-time job to work with Exusia's clients to design and develop the system components for the extract and transform or conversion of data from various source systems to the target applications; Create data structures or data models that support data across all stages of the extract, transform and load process. Should have good hands on experience in implementing the parallel and sequence jobs using Snowflake and or or Azure or Databricks or Spark or Scala. Analyzing, developing, monitoring and administration of the ETL process using any tool. Experience working with on-shore/off-shore delivery model is a big plus.The Lead Data Engineer will implement monitoring solutions, optimize ETL jobs for performance, and provide comprehensive support from data ingestion to final output. With in-depth knowledge of the Databricks platform and strong analytical skills, this position will significantly enhance the department's ability to deliver high quality, data-driven insights and solutions.Criteria for the Role!Have a minimum of 6+ years\u2019 experience working as a Azure Databricks Developer in to a data analytics domain\u00a0\u00a0Azure Databricks Specific\u2022\u00a0\u00a0\u00a04+ years of experience in Azure services and in Databricks (ADLS, ADF, DevOps, etc.)\u2022\u00a0\u00a0\u00a010+ years of experience in designing and implementing ETL/ELT pipelines using ADF, integrating with other Azure services.\u2022\u00a0\u00a0\u00a07+ years of Python development experience. Should have capability of developing own libraries\u2022\u00a0\u00a0\u00a04+ years of experience in Snowflake or SQL experience (No-SQL experience is a plus)\u2022\u00a0\u00a0\u00a0Experience designing, building, and maintaining data processing systems\u2022\u00a0\u00a0\u00a0Experience with or knowledge of Agile Software Development methodologies\u2022\u00a0\u00a0\u00a0Education Qualification \u2013 Bachelor of Science (preferably in Computer and Information Sciences or Business Information Technology) or an Engineering degree in the above areasResponsibilities:\u00b7\u00a0\u00a0\u00a0\u00a0Design, develop, and maintain data pipelines and ETL processes using Azure Data Factory, Azure Databricks\u00b7\u00a0\u00a0\u00a0\u00a0Build and optimize data storage solutions using Azure Data Lake, Azure SQL Database, and Azure Cosmos DB or Vector DB\u00a0\u00b7\u00a0\u00a0\u00a0\u00a0Collaborate with data scientists, analysts, and business stakeholders to understand data requirements and deliver solutions.\u00b7\u00a0\u00a0\u00a0\u00a0Implement data quality checks, data governance, and security best practices across data platforms.\u00b7\u00a0\u00a0\u00a0\u00a0Monitor, troubleshoot, and optimize data workflows for performance and scalability.\u00b7\u00a0\u00a0\u00a0\u00a0Automate data integration and transformation processes using Azure DevOps and CI/CD pipelines.\u00b7\u00a0\u00a0\u00a0\u00a0Stay up-to-date with emerging Azure technologies and data engineering trends.\u00b7\u00a0\u00a0\u00a0\u00a0Deep understanding of data management, data governance, cloud and AI technologies, and their application in a business context.\u00b7\u00a0\u00a0\u00a0\u00a0Excellent problem solving and troubleshooting skills\u2022\u00a0\u00a0\u00a0Excellent oral and written communication skills\u00b7\u00a0\u00a0\u00a0\u00a0Strong analytical and problem-solving skills\u00b7\u00a0\u00a0\u00a0\u00a0Data experience working with multiple, large datasets / big data\u00b7\u00a0\u00a0\u00a0\u00a0Demonstrated experience in effectively working with multiple projects and activities in a timely, organized manner\u00b7\u00a0\u00a0\u00a0\u00a0Demonstrated experience in organizing and prioritizing work, meeting deadlines and working independently as well as collaboratively with others\u00b7\u00a0\u00a0\u00a0\u00a0Experience with Excel and PowerPoint.RequirementsNice \u2013 to \u2013 have Skills:\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02+ years of experience with Azure stack example Azure Kubernetes Service (AKS), Cosmos DB\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Understanding on cloud streaming technique GCP, Snowpipe\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Experience in designing API development and integrating with React JS within cloud platform\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Leverage modern technologies, such as AI and ML\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Must have strong DWH and ETL basics\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Should have basic loud knowledge - Storage and Compute\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Work with business stakeholders to gather and analyze business requirements, building a solid understanding of the Data Analytics domain\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Document, discuss and resolve business, data and reporting issues within the team, across functional teams, and with business stakeholders\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Support all phases of the software development lifecycle (SDLC)\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Present written and verbal data analysis findings, to both the project team and business stakeholders\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Manage changing priorities and work on multiple projects concurrently\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Be self-motivated and proactive in a fast-paced environment and flexible to work\u00a0About ExusiaExusia (http://exusia.com/) is a global technology consulting company that empowers its clients to gain a competitive edge by accelerating business objectives and providing strategy and solutions in data management and analytics. The company has established its leadership position by solving some of the world's largest and most complex data problems in the financial, healthcare, telecommunications and high technology industries.\u00a0Exusia\u2019s mission is to transform the world through the innovative use of information.\u00a0Exusia was recognized by Inc. 5000 and by Crain\u2019s publications as one of the fastest growing privately held companies in the world. Since the company\u2019s founding in 2012, Exusia has experienced an impressive seven years of revenue growth and has expanded its operations in the Americas, Asia, Africa and UK. Exusia has recently also been recognized by publications such as the CIO Review, Industry Era, Insight Success and the CIO Bulletin for the company\u2019s innovation in IT Services, the Telecommunications and Healthcare industries and its entrepreneurship. The company is headquartered in Miami city of Florida, United States with development centers in Pune, Hyderabad, Bengaluru and Gurugram, India.\u00a0Interested applicants should apply by forwarding their CV to:\u00a0elevate@exusia.com\n",
    "location": "United States"
  },
  "76": {
    "id": "4162098099",
    "Company": "Pax8",
    "title": "Data Engineer 1",
    "description": "Pax8 is the leading cloud-based technology marketplace, simplifying the cloud journey for our partners by integrating technology, business intelligence and proactive service to deliver an unparalleled experience. Serving thousands of partners through the indirect sales channel, our mission is to build the technology marketplace of the future. We are a fast-growing, dynamic and high-energy startup organization, allowing you to make a meaningful impact on the business. Culture is important to us, and at Pax8, it\u2019s business, and it IS personal. We are passionate, creative and humorously offbeat. We work hard, keep it fun, and expect the best.We Elev8 each other. We Advoc8 for our partners. We Innov8 continuously. We Celebr8 life.No matter who you are, Pax8 is a place you can call home. We know there\u2019s no such thing as a\u202f\u201cperfect\"\u202fcandidate, so we don\u2019t look for the right \"fit\" \u2013 instead, we look for the add.\u202fWe encourage you to apply for a role at Pax8 even if you don\u2019t meet 100% of the bullet points.\u202fWe\u202fbelieve in cultivating an environment with a diversity of perspectives, in hopes that we can all thrive in an inclusive environment.We are only as great as our people. And we have great people all over the world. No matter where you live and work, you\u2019re a part of the Pax8 team. This means embracing hybrid- and remote-work whenever possible.Overview: Pax8 is the leading value-added cloud-based SaaS marketplace, simplifying the cloud journey for our partners by integrating technology, business intelligence, and proactive service to deliver an unparalleled experience. Serving thousands of partners through the indirect sales channel, our mission is to be the world\u2019s favorite marketplace for technology professionals to buy cloud products. We are a fast-growing, dynamic, and high-energy organization with a start-up feel, allowing you to make a meaningful impact on the business. Culture is important to us, and at Pax8, it\u2019s business, and it IS personal. We are passionate, creative, and humorously offbeat. We work hard, keep it fun, and expect the best. We Elev8 each other. We Advoc8 for our partners. We Innov8 continuously. We Celebr8 life. Pax8 not only continuously wins numerous awards in our field but we have time and again been voted among the best places to work in Denver. We are committed to our goal of being the #1 employer of choice!Position Summary: The Data Engineer I helps to develop systems that collect, transform, store, and manage data for end users, involving the routine application of well-documented techniques. They work with technical leaders to prepare detailed plans, generally spanning several weeks, and conduct simple investigative analyses and tests.Essential Responsibilities:Learns coding techniques/standards and applies them to their work. (20%) Define, build, test, and implement scalable data pipelines using Python and SQL. (20%) Transforms data to support varied use cases. (20%) Optimizes existing data pipelines and improves existing code quality. (20%) Writes unit and integration tests. (10%) Works collaboratively with peers to solve pressing data issues. (5%) Participates in on-call rotation. (5%) Ideal Skills, Experience, and Competencies:At least one (1) to three (3) years of relevant data engineering experience. Intermediate experience with the Python programming language. Intermediate experience with SQL. Experience with Data Modeling. Exposure to a JVM language. Exposure to Apache Spark or other distributed processing engines. Exposure to Apache Kafka or other stream processing frameworks. Exposure to Terraform, Docker, Kubernetes, or other similar infrastructure tooling. Exposure to job orchestration and/or ETL tools such as Airflow, Prefect, Glue, Talend, or Informatica. Exposure to cloud environments such as AWS, Azure, or Google Cloud. Exposure to analytical databases such as Redshift, Athena, Big Query, and Presto. Ability to build partnerships and work collaboratively with others to meet shared objectives. Ability to actively seek new ways to grow using both formal and informal development challenges. Ability to effectively absorb and apply peer feedback. Required Education & Certifications:B.A./B.S. in a related field or equivalent work experience.Compensation:Qualified candidates can expect a compensation range of $93,000 to $115,000 or more depending on experience.Will you now or in the future require Pax8 to commence (\u201csponsor\u201d) or transfer an immigration case (such as an H-1B Visa, L-1 Visa, etc.) to employ you, commonly known as \u201csponsorship\u201d? Please note, Pax8 currently is unable to consider applicants who require the sponsorship of a new employment-based visa or the transfer of an existing employment-based visa for this position. Pax8 remains committed to complying with all legal requirements and thank you for your understanding in this matter.Expected Closing Date: 3/7/2025*** Colorado law requires an estimated closing date for job postings. Please don't be discouraged from applying if you see this date has passed ***At Pax8 we believe that your Total Rewards should include a benefits package that shows how much we value our greatest assets. All FTE Pax8 people enjoy the following benefits:Non-Commissioned Bonus Plans or Variable Commission401(k) plan with employer matchMedical, Dental & Vision InsuranceEmployee Assistance ProgramEmployer Paid Short & Long Term Disability, Life and AD&D InsuranceFlexible, Open VacationPaid Sick Time OffExtended Leave for Life eventsRTD Eco Pass (For local Colorado Employees)Career Development ProgramsStock Option EligibilityEmployee-led Resource Groups Pax8 is an EEOC Employer.Equal OpportunitiesPax8 is an equal opportunities employer and welcome individuals who are in possession of the appropriate requirements to work within the country the role is based in. Offered individuals will be asked to undertake identity, security compliance and reference checks. Your privacy is important to us. Your data will be held in accordance with Data Privacy best practices and processed only in accordance with our recruiting processes.Job Applicant Privacy Notice\n",
    "location": "Colorado, United States"
  },
  "77": {
    "id": "4174914998",
    "Company": "Guardian Life",
    "title": "Data Engineer",
    "description": "We are seeking a highly skilled and motivated Data Engineer to join the Enterprise Data & AI team. In this role, you will play a crucial part in designing, building, and maintaining the data pipelines that power our predictive & descriptive use cases.You Will:Design, build and maintain robust & scalable data pipelines to feed descriptive analytics, predictive analytics, and reporting use cases.Construct meaningful data assets sourced from structured, semi structured, and unstructured data.Ensure data quality, accuracy and integrity of the data assets created.Automate the data pipelines end to end.Participate in data modeling and schema design.Work closely with business partners, data scientists & BI team to understand the business process and data needs to translate them into an effective data solution.5 bullet points or sentences hereCollaborate with Stakeholders, Analysts, Data Scientists to understand the data requirements & deliver optimal solutions.Collaborate with cross-functional teams to identify the right data sources for the use cases.Perform data discovery and analysis on various data sources.Find and tell the compelling narrative of the data that aligns to the intended audience.Create and maintain Data Dictionary for the data pipelines.Participate in code reviews and documentation efforts to maintain high-quality code and data documentation.You Have:3-5 Bachelors' degree in STEM related field or equivalent.At least 4 years of experience in Data Engineering or similar role.Strong understanding of data engineering principles, standard methodologies, and software development life cycle.Proficiency in SQL, Python, PySpark and bash scripts.Knowledge of Databricks.Experience with the design and development of ETL/ELT process.Experience working in a Dataware house.Understanding of Data modeling and in particular Dimension modeling.Strong analytical and problem-solving skills.Excellent communication and collaboration skills to work across multiple groups within the organization.Experience with Alteryx.Experience in the insurance industry.Experience with Databricks Workflows.Location & Travel:Bethlehem, NJ, Holmdel, NJ, NY, NY\n",
    "location": "Bethlehem, PA"
  },
  "78": {
    "id": "4181193336",
    "Company": "Consumer Reports",
    "title": "Data Engineer",
    "description": "Who We AreConsumer Reports is an independent, nonprofit organization dedicated to a fair and just marketplace for consumers. Our team is made up of truth tellers, change agents, and consumer advocates who investigate and build coalitions to fight for fairness and justice in the marketplace for consumers. We leverage our evidence-based approach to demand safer products, a healthier environment, and equitable services for everyone.Our mission starts with you. We offer medical benefits that start on your first day as a CR employee that include behavioral health coverage, family planning and a generous 401K match. Learn more about how CR advocates on behalf of our employees.OverviewData is at the heart of CR\u2019s identity and mission; CR was created and continues to equip people with the credible, trustworthy information they need to make informed choices.The Data Engineer keeps the magnitude of that data flowing throughout the organization. Reporting to the Director of Data Architecture and Engineering, this role will develop software components within our data architecture, driving impact throughout CR.If you love collaborating within a team and throughout the organization, solving complex data problems, and want to make a meaningful impact, this is the role for you.Under our CR Flex Policy, this is a hybrid position. The successful candidate must be able to travel to our Yonkers, NY headquarters at least once per month. This position is not eligible for sponsorship.How You\u2019ll Make An ImpactCR knows data powers our ability to move efficiently and with conviction. Your role will help us strengthen consumer influence, address consumer needs, and guide decision-making to achieve our goals. On a daily basis you may:Design, develop, and maintain robust, scalable ETL/ELT data pipelines to ingest, transform, and store structured and unstructured data from various sourcesAssist in analyzing and defining technical requirements for data projectsEnsure the quality and reliability of our data and solutionsSupport the deployment of data solutions and provide technical assistance in production environments.About YouYou\u2019ll be Highly Rated IfYou thrive on teamwork. The Data Team collaborates, coding pairs are the norm. You will work collaboratively to achieve shared goals while valuing differences, offering support and help and sharing knowledge and expertise to help the team.You love data. We have a lot of data, and it\u2019s stored in many places. There are a lot of things to consider as you build solutions and work with teams across the organization. Bring your analytical and problem-solving skills, and your strong attention to detail in quality assurance.You'll Be One of Our Top Picks IfYou have the degree. Bachelor\u2019s degree in computer science engineering or related fieldYou have the experience. 1+ year of experience with PySparkYou know the systems. Experience with Databricks Lakehouse, Python, and SQLFair Pay & A Just WorkPlaceAt Consumer Reports, we are committed to fair, transparent pay and we strive to provide competitive, market-informed compensation. The target salary range for this position is $110K-$120K. This posted range is reflective of the NY/CA market. It is anticipated that most qualified candidates will fall near the middle of this range. Compensation for the successful candidate will be informed by the candidate\u2019s particular combination of knowledge, skills, competencies, and experience.Consumer Reports is an equal opportunity employer and does not discriminate in employment on the basis of actual or perceived race, color, creed, religion, age, national origin, ancestry, citizenship status, sex or gender (including pregnancy, childbirth, related medical conditions or lactation), gender identity and expression (including transgender status), sexual orientation, marital status, military service or veteran status, protected medical condition as defined by applicable state or local law, disability, genetic information, or any other basis protected by applicable federal, state or local laws. Consumer Reports will provide you with any reasonable assistance or accommodation for any part of the application and hiring process.\n        ",
    "location": "Yonkers, NY"
  },
  "79": {
    "id": "4182805473",
    "Company": "Xperi Inc.",
    "title": "Data Engineer",
    "description": "Xperi invents, develops and delivers technologies that create extraordinary experiences at home and on the go for millions of people around the world. Powering billions of consumer electronics, connected cars and digital content titles, we make entertainment more immersive, driving more intelligent and every interaction seamlessly personalized through our renowned consumer brands: DTS\u00ae, HD Radio\u2122, IMAX\u00ae Enhanced and TiVo\u00ae.Xperi (NYSE: XPER) is a publicly traded technology company headquartered in San Jose, CA with over 2,000 employees across North America, Europe and Asia. Come join a thriving team where you can play an integral role in shaping the future of entertainment technology.\u00a0Title: Data EngineerPrimary Job Responsibilities:\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Support in designing, developing and maintaining data pipelines\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Collaborate with DevOps, Data Engineering, and analytics research teams\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Develop, test, and launch analytics reports and dashboards\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Apply expertise in quantitative analysis, data mining\u00a0and communicate results within the data analytics team\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Document processes, issues, and resolutions as needed\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Identify and Implement automation opportunities\u00a0\u00a0Qualifications:\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1+ year experience working on ETL pipelines and data analytics\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Bachelor\u2019s Degree in computer science, Math, Engineering, or related quantitative field\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Write complex SQL and ETL processes for data at scale\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Working knowledge of programming in Python.\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Knowledge of Cloud services like AWS environment including S3, cloud computing approach, Athena, and EMR or Other cloud services like GCP, Azure\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Knowledge of scheduling, automation & orchestration software (such as Airflow, Control-M, Cloud Formation)\u00a0\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Knowledge of Enterprise software like Databricks or Qubole and BI platform like Tableau, Power BI\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Excellent analytical, troubleshooting and communication skillsLife @ Xperi:At Xperi, we value People, Customers, Performance and Innovation. We are dedicated to creating a workplace where all employees have a voice and sense of belonging, feel safe and valued, and are acknowledged for how their unique differences contribute to organizational culture and business outcomes.Our employees and their families are important to us, and our comprehensive pay, stock and benefits programs reflect that. Xperi supports personal well-being, builds financial security and enables employees to share in our collective success.Rewards include:Competitive compensation (salary, equity and bonuses) and comprehensive benefits designed to foster work-life balance, care for your health, protect your finances and help you save and invest for the future.Generous paid time away from work, including flexible time off, holidays and sick time, health and wellness initiatives, and a charitable match program to help you give back to your community.Great perks, which vary by location and can be site-specific: employee discounts, transportation reimbursements, subsidized cafes and fitness facilities.A flexible, hybrid work environment combining the best of in-office collaboration and community-building along with the benefits of working from home.The estimated base salary range for this full-time position is $101,469 - $124,300 plus bonus, equity, and benefits, and can vary if outside of this location. Our salary ranges are determined by role, level, and location. Within the range, individual pay is determined by work location and additional factors, including job-related skills, competencies, experience, market demands, internal parity, and relevant education or training. Your recruiter can share more about the specific salary range and perks and benefits for your location during the hiring process.\n",
    "location": "San Jose, CA"
  },
  "80": {
    "id": "4184546237",
    "Company": "U.S. Bank",
    "title": "Software Engineer 1- Data Engineer",
    "description": "At U.S. Bank, we\u2019re on a journey to do our best. Helping the customers and businesses we serve to make better and smarter financial decisions and enabling the communities we support to grow and succeed. We believe it takes all of us to bring our shared ambition to life, and each person is unique in their potential. A career with U.S. Bank gives you a wide, ever-growing range of opportunities to discover what makes you thrive at every stage of your career. Try new things, learn new skills and discover what you excel at\u2014all from Day One.Job DescriptionThis position will be responsible for the analysis, design, testing, development and maintenance of best in class software experiences. The candidate is a self-motivated individual who can collaborate with a team and across the organization. The candidate takes responsibility of the software artifacts produced adhering to U.S. Bank standards in order to ensure minimal impact to the customer experience. The candidate will be adept with the agile software development lifecycle and DevOps principles.Essential Responsibilities: Responsible for designing, developing, testing, operating and maintaining products Takes full stack ownership by consistently writing production-ready and testable code Consistently creates optimal design adhering to architectural best practices; considers scalability, reliability and performance of systems/contexts affected when defining technical designs Performs analysis on failures, propose design changes, and encourage operational improvements Makes sound design/coding decisions keeping customer experience in the forefront Takes feedback from code review and apply changes to meet standards Conducts code reviews to provide guidance on engineering best practices and compliance with development procedures Accountable for ensuring all aspects of product development follow compliance and security best practices Exhibits relentless focus in software reliability engineering standards embedded into development standards Embraces emerging technology opportunities and contributes to the best practices in support of the bank\u2019s technology transformation Contributes to a culture of innovation, collaboration and continuous improvement Reviews tasks critically and ensures they are appropriately prioritized and sized for incremental delivery; anticipates and communicates blockers and delays before they require escalationBasic Qualifications Bachelor\u2019s degree, or equivalent work experience Two to three years of relevant experiencePreferred Skills/ExperienceStrong experience with database management tools such as Apache Cassandra, Apache Hive, NoSQL, Data API, Data Modeling. Experience designing and developing\u202fdata\u202fprocessing and ETL frameworks and tools, such as Kafka Streaming, Spark, MySQL, Postgres Big data experience such as Spark and Hadoop preferredProficient understanding of algorithms,\u202fdata\u202fstructures, architectural design patterns and best practices Strong programming skills in SQL, Python and/or Java Experience with\u202fGCP and BigQuery data\u202fwarehouse technologies preferredHands-on Experience with building relational and/or dimensional conceptual/logical\u202fdata\u202fmodels, transforming conceptual/logical\u202fdata\u202fmodels into physical models and\u202fdata\u202fartifacts, and in the development of\u202fdata\u202farchitectures using application/tools Experience with web services and API integration Familiarity with one of the core cloud provider servicesFamiliar with Data platform technology Knowledge of Data integration process. Location ExpectationsThe role offers a hybrid/flexible schedule, which means there's an in-office expectation of 3 or more days per week and the flexibility to work outside the office location for the other days. If there\u2019s anything we can do to accommodate a disability during any portion of the application or hiring process, please refer to our disability accommodations for applicants.Benefits:Our approach to benefits and total rewards considers our team members\u2019 whole selves and what may be needed to thrive in and outside work. That's why our benefits are designed to help you and your family boost your health, protect your financial security and give you peace of mind. Our benefits include the following (some may vary based on role, location or hours):Healthcare (medical, dental, vision)Basic term and optional term life insuranceShort-term and long-term disabilityPregnancy disability and parental leave401(k) and employer-funded retirement planPaid vacation (from two to five weeks depending on salary grade and tenure)Up to 11 paid holiday opportunitiesAdoption assistanceSick and Safe Leave accruals of one hour for every 30 worked, up to 80 hours per calendar year unless otherwise provided by lawEEO is the LawU.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Applicants can learn more about the company\u2019s status as an equal opportunity employer by viewing the federal KNOW YOUR RIGHTS EEO poster.E-VerifyU.S. Bank participates in the U.S. Department of Homeland Security E-Verify program in all facilities located in the United States and certain U.S. territories. The E-Verify program is an Internet-based employment eligibility verification system operated by the U.S. Citizenship and Immigration Services. Learn more about the E-Verify program.The salary range reflects figures based on the primary location, which is listed first. The actual range for the role may differ based on the location of the role. In addition to salary, U.S. Bank offers a comprehensive benefits package, including incentive and recognition programs, equity stock purchase 401(k) contribution and pension (all benefits are subject to eligibility requirements). Pay Range: $92,820.00 - $109,200.00 - $120,120.00U.S. Bank will consider qualified applicants with arrest or conviction records for employment. U.S. Bank conducts background checks consistent with applicable local laws, including the Los Angeles County Fair Chance Ordinance and the California Fair Chance Act as well as the San Francisco Fair Chance Ordinance. U.S. Bank is subject to, and conducts background checks consistent with the requirements of Section 19 of the Federal Deposit Insurance Act (FDIA). In addition, certain positions may also be subject to the requirements of FINRA, NMLS registration, Reg Z, Reg G, OFAC, the NFA, the FCPA, the Bank Secrecy Act, the SAFE Act, and/or federal guidelines applicable to an agreement, such as those related to ethics, safety, or operational procedures.Applicants must be able to comply with U.S. Bank policies and procedures including the Code of Ethics and Business Conduct and related workplace conduct and safety policies.\n        ",
    "location": "Irving, TX"
  },
  "81": {
    "id": "4160671914",
    "Company": "Discover Financial Services",
    "title": "Data Engineer (multiple openings) - IHM",
    "description": "Discover. A brighter future.With us, you\u2019ll do meaningful work from Day 1. Our collaborative culture is built on three core behaviors: We Play to Win, We Get Better Every Day & We Succeed Together. And we mean it \u2014 we want you to grow and make a difference at one of the world's leading digital banking and payments companies. We value what makes you unique so that you have an opportunity to shine.Come build your future, while being the reason millions of people find a brighter financial future with Discover.Job DescriptionEmployer: DFS Corporate Services LLCJob Title: Data Engineer (multiple openings)Job Location: Riverwoods, IllinoisJob Type: Full TimeDuties: Develops and troubleshoots data integration solutions with complex data transformations and provides guidance to other team members. Influences other team members to achieve commitments per guidance from Chapter Leads and actively contributes to agile ceremonies. Telecommuting and/or working from home may be permissible pursuant to company policies. 10% domestic travel is required.Requirements: Employer will accept a Bachelor's degree in Computer Science or related field and 3 years of experience in the job offered or in an Engineering-related occupation.Position Required Skills: Position requires experience in the following: ETL/ELT Tools including AbInitio, DataStage, and Informatica; Cloud Tools, Databases, and Platforms including AWS, Snowflake, GCP, and Azure; Programming languages such as Unix scripting or Python; Leveraging CI/CD framework for data integration and Open Source; Key infrastructure concepts, including data centers and cloud hosting platforms, to support business data needs; Optimizing relational SQL and NoSQL.Position eligible for incentives under Employee Referral ProgramRate of Pay: The base pay for this position generally ranges between $122,117.00 to $150,200.00. Additional incentives may be provided as part of a market competitive total compensation package. Factors, such as but not limited to, geographical location, relevant experience, education, and skill level may impact the pay for this position. Benefits: We also offer a range of benefits and programs based on eligibility. These benefits include: Paid Parental Leave; Paid Time Off; 401(k) Plan; Medical, Dental, Vision, & Health Savings Account; STD, Life, LTD and AD&D; Recognition Program; Education Assistance; Commuter Benefits; Family Support Programs; Employee Stock Purchase Plan. Learn more at mydiscoverbenefits.com.QUALIFIED APPLICANTS: Please apply directly through our website by clicking on \u201cApply Now.\u201dApplication DeadlineThe application window for this position is anticipated to close on Apr-21-2025. We encourage you to apply as soon as possible. The posting may be available past this date, but it is not guaranteed.BenefitsWe also offer a range of benefits and programs based on eligibility. These benefits include:Paid Parental LeavePaid Time Off401(k) PlanMedical, Dental, Vision, & Health Savings AccountShort and Long Term Disability, Life, and Accidental Death & Dismemberment insurancesRecognition ProgramEducation AssistanceCommuter BenefitsFamily Support ProgramsEmployee Stock Purchase PlanLearn more at mydiscoverbenefits.com.What are you waiting for? Apply today!All Discover employees place our customers at the very center of our work. To deliver on our promises to our customers, each of us contribute every day to a culture that values compliance and risk management.Discover is committed to a diverse and inclusive workplace. Discover is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status, or other legally protected status. (Know Your Rights & Pay Transparency Nondiscrimination Provision)Discover complies with federal, state, and local laws applicable to qualified individuals with disabilities and is committed to providing reasonable accommodations. If you require a reasonable accommodation to search for a position, to complete an application, and/or to participate in an interview, please email HireAccommodation@discover.com. Any information you provide regarding your accommodation needs will be kept confidential and will only be used to determine and provide necessary accommodation.At Discover, we are committed to creating an inclusive and equitable workplace through our Fair Chance Hiring practices. Fair Chance Hiring means that we base our hiring decisions on an applicant\u2019s qualifications rather than their criminal record. All our positions are subject to Section 19 of the Federal Deposit Insurance Act. Our applicants go through a background check, and we follow all applicable local laws, including the Los Angeles County Fair Chance Hiring Ordinance (LA County Fair Chance).Applicants must be 18 or older at the time of hire.\n        ",
    "location": "Riverwoods, IL"
  },
  "82": {
    "id": "4176111658",
    "Company": "Preferred Travel Group",
    "title": "Data Engineer I",
    "description": "General SummaryThe Data Engineer I assists in building and maintaining the company\u2019s data systems, including writing queries, automating data workflows, and creating integrations to support various business functions. This role provides an opportunity to learn under the guidance of senior team members.ORGANIZATIONAL RELATIONSHIPUnder the direct supervision of the Director of Data, AI, and Integrations, the Data Engineer works closely with other engineers in the Data and QA teams.Duties & ResponsibilitiesData EngineeringWrite SQL queries and Python scripts to manipulate and transform data.Create and maintain automated workflows using tools like Azure Data Factory, Jupyter Notebooks, or SQL Agent.Integration DevelopmentSupport the development of web APIs (SOAP, REST) for data sharing between systems.Reporting and VisualizationBuild reports, dashboards, and data visualizations using SSRS, Power BI, and Excel.Operational SupportFollow procedural checklists for recurring processes.Troubleshoot issues reported by users and implement fixes.CollaborationManage tasks in the ticketing system and provide updates during stand-ups.Commit work to shared Git repositories with proper version control practices.Participate in team code reviews.QualificationsBachelor\u2019s degree in Computer Science, Information Systems, or equivalent experience (e.g., bootcamps, certifications, internships).Familiar with at least one relevant language, such as SQL, Python, or C#Strong problem-solving skillsAbility to learn new tools quicklyWORKING CONDITIONSThis is a work from home position, can be hybrid if desired and near our locations in Chicago, IL or Newport Beach, CA. All technology required will be provided.Required TrainingOrientation via some live remote and some pre-recorded video sessionsIT security trainingInternal development process and proceduresDISCLAIMER: The above information on this description has been designed to indicate the general nature and level of work performed by associates within this classification. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job.Salary range: $55,000-$70,000 annually; actual compensation within this range will be determined by multiple factors including candidate location, experience and expertise.\n        ",
    "location": "Chicago, IL"
  },
  "83": {
    "id": "4156379611",
    "Company": "Moore",
    "title": "Data Integration Engineer",
    "description": "As the Data Integration Engineer, you will play a pivotal role in building and optimizing global data integration processes. You will work closely with our data ingestion platform to write and maintain Scala code to feed ETL (Extract, Transform, Load) pipelines, ensure data consistency, and develop scalable solutions for integrating diverse datasets from multiple sources. Your work will contribute to ensuring high data quality, integrity, and accessibility for the entire enterprise across multiple use cases.Moore is a data-driven constituent experience management (CXM) company achieving accelerated growth for clients through integrated supporter experiences across all platforms, channels and devices. We are an innovation-led company that is the largest marketing, data and fundraising company in North America serving the purpose-driven industry with clients across education, association, political and commercial sectors.Check out www.WeAreMoore.com for more information.Your Impact: Design and implement data integration solutions by writing and optimizing Scala code to support scalable data ingestion and transformation pipelines. Develop automated processes to ensure the consistency of incoming data across multiple sources and ensure alignment with universal data standards. Implement automated scripts and processes in Scala to cleanse and standardize incoming data to maintain consistency across datasets. Continuously enhance the performance and scalability of data ingestion processes to handle growing data volume and new data types. Ensure accurate data transformation from raw input to structured formats suitable for downstream application. Collaborate with cross-functional team members such as data architects, data scientists, data analysts, and operations teams to integrate new data sources and build efficient data conversion processes. Create comprehensive documentation of data integration solutions, best practices, and coding standards for both technical and non-technical audiences. Participate in Agile workflows involving large development efforts. Design and develop automated checks to ensure the completeness, accuracy, and validity of ingested data, identifying anomalies or inconsistencies as early as possible. Establish key performance indicators (KPIs) for data quality and implement processes to track and report these metrics regularly. Create robust validation frameworks that can automatically detect and flag issues like missing, duplicated, or incorrect data before it enters downstream systems and implement fallback mechanisms for error handling to maintain data integrity. Maintain a log of data quality issues, how they were addressed, and lessons learned to help improve future integration processes and ensure transparency across the organization. Conduct thorough testing and quality checks on new data sources before they are integrated into the data pipeline, ensuring alignment with internal data quality requirements. Collaborate with other teams to investigate and resolve data discrepancies between source systems and integrated data, ensuring any issues are quickly resolved.Your Profile: Bachelor\u2019s degree in Computer Science, Data Engineering, or a related field (or equivalent work experience). 3+ years of experience in data integration, ETL development, or related roles with a focus on large-scale data systems. At least 2 years of experience with a Cloud database such as Snowflake or Azure SQL. 1 or more years of experience writing and optimizing production-grade Scala code, particularly for data processing and transformation. At least 1 year of experience using CRM software. In-depth understanding of donor management and data services, data processing, and analytics. Experience working in a data co-op or data-driven environment where multiple stakeholders contribute to and use shared data. Ability to troubleshoot and optimize data flows and resolve performance bottlenecks in data processes. Expertise in developing efficient code and writing advanced queries to handle large datasets and complex data transformations. Strong attention to detail with a focus on data quality, consistency, and integrity. Strong problem-solving skills and capable of handling complex data integration challenges in a fast-paced environment. Excellent written and verbal communication skills, with the ability to explain technical concepts to both technical and non-technical audiences. Experience with JIRA and GitHub a plus.How We\u2019ll Support You:Join the largest marketing and fundraising company in North America serving the nonprofit industry where we prioritize innovation and professional growth. Collaborate with industry subject matter experts with over 5,000 employees across the enterprise. To help you stay energized, engaged and inspired, we offer a wide range of benefits including comprehensive healthcare, paid holidays and generous paid time off so you can have the time and space to recharge and pursue your other passions and be with the people you care about.Moore is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.\n",
    "location": "Peabody, MA"
  },
  "84": {
    "id": "4174954424",
    "Company": "HAVI",
    "title": "Data Engineer",
    "description": "Job DescriptionA talented and motivated individual who will be part of a group of skilled analytic resources to architect, des\u00ad\u00adign, implement, enhance, and maintain highly scalable, available, secure, and elastic cloud-ready data solutions based on industry best practices using cutting-edge technologies for our Data Science team.This person will become an expert in our data domains, act as a trusted partner and advisor to solutions architects and data scientists, and become a crucial part of the analytics solution lifecycle \u2013 from prototype to production and operations of our data science and advanced analytics solutions in areas such as promotions, supply and demand planning, item/menu level analytics, supply chain simulations, and optimization, competitive benchmarking, and root cause analysis. Ensuring the high quality of the deliverables will be an essential part of this role.Be current on trends and developments in the market with an eye on bringing new advancements into our data solutions.This role will be part of the Analytics and Insights team in the Global Planning and Analytics group.Data Management, Analytics and Business Insights, Global Planning and AnalyticsData Science, Analytics and Business Insights, Global Planning and AnalyticsDecision Science, Analytics and Business Insights, Global Planning and AnalyticsSupply Chain TechnologyProduct Management, Global Planning and AnalyticsGlobal Supply Chain usersQualificationsBachelor\u2019s degree in computer science, data management, information systems, information science or a related field; advanced degree in computer science, data management, information systems, information science or a related field preferredThe ideal candidate will have a combination of IT skills, data governance skills, analytics skills, and supply chain knowledge with a technical or computer science degree.Knowledge and Experience3+ years of work experience in data management and engineering disciplines, including data pipelines' design, integration, modeling, optimization and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks3+ years of work experience working in cross-functional teams and collaborating with business stakeholders in support of data management and analytics initiativesStrong experience with tools for Object-oriented/object function scripting using languages such as Python, Java, C++, Scala, and R.Strong experience in Python and DatabricksStrong experience with database programming languages for relational databases and non-relational databasesExperience working within the Azure ecosystemSkills and Leadership BehaviorsDirectionSelf-starter, with flexibility and patience in a fluid and fast-paced environmentAbility to synthesize complex information and communicate results effectivelyDriveSets high standards, clear goals & focus on problem-solvingChallenges the status quo to enhance solution qualityPassionate about data engineering and data solutionsCommitment to continuous learning and developmentExecutionDesigns and develops high-quality data deliverables working with complex and large volumes of dataAbility to thrive in a changing environment and deal with ambiguityInfluenceAble to quickly identify potential roadblocks and devise ways to address challengesLeverages from and collaborates with all key stakeholders to drive efficiency and effectivenessInnovationEncourages others to challenge the status quo, thinks out of the boxBrings new perspectives on data solutionsRelationship & people leadershipAble positively impact the performance of team membersBuilds trust and credibility with analytics colleagues and other supply chain teamsResponsibilitiesResponsible for working with the data management, data science, decision science, and technology teams to solve supply chain problems such as demand and supply planning, replenishment, pricing, and optimizationDevelop/refine the data requirements, design/develop data deliverables, and optimize data pipelines in non-production and production environmentsDesign, build, and manage/monitor data pipelines for data structures encompassing data transformation, data models, schemas, metadata, and workload management. The ability to work with both IT and businessIntegrate analytics and data science output into business processes and workflowsBuild and optimize data pipelines, pipeline architectures, and integrated datasets. These should include ETL/ELT, data replication/CI-CD, API design, and accessWork with and optimize existing ETL processes and data integration and preparation flows and help move them to productionWork with popular data discovery, analytics, and BI tools such as Tableau and Power BI for semantic-layer data discovery Adept in agile methodologies and capable of applying DevOps and DataOps principles to data pipelines to improve communication, integration, reuse, and automation of data flows between data managers and data consumers across the organizationStarting Salary $95,000-$105,000 with a 5% targeted bonusWe OfferTOTAL REWARDSBenefitsOur total rewards philosophy integrates programs for compensation, benefits, recognition, learning and development, corporate culture, corporate citizenship and work-life balance. While individual program components may differ by country, some things remain constant:Our commitment to rewarding resultsOpportunities to work with talented and driven individuals at every level of our company who respect each other, treat each other fairly and hold one another accountable for our customers\u2019\u2014and our company\u2019s\u2014successThere's more ...Inclusive employee resource groupsGenerous Medical, Dental, Vision And Other Great BenefitsPaid parental and medical leave programs401(k) with a company match component and profit sharing15 days of paid time off plus company holidaysHybrid work model with flexibilityTuition reimbursement and student loan repayment assistanceEQUAL OPPORTUNITY EMPLOYERWe are an equal opportunity employer and we value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, sex, gender, gender expression, sexual orientation, age, marital status, veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation. RECRUITING AGENCIESHAVI does not accept agency resumes submitted by third-party vendors unless a valid agreement has been signed and the HAVI Talent Acquisition Team has granted authorization for submissions for a specified position. Please do not submit or forward resumes to our site, HAVI employees, or any other company location. HAVI is not responsible for any fees related to unsolicited resumes.\n        ",
    "location": "Chicago, IL"
  },
  "85": {
    "id": "4117848871",
    "Company": "Q2",
    "title": "Data Engineer",
    "description": "As passionate about our people as we are about our mission.What We\u2019re All AboutQ2 is proud of delivering our mobile banking platform and technology solutions, globally, to more than 22 million end users across our 1,300 financial institutions and fintech clients. At Q2, our mission is simple: Build strong, diverse communities by strengthening their financial institutions. We accomplish that by investing in the communities where both our customers and employees serve and live.What Makes Q2 Special?Being as passionate about our people as we are about our mission. We celebrate our employees in many ways, including our \u201cCircle of Awesomeness\u201d award ceremony and day of employee celebration among others! We invest in the growth and development of our team members through ongoing learning opportunities, mentorship programs, internal mobility, and meaningful leadership relationships. We also know that nothing builds trust and collaboration like having fun. We hold an annual Dodgeball for Charity event at our Q2 Stadium in Austin, inviting other local companies to play, and community organizations we support to raise money and awareness together.The Job At-A-GlanceQ2 is seeking a Data Engineer to join our Data Insights organization. As part of the Data Engineering team, you will build and maintain the Cloud infrastructure for streaming data pipelines and messaging solutions.\u202fOur team collaborates closely with cross-functional teams to integrate Kafka into various applications and ensures optimal performance and reliability of the data infrastructure.A Typical DayDesign, implement, and manage Kafka-based data pipelines and messaging solutions to support critical business operations and enable real-time data processing. Configure, deploy, and maintain Kafka clusters, ensuring high availability and scalability to maximize uptime and support business growth. Monitor Kafka performance and troubleshoot issues to minimize downtime and ensure uninterrupted data flow, enhancing decision-making and operational efficiency. Collaborate with development teams to integrate Kafka into applications and services. Develop and maintain Kafka connectors such as JDBC, MongoDB, and S3 connectors, along with topics and schemas, to streamline data ingestion from databases, NoSQL data stores, and cloud storage, enabling faster data insights. Implement security measures to protect Kafka clusters and data streams, safeguarding sensitive information and maintaining regulatory compliance. Optimize Kafka configurations for performance, reliability, and scalability. Automate Kafka cluster operations using infrastructure-as-code tools like Terraform or Ansible to increase operational efficiency and reduce manual overhead. Bring Your Passion, Do What You Love. Here\u2019s What We\u2019re Looking For:Degree in Computer Science, Information Systems, or equivalent experience 3-5\u202fyears of related professional experience\u202f Advanced knowledge\u202fin\u202fmultiple areas\u202fof\u202f data transformations,\u202fdata\u202fpipelines, workflow automation,\u202fand scheduling systems\u202f\u202f data-centric systems architecture\u202f software engineering\u202fand\u202fdistributed software design\u202f database systems, data warehouses, distributed file storage and compute platforms\u202f Experience with several of the technologies we\u202fcurrently use:\u202f Cloud\u202fProviders: AWS, Azure\u202f Data Movement Tools: Kafka, Redis, Kinesis Data Pipelines: Apache Airflow, dbt Data Tools:\u202fPyspark, Snowpark, AWS Glue, Pandas, Databricks, SageMaker Databases: Snowflake,\u202fSqlServer,\u202fPostgresql\u202f Containerization: Kubernetes, Docker\u202f Languages: Python, C#, Golang, Bash, SQL\u202f CI/CD: GitLab, Azure DevOps SCM: Git,\u202fGithub, GitLab\u202f \u202fWe look for engineers that are comfortable working in a team setting as well as working individually under their own responsibility. Regardless of experience level, all our engineers strive to learn more than they knew yesterday. We pride ourselves on elevating the folks around us. We grow leaders and encourage engineers at all levels of experience to take ownership of challenging work.\u202f\u202f\u202fThis position requires fluent written and oral communication in English. Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.Health & WellnessHybrid Work OpportunitiesFlexible Time Off Career Development & Mentoring Programs Health & Wellness Benefits, including competitive health insurance offerings and generous paid parental leave for eligible new parents Community Volunteering & Company Philanthropy Programs Employee Peer Recognition Programs \u2013 \u201cYou Earned it\u201dClick here to find out more about the benefits we offer.How We Give Back To The CommunityYou can learn more about our Q2 Spark Program, Q2 Philanthropy fund, and our employee volunteering programs on our Q2 Community page. Q2 supports dozens of wide-reaching organizations, such as the African American Leadership Institute, and The Trevor Project, promoting diversity and success in leadership and technology. Other deserving beneficiaries include Resource Center helping LGBTQ communities, JDRF, and Homes for our Troops, a group helping veterans rebuild their lives with specially adapted homes.At Q2, our goal is to be a diverse and inclusive workforce that fosters mutual respect for our employees and the communities we serve. Q2 is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status.\n",
    "location": "Austin, TX"
  },
  "86": {
    "id": "4188421521",
    "Company": "Shamrock Trading Corporation",
    "title": "Data Engineer",
    "description": "DescriptionCompany OverviewShamrock Trading Corporation is the parent company for a family of brands in transportation services, finance and technology. Headquartered in Overland Park, KS, Shamrock is frequently recognized among the \u201cBest Places to Work\u201d in Kansas City and Chicago and was most recently recognized as one of America\u2019s top 100 \u201cMost Loved Workplaces\u201d by Newsweek. We also have offices in Atlanta, Chicago, Dallas, Ft. Lauderdale, Houston, Laredo, Nashville, Philadelphia and Phoenix.With an average annual revenue growth of 25% over several decades, Shamrock\u2019s success is attributed to three key factors: hiring the best people, cultivating long-term relationships with our customers and continually evolving in the marketplace.ResponsibilitiesShamrock Trading Corporation is looking for a Data Engineer who wants to utilize their expertise in data warehousing, data pipeline creation/support and analytical reporting skills by joining our Data Services team. This role is responsible for gathering and analyzing data from several internal and external sources, designing a cloud-focused data platform for analytics and business intelligence, reliably providing data to our analysts. This role requires significant understanding of data mining and analytical techniques. An ideal candidate will have strong technical capabilities, business acumen, and the ability to work effectively with cross-functional teams. Responsibilities include but are not limited to:Develop & Maintain Scalable Data Pipelines: Build, optimize, and maintain ETL/ELT pipelines using Databricks, Apache Spark, and Delta Lake.Optimize Data Processing: Implement performance tuning techniques to improve Spark-based workloads.Cloud Data Engineering: Work with AWS services (S3, Lambda, Glue, Redshift, etc.) to design and implement robust data architectures.Real-time & Streaming Data: Develop streaming solutions using Kafka and Databricks Structured Streaming.Data Quality & Governance: Implement data validation, observability, and governance best practices using Unity Catalog or other tools.Cross-functional Collaboration: Partner with analysts, data scientists, and application engineers to ensure data meets business needs.Automation & CI/CD: Implement infrastructure-as-code (IaC) and CI/CD best practices for data pipelines using tools like Terraform, dbt, and GitHub Actions. QualificationsBachelor\u2019s degree in computer science, data science or related technical field, or equivalent practical experience2-5+ years of experience in data engineering, with a focus on cloud-based platforms.Strong hands-on experience with Databricks (including Spark, Delta Lake, and MLflow).Experience building and maintaining AWS based data pipelines: currently utilizing AWS Lambda, Docker / ECS, MSK, Airflow, Databricks, Unity CatalogDevelopment experience utilizing two or more of the following:Python: (Pandas/Numpy, Boto3, SimpleSalesforce)Databricks (pySpark, pySQL, DLT)Apache SparkKafka and the Kafka Connect ecosystem (schema registry and Avro)Familiarity with CI/CD for data pipelines and infrastructure as code (Terraform, dbt)Strong SQL skills for data transformation and performance tuning.Understanding of data security and governance best practices.Enthusiasm for working directly with customer teams (Business units and internal IT) Preferred QualificationsProven experience with relational and NoSQL databases (e.g. Postgres, Redshift, MongoDB)Experience with version control (git) and peer code reviewsFamiliarity with data lakehouse architectures and optimization strategies.Familiarity with data visualization techniques using tools such as Grafana, PowerBI, AWS Quick Sight, and Excel. Benefits PackageAt Shamrock we hire bright, ambitious people and give them the tools they need to be successful. By investing in training and development, we hope to become a long-term career for employees, where there are always opportunities for advancement. Shamrock also offers a premier set of benefits for employees and their families:Medical: Fully paid healthcare, dental and vision premiums for employees and eligible dependents Work-Life Balance: Competitive PTO and paid leave policies Financial: Generous company 401(k) contributions and employee stock ownership after one year Wellness: Onsite gym and discounted membership to select fitness centers. Jogging trails available at Overland Park offices\n",
    "location": "Overland Park, KS"
  },
  "87": {
    "id": "4167870671",
    "Company": "TieTalent",
    "title": "Data Engineer Intern",
    "description": "AboutCerro Wire LLCCome join a team where People make the difference! As a part of Marmon Holdings, Inc., a highly decentralized organization, we rely heavily on people with the aptitude, attitude, and entrepreneurial spirit to drive our success, and we're committed to attracting and retaining top talent.A Summer Experience. A Lifetime of Value.We'll meet you where you are and help you go further.This summer, we're committed to bringing early-in-career talent together, trust you to own your work and help you level up through professional development, networking, and exposure to real-world projects.We're doing things that matter.Energizing North America: That's the impact of your work at Cerrowire. Copper building wire doesn't just supply electricity. It turns houses into homes. It allows hospitals to deliver the care patients need. It keeps every single industry we rely on up and running. And it all starts with you.As a part of Marmon, your impact also goes way beyond North America. You're helping keep millions around the world healthy, connected and safe.Join the Biggest Small Business You'll Ever Find.Headquartered in Hartselle, Alabama, Cerrowire is a copper wire manufacturing company with plants in Alabama, Georgia, Indiana, and Utah. We build, energize, and inspire by supplying building wire and cable throughout North America for commercial, industrial and residential use. We bring power to life.Cerrowire is hiring talented and innovative interns looking to power up their futures and forge a meaningful career with us. Be part of our efforts to help improve the quality of life for millions of people by engineering solutions to many of the world's greatest needs.You will gain substantial personal learning and career development opportunities through ownership of real-world job assignments, strong coaching and mentorship, and networking opportunities with senior leaders and other interns across our organization. Our interns receive the opportunity to showcase their achievements to our Leaders for the opportunity to grow and make an impact in the current workplace, across a group, and around the world.At Cerrowire, a Marmon company, you get the best of both worlds. The strength and stability that comes with being part of Berkshire Hathaway, plus the autonomy and opportunity that comes with working at one of our 120+ companies.What You'll DoThe Data Engineer Intern will design and implement an automated ETL pipeline to collect and process data from production equipment, sensors, and other sources for analysis. They will manage and evaluate solutions for large scale data processing systems, data warehousing services and solutions using advanced database technologies. Additional projects and assignments as needed.What You'll NeedPursuing an undergraduate or graduate degree in Computer Science, or related fieldRising Junior or SeniorRequired Experience With SQL, Power BI, AzurePreferred experience with Python, Snowflake, Apache, Git, or related toolsExcellent written, verbal communication and presentation skillsAbility to effectively communicate issues and solutions to all levels of the teamStrong analytical and problem-solving skillsCompensation$20.00 - $24.00 per hour, commensurate with relevant experience and educational backgroundWork Hours/Length Of ProgramThe internship will run for 12 weeks from May to AugustFull Time, targeting 40 hours per week (possibility for more flexible hours and Part Time if necessary)Exact start and end dates are flexible based on school schedules and the needs of the businessThis is a paid internshipWorking Conditions And Physical DemandsThis position is based in our Corporate Office, and employees are expected to wear business casual dress. The office is connected to a state-of-the-art manufacturing facility and may involve general exposure to a manufacturing environment, including exposure to noise, temperature fluctuations, or other factors. Employees entering the manufacturing facility are required to wear high visibility vests, eye protection, ear protection, and steel-toed shoes.Successful completion of a drug screening is required for this roleThis position involves sitting for extended periods of time.May involve close vision, color vision, depth perception, focus adjustment, and viewing computer monitor for extended periods of time.Involves manual dexterity for using keyboard, mouse, and other office equipment.Location: Hartselle, AL; Remote with 20% on-site, Must be located near Hartselle/Huntsville/Birmingham, etc.Following receipt of a conditional offer of employment, candidates will be required to complete additional job-related screening processes as permitted or required by applicable law.We are an equal opportunity employer, and all applicants will be considered for employment without attention to their membership in any protected class. If you require any reasonable accommodation to complete your application or any part of the recruiting process, please email your request to careers@marmon.com, and please be sure to include the title and the location of the position for which you are applying.Nice-to-have skillsSQLPower BIAzurePythonApacheGitAlabama, United StatesWork experienceData EngineerLanguagesEnglish\n",
    "location": "Alabama, United States"
  },
  "88": {
    "id": "4185731769",
    "Company": "Collabera",
    "title": "Data Engineer",
    "description": "Industry:\u00a0Banking & FinanceWork Location:\u00a0Chicago, ILJob Title:\u00a0Data EngineerPay Rate: $60-65/ hrMust Haves:Good Programming backgroundStrong Hadoop/ Big Data experiencePython, Spark/Scala, Kafka expertiseStrong knowledge in Spark and KafkaSr. Level experience\n",
    "location": "Chicago, IL"
  },
  "89": {
    "id": "4182947862",
    "Company": "Xylem",
    "title": "Data Engineer",
    "description": "Xylem is a Fortune 500 global water solutions company dedicated to advancing sustainable impact and empowering the people who make water work every day. As a leading water technology company with 23,000 employees operating in over 150 countries, Xylem is at the forefront of addressing the world's most critical water challenges. We invite passionate individuals to join our team, dedicated to exceeding customer expectations through innovative and sustainable solutions.The RoleAs a Data Engineer, you will design, develop, and optimize scalable data pipelines and workflows to support advanced analytics and business intelligence needs. You will collaborate with cross-functional teams to ensure data accessibility, integrity, and security.Core ResponsibilitiesDesign, develop, and implement robust data pipelines for data collection, transformation, and integration.Collaborate with senior engineers to architect scalable data solutions using Azure services, including Azure Data Factory and Databricks.Integrate data from SAP ERP systems and other enterprise platforms into modern cloud-based data ecosystems. Leverage Databricks for big data processing and workflow optimization.Work with stakeholders to understand data requirements, ensuring data quality and consistency.Maintain data governance practices to support compliance and security protocols.Support analytics teams by providing well-structured, reliable data for reporting and machine learning projects.Troubleshoot and resolve data pipeline and workflow issues.NOTE: Xylem does not provide visa sponsorship for this position QualificationsBachelor\u2019s degree in Computer Science, Data Engineering, Information Systems, or a related field.3\u20135 years of experience in data engineering or a related role.Proficiency in Azure technologies, including Azure Data Factory, Azure SQL Database, and Databricks.Experience with SAP data integration is a plus.Strong SQL and Python programming skills for data engineering tasks. Familiarity with data modeling concepts (e.g., star and snowflake schemas) and best practices.Experience with CI/CD pipelines for deploying data workflows and infrastructure.Knowledge of distributed file systems like Azure Data Lake or equivalent cloud storage solutions.Basic understanding of Apache Spark for distributed data processing.Strong problem-solving skills and a collaborative mindset.Technical KnowledgeDeep understanding of Azure cloud infrastructure and services, particularly those related to data management (e.g., Azure Data Lake, Azure Blob Storage, Azure SQL Database).Experience with Azure Data Factory (ADF) for orchestrating ETL pipelines and automating data workflows.Familiarity with Azure Databricks for big data processing, machine learning, and collaborative analytics.Expertise in Apache Spark for distributed data processing and large-scale analytics.Familiarity with Databricks, including managing clusters and optimizing performance for big data workloads.Understanding of Databricks Bronze, Silver, and Gold Model.Understanding of distributed file systems like HDFS and cloud-based equivalents like Azure Data Lake.Proficiency in SQL and NoSQL databases, including designing schemas, query optimization, and managing large datasets.Experience with data warehousing solutions like Databricks, Azure Synapse Analytics or Snowflake.Familiarity with connecting data Lakehouse\u2019s with Power BI.Understanding of OLAP (Online Analytical Processing) and OLTP (Online Transaction Processing) systems.Strong grasp of data modeling techniques, including conceptual, logical, and physical data models.Experience with star schema, snowflake schema, and normalization for designing scalable, performant databases.Knowledge of data architecture best practices, ensuring efficient data flow, storage, and retrieval.Knowledge of CI/CD pipelines for automating the deployment of data pipelines, databases, and infrastructure.Experience with infrastructure as code tools like Terraform or Azure Resource Manager to manage cloud resources.Preferred QualificationsFamiliarity with tools like Apache Airflow or other workflow orchestration tools.Knowledge of Azure Monitor or similar tools for system performance tracking.Certifications in Azure Data Engineering or related cloud platforms.Salary Range$76,300.00 - $122,100.00Join the global Xylem team to be a part of innovative technology solutions transforming water usage, conservation, and re-use. Our products impact public utilities, industrial sectors, residential areas, and commercial buildings, with a commitment to providing smart metering, network technologies, and advanced analytics for water, electric, and gas utilities. Partner with us in creating a world where water challenges are met with ingenuity and dedication; where we recognize the power of inclusion and belonging in driving innovation and allowing us to compete more effectively around the world.At Xylem, you'll not only contribute to solving water issues but also have the chance to make a difference through our paid Volunteer Program, Xylem Watermark. We prioritize our employees' well-being through inclusion and belonging as well as our Employee Resource Groups (ERG). Proud to be an Equal Employment Opportunity (including disability and veterans) and Affirmative Action workplace, Xylem fosters an inclusive environment free from discrimination or harassment.\u202fPlease note that the information in this job description outlines the general nature of the position and is not an exhaustive list of duties. Xylem is dedicated to providing reasonable accommodations to enable all employees to perform their essential job functions. We reserve the right to modify this job description and assign additional duties as needed. Embrace the opportunity to be part of Xylem's transformative journey in shaping the future of water technology! #XylemCareers #GlobalImpact #WaterInnovation\n        ",
    "location": "Charlotte, NC"
  },
  "90": {
    "id": "4164576735",
    "Company": "Travelers",
    "title": "Data Engineer I",
    "description": "Who Are We?Taking care of our customers, our communities and each other. That\u2019s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.Compensation OverviewThe annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.Salary Range$106,300.00 - $175,400.00Target Openings1What Is the Opportunity?Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate the stories found in data. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Artificial Intelligence, Machine Learning and business intelligence/insights.What Will You Do?Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.Design data solutions.Analyze sources to determine value and recommend data to include in analytical processes.Incorporate core data management competencies including data governance, data security and data quality.Collaborate within and across teams to support delivery and educate end users on data products/analytic environment.Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate.Test data movement, transformation code, and data components.Perform other duties as assigned. What Will Our Ideal Candidate Have?Bachelor\u2019s Degree in STEM related field or equivalentSix years of related experienceProficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices.Databricks experience requiredExperience with Cloud platforms (AWS)The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions.Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on.Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems.Strong verbal and written communication skills with the ability to interact with team members and business partners.Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities.What is a Must Have?Bachelor\u2019s degree or equivalent training with data tools, techniques, and manipulation.Four years of data engineering or equivalent experience.What Is in It for You?Health Insurance: Employees and their eligible family members \u2013 including spouses, domestic partners, and children \u2013 are eligible for coverage from the first day of employment.Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.Wellness Program: The Travelers wellness program is comprised of tools, discounts and resources that empower you to achieve your wellness goals and caregiving needs. In addition, our mental health program provides access to free professional counseling services, health coaching and other resources to support your daily life needs.Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.Employment PracticesTravelers is an equal opportunity employer. We value the unique abilities and talents each individual brings to our organization and recognize that we benefit in numerous ways from our differences.In accordance with local law, candidates seeking employment in Colorado are not required to disclose dates of attendance at or graduation from educational institutions.If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email so we may assist you.Travelers reserves the right to fill this position at a level above or below the level included in this posting.To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/.\n        ",
    "location": "Hartford, CT"
  },
  "91": {
    "id": "4177818058",
    "Company": "Focus Financial Partners",
    "title": "Jr. Data Engineer",
    "description": "Position SummaryThe Junior Data Engineer will play a key role in developing and maintaining robust data pipelines, ensuring data integrity, and supporting the firm's data infrastructure. This position is ideal for candidates with at least two years of hands-on experience in SQL and Python, coupled with a solid foundation in data engineering principles. The ideal candidate is a proactive learner, passionate about working with modern data technologies, and eager to contribute to the development of efficient, scalable data workflows.This role is based in St. Louis, MO.Primary ResponsibilitiesDevelop and optimize ELT pipelines under the guidance of senior engineers to ensure efficient data processing. Write and refine SQL queries for data transformation, processing, and performance optimization. Contribute to data model design to support scalable analytics and reporting solutions. Monitor, troubleshoot, and enhance data pipelines to maintain reliability, accuracy, and efficiency. Collaborate with data analysts and engineers to understand business needs and implement effective data solutions. Support data governance initiatives, including metadata management and data quality assurance. Document data workflows, system architecture, and transformation logic to enhance transparency and maintainability. Stay ahead of industry best practices in data engineering, automation, and cloud-based data platforms, continuously improving skills and processes.Qualifications2+ years of experience in data engineering, analytics, or software development.Proficiency in SQL and Python for data processing and transformation.Familiarity with cloud-based data platforms such as Snowflake, AWS, or GCP.Understanding of ELT/ETL concepts and data modeling principles.Exposure to modern data tools such as Airflow, dbt, or Spark is a plus.Strong problem-solving skills and attention to detail.Ability to work independently while being open to mentorship and learning.About Focus Financial PartnersFocus is a leading partnership of fiduciary wealth management and related financial services firms. Focus provides access to best practices, greater resources, and continuity planning for its affiliated advisory firms, which serve individuals, families, employers, and institutions with comprehensive financial services. Focus firms and their clients benefit from the solutions, synergies, scale, economics, and best practices offered by Focus to achieve their business objectives. For more information about Focus, please visit www.focusfinancialpartners.com .The annualized base pay range for this role is expected to be $55,000 - $65,000. Actual base pay could vary based on factors including but not limited to experience, subject matter expertise, geographic location where work will be performed, and the applicant's skill set. The base pay is just one component of the total compensation package for employees. Other rewards may include an annual cash bonus and a comprehensive benefits package.\n        ",
    "location": "St Louis, MO"
  },
  "92": {
    "id": "4173664780",
    "Company": "Optimum Healthcare IT",
    "title": "Junior Data Engineer",
    "description": "Entry Level Healthcare IT AnalystStart Your Career in Healthcare Information Technology Today!Getting your \ufb01rst job can be dif\ufb01cult when employers want experience, but to gain that experience, you need your \ufb01rst job. We bridge the gap between your education and professional career by helping you gain the experience and training you need within the Healthcare Information Technology Industry.Optimum Healthcare IT is looking for recent college graduates with an interest in moving into the Healthcare IT Industry. Our Optimum CareerPath training program will equip you with the tools needed for your success as a Healthcare IT Analyst.Healthcare IT Analyst Job Responsibilities:\u00b7 The Healthcare IT Analyst will have primary responsibility for the design, build/configuration, testing, validation, documentation, and ongoing support for the Healthcare applications.\u00b7 This position will implement, administer, and support assigned systems under the guidance of senior members of the team.\u00b7 The position will have a good understanding of healthcare organizations, ancillary systems, and health system operations.\u00b7 Analyze and document user requirements, procedures, and problems to automate or improve existing systems. Review system capabilities, workflow, and scheduling limitations.\u00b7 Document workflows, configure and/or build activities, change management adherence, end-user notifications, training information, and status reporting in the appropriate system.\u00b7 Develop, document, and revise system design procedures, test procedures, and quality standards.\u00b7 Expand or modify the system to serve new purposes or improve workflows.\u00b7 Review and analyze the system and performance indicators to locate problems and correct errors. Escalate problems and issues to appropriate staff to ensure timely resolution.\u00b7 Coordinate projects, and schedule and facilitate meetings as necessary to complete assignments.\u00b7 Technical and functional analyst support of systems that may include Electronic Health Records platforms (Epic, Cerner), IT Project Management, ERP Systems (Workday, Oracle, PeopleSoft, UKG), ITSM applications (ServiceNow), data and analytics applications (Tableau, PowerBI), cloud deployments (GCP, Azure, AWS), and other digital platforms and services.Requirements:\u00b7 Bachelor\u2019s Degree\u00b7 US work authorization (This position is not open to any H1B /F1 OPT/STEM degrees)\u00b7 Excellent communication skills (verbal and written)\u00b7 Ability to exercise tact and good interpersonal skills\u00b7 Superb analytical and time management skills required\u00b7 Self-starter, self-motivated, high level of initiative\u00b7 Result-focused, ability to solve complex problems and resolve conflicts in a timely manner\u00b7 Internships or Research Project Work is highly desired in a healthcare setting\u00b7 Understanding of how data works and looks coming from different formats is preferred\u00b7 Experience with SQL\n",
    "location": "United States"
  },
  "93": {
    "id": "4183358389",
    "Company": "Continuum Media",
    "title": "Data Engineer",
    "description": "Job TypeFull-timeDescriptionFull-timeBase Salary: $100k \u2013 $110KLocation: Highlands Ranch, CO w/Hybrid Schedule or RemoteWho We AreContinuum Media is a national advertising platform specializing in providing solutions for brands to access and measure large sources of hard to reach television ad impressions. Headquartered in Denver with offices in New York City, Chicago, and the U.K., we are building a workplace where teammates, the company, and our clients can thrive. The degree to which we collectively succeed is predicated upon respect, trust, safety, transparency and personal accountability. We encourage camaraderie, laughter and an open-minded and curious workplace, while holding all employees to the highest professional and ethical standards. Talent runs deep, but character even deeper. Each office has its own unique character(s). We\u2019re spread out across time zones, even countries, but that doesn\u2019t mean we don\u2019t have a common sense of commitment.The Business Intelligence TeamThe Business Intelligence team is a highly collaborative, pragmatic, and iterative group of individuals. The team fully believes that each member should be a well-rounded contributor, eliminating single points of failure and ensuring seamlessness when delivering on the day-to-day and long-term projects alike, gaining the knowledge of not just specific functionalities, but all within the department. Whether it is creating and managing visualizations in our reporting platform for our customers to utilize on the front-end, engineering ETLs regarding numerous data sources feeding our dashboards and databases on the back-end, or supporting our operational teams with custom data processes, you will have a hand in the entirety of the BI platform.ResponsibilitiesEngineer robust, scalable ETLs for ingesting new datasets; utilize data for reporting purposesBuild and maintain dashboards for use by internal stakeholders and external clientsAutomate, modularize, and improve existing data ingestions while contributing to orchestration pipelines and expanding data quality assuranceSignificantly improve and expand the company\u2019s cloud-based, data platformImplement and/or refine CI/CD, IaC, test-driven development, agile, and reporting strategies for the department that can scaleGather requirements from clients, partners, and all internal departments via understanding their needs, challenges, and goalsCollaborate with software developers and data scientists to deliver cross-departmental solutionsCommunicate new features and lead trainings for internal users, clients, and partnersFoster and embrace a healthy culture of collaboration, strengthening the team\u2019s agile rootsRequirements1-3 years of experience in BI platform development and/or data engineering1-3 years of experience scripting and programming in Python1-3 years of experience with SQL-based database required (Redshift, Athena, SQL Server, MySQL, etc.)1-3 years of experience with one cloud computing service required (AWS, GCP, Azure, etc.)1-3 years of experience with Git version control required (Bitbucket, GitHub, etc.)1-2 years of experience with IaC tools preferred (CloudFormation, Terraform, etc.)1-2 years of experience building and maintaining CI/CD pipelines preferred (Bitbucket Pipelines, Jenkins, GitHub Actions, etc.)1-2 years of experience working within a Kanban team or environment preferred1-2 years of experience practicing test-driven development principles preferred1-2 years of experience with data visualization tools preferred (Tableau, QuickSight, Power BI, etc.)What We OfferWe provide employees with an environment in which they can develop their skills and grow as individuals within our team. We do this by providing training opportunities, conference attendance, and subscriptions to online courses. Our aim is that you will think of a role at Continuum Media as more than a salary and more than just a job. To demonstrate this to employees, we provide:100% employer-paid premiums for medical, vision, dental and disabilityParticipation in employee profit-sharing bonuses (subject to company performance)Life Insurance, EAP, HSA with employer contribution, Pet Insurance optionUnlimited PTOVacation Incentive ProgramHybrid In-Office Schedule401K company match\n",
    "location": "Denver, CO"
  },
  "94": {
    "id": "4119933863",
    "Company": "BeaconFire Inc.",
    "title": "Data Engineer",
    "description": "BeaconFire is based in Central NJ, specializing in Software Development, WebDevelopment, and Business Intelligence; looking for candidates who are good communicators and self-motivated. You will play a key role in building, maintaining, and operating integrations,reporting pipelines, and data transformation systems.Qualifications:\u25cf Passion for data and a deep desire to learn.\u25cf Master\u2019s Degree in Computer Science/Information Technology, Data Analytics/DataScience, or related discipline.\u25cf Intermediate Python. Experience in data processing is a plus. (Numpy, Pandas, etc)\u25cf Experience with relational databases (SQL Server, Oracle, MySQL, etc.)\u25cf Strong written and verbal communication skills.\u25cf Ability to work both independently and as part of a team.Responsibilities:\u25cf Collaborate with the analytics team to find reliable data solutions to meet the businessneeds.\u25cf Design and implement scalable ETL or ELT processes to support the business demand fordata.\u25cf Perform data extraction, manipulation, and production from database tables.\u25cf Build utilities, user-defined functions, and frameworks to better enable data flow patterns.\u25cf Build and incorporate automated unit tests, participate in integration testing efforts.\u25cf Work with teams to resolve operational & performance issues.\u25cf Work with architecture/engineering leads and other teams to ensure quality solutions areimplemented, and engineering best practices are defined and adhered to.Compensation: $65,000.00 to $80,000.00 /yearBeaconFire is an e-verified company. Work visa sponsorship is available.\n",
    "location": "New Jersey, United States"
  },
  "95": {
    "id": "4181217029",
    "Company": "Linq",
    "title": "Data Engineer",
    "description": "Job Title: Data EngineerLocation: Birmingham, ALDepartment: TechnologyEmployment Type: Full Time - In OfficeReports To: CTOCompensation Range: $65,000 - $95,000/year + Stock & Benefits. We do offer relocation assistance for qualified candidates.About the RoleWe are seeking a Data Engineer to join our technology team and play a critical role in managing and optimizing Linq\u2019s data infrastructure. In this role, you\u2019ll build and maintain the pipelines that power our data systems, ensuring accuracy, reliability, and alignment with business needs. You will work closely with cross-functional teams to ensure our data empowers company growth. Additionally, you will contribute to creating and maintaining data visualizations for telemetry and monitoring big boards. This is an in-office role.ResponsibilitiesEnsure data accuracy, reliability, and resiliency.Design, build, and maintain data pipelines and ETL processes to support business operations and analytics.Collaborate with teams to understand data requirements and align data reporting and systems with business needs.Monitor and optimize database performance and data workflows.Implement data validation and quality checks to maintain integrity and trust in our data.Provide technical expertise for data-related projects and initiatives.Collaborate on creating and maintaining data visualizations for telemetry and monitoring dashboards.What We\u2019re Looking ForStrong problem-solving skills and attention to detail.Proficiency in SQL, Python, and ETL tools.Familiarity with database technologies (e.g., Postgres, Redshift, MySQL, Snowflake).Knowledge of best practices for data governance and quality.Excellent communication and collaboration skills.Enthusiasm for building scalable, high-quality data systems.Understanding of data visualization platforms like Grafana.Nice to HavesExperience with dbt (data build tool) or similar tools.Experience with time series databases (e.g., InfluxDB, TimescaleDB).Experience with cloud platforms (e.g., AWS, GCP, Azure) and data warehousing solutions.Familiarity with APIs and integrating external data sources.Bachelor\u2019s degree in Computer Science, Data Science, or a related field.Why Join Linq?Health, dental, and vision insurance.Live and work in Birmingham, AL, where you can enjoy affordable housing, a thriving culinary scene, and easy access to outdoor activities, all while benefiting from a lower cost of living than other major cities.Generous PTO.Collaborative team culture with room for growth and learning.Opportunity to shape and optimize data infrastructure in a fast-growing company.Work on a talented, collaborative, and ambitious team, helping you grow your skills in an exciting and fast-paced environment.Competitive stock options as part of our commitment to shared success and ownership.\n",
    "location": "Birmingham, AL"
  },
  "96": {
    "id": "4129678250",
    "Company": "Jackson",
    "title": "Data Engineer I",
    "description": "If you are an internal associate, please login to Workday and apply through Jobs Hub.Job PurposeThe Data Engineer I will be responsible for building and supporting the data platform. The Data Engineer will provide the development and automation of computing processes to detect, and respond to opportunities in business operations. The Data Engineer will work with a variety of disparate datasets that encompass many disciplines and business units. They will strive to transform and implement true business integration, leveraging top-notch data integration best practices. Merging and securing data in a way that reduces the cost to maintain and increases the utilization of enterprise-wide data as an asset and developing business intelligence.Essential ResponsibilitiesEngineers and implements solutions that align to architecture patterns and security guidelines for data, compute and technology platforms, meeting business needs.Works in partnership with business product owners, and DevSecOps to engineer and build mature scalable and robust business capabilities.Follows IT architecture and IT operations guidelines and requirements while implementing optimal engineering solutions for the company.Participates in engineering and planning initiatives related to capabilities, future roadmaps, operations, and strategic planning.Engineers and implements business and technology innovation that drives the organization's top and bottom lines.Stays current with industry trends, making recommendations of new technologies that deliver strategic business value and reduce costs.Engineers and implements architecture solutions that maximize reuse and are efficient, maintainable, scalable, stable, highly available, portable, secure, and implemented correctly.Strong communication competency across all levels of stakeholders, providing engineering guidance and insight into best practices.Follows organizational policies and goals for IT security, change management, and operational risk.Synthesizes information into clear and concise materials with thoughtful attention to detail and quality.Provides engineering consulting services within domain to help achieve desired business and operational outcomes across Jackson.Engineers and implements the service management processes for maintaining the technology platforms in production.Acquires and maintains a working knowledge of Jackson processes and procedures across various departments.Assists in researching, planning, and executing the migration of on-prem data infrastructure to Azure, including evaluating tradeoffs between cost, performance, and maintainability.Develops and maintains data pipelines for complex business use cases.Develops, designs, and builds solutions on a platform dedicated to large-scale processing of various data sets.Collaborates with IT architecture and IT operations to identify and implement the most optimal data engineering solutions for the company.Collaborates with team to execute and iterate on application development, including interfacing withlegacy databases, parsing raw structured and unstructured data, documenting the data warehouse, debugging, and deploying to production environments.Collaborates with other teams to clarify and answer complex business questions using statistical and graphical approaches.Employs exceptional problem-solving skills, with the ability to see and solve issues before applications are moved to production systems.Sets, communicates, and reinforces technical standards.Leads and develops best practices for larger Data Engineer Community of Practice (Data CoP).Stays current with industry trends, makes recommendations of new technologies that deliver strategic business value and reduce costs.Performs other duties and/or projects as assigned.Knowledge, Skills And AbilitiesKnowledge of industry standards, emerging technologies; and security and system best practices.Excellent verbal and written communication skills including presentation creation and delivery.Experience with the Microsoft Cloud ecosystem.Strong organizational skills; ability to independently prioritize tasks and projects to meet deadlines.Strong collaboration skills with the ability to build consensus, influencing across all levels in the organization.Ability to learn and maintain a comprehensive understanding of finance/insurance business and technology.Knowledge of Lean and Agile principles, systems, and tools.Ability to explain and communicate technical concepts clearly.Ability to conduct gap analysis and identify possible solutions for continuous improvements.Basic programming skills in Python, R, Powershell and or Java with experience parsing, manipulating, and converting data to and from a wide range of formats (CSV, json, XML, html, SQL tables, etc.).Good understanding of modern database concepts and SQL syntax, including experience with DB2, MongoDB, SQL Server, CosmosDB, Data Lake, Hadoop, etc.Experience building and optimizing \u2018big data\u2019 data pipelines, architectures and data sets.Strong analytic skills related to working with unstructured datasets.Build processes supporting data transformation, data structures, metadata, dependency and workload management.QualificationsBachelor's Degree in Computer Science/Computer Engineering and/or equivalent experience required.2+ years related IT experience required.1+ years in Data Engineering required.Software development or related position experience preferred.1+ years SQL, relational database experience and unstructured datasets required.1+ years experience Azure Databricks, Azure Data Factory, Python, C# (preferred) Java, SDLC, Terraform, Spark, Config Management and Monitoring required.1+ years experience using Agile methodologies, data streaming, data as a service (REST APIs), CI/CD pipelines, Parquet, JSON preferred.1+ years experience working with Jupyter notebooks, microservices and data modeling preferred.1+ years IT infrastructure/operations role preferred.Certification in Azure Fundamentals & Data Engineer upon hire preferred.We don't just accept difference - we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Jackson is proud to be an equal opportunity workplace. The Company subscribes to and endorses federal and state laws and regulations relating to equal employment opportunity for all persons without regard to race, color, religion, gender, age, national origin, legally-recognized disability, marital status, legally-protected medical condition, citizenship, ancestry, height, weight, sexual orientation, veteran status, or any other factor not related to the needs of the job. The Company is committed to a policy of equal opportunity. Company facilities and campuses are tobacco-free environments.\n        ",
    "location": "Lansing, MI"
  },
  "97": {
    "id": "4174526332",
    "Company": "Niagara Bottling",
    "title": "Data Engineer I",
    "description": "At Niagara, we\u2019re looking for Team Members who want to be part of achieving our mission to provide our customers the highest quality most affordable bottled water.Consider Applying Here, If You Want ToWork in an entrepreneurial and dynamic environment with a chance to make an impact. Develop lasting relationships with great people. Have the opportunity to build a satisfying career.We offer competitive compensation and benefits packages for our Team Members.Data Engineer IAs an integral member of our team, the Data Engineer I is responsible for preparation of detailed specifications, technical design, development, documentation, testing, implementation and support of large-scale projects in the cloud, with a firm grasp of emerging technologies, platforms, and applications, and ability to customize them to help our business become more secure and efficientParticipate in business analysis activities to gather required reporting and dashboard requirements Translate business requirements into specifications that will be used to implement the required reports and dashboards, created from potentially multiple data sources Transition developed reports and dashboards to the Operations & Support team Provide support as required to ensure the availability and performance of developed reports and dashboards for both external and internal users Ensure proper configuration management and change controls are implemented Provide training, user documentation and assistance to end users as requiredProvide reporting, analysis and insightful recommendations to business leaders on KPIs pertaining to finance, Sales, Marketing, purchasing, manufacturing and warehouse operationsSupport existing reports and fine tuning for optimal performance Follow and implement Business Intelligence best practices and architectural standardsParticipate and handle full Software Development Life Cycle (SDLC)Please note that this job description is not designed to contain a comprehensive list of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without prior noticeAdditionally, The Data Engineer Is Expected To DemonstrateAccuracy, completeness and pay close attention to detailAbility to work independently and multi-task effectivelySolid problem solving and analytical capabilitiesFlexible and willing to accept a change in priorities as necessaryPromptly notifies his/her manager about any problems that affect his/her ability to accomplish planned goalsMust be a resourceful and innovative problem solverAbility to work in a professional manner, be flexible, and handle interactions with all levels of the organizationDemonstrate ability to communicate well with other members of the IT departmentSelf-directed, organized, and motivatedGood written and verbal communications skills are requiredExcellent team playerQualificationsMinimum Qualifications:1-2 Years \u2013 Experience in Data Engineering and Snowflake1-2 Years \u2013 Experience in Azure data factory, Databricks1-2 Years \u2013 Experience ETL/ELT1-2 Years \u2013 Experience building complex reportsexperience may include a combination of work experience and educationPreferred Qualifications:2-4 Years \u2013 Experience in Data Engineering and Snowflake2-4 Years \u2013 Experience in Azure data factory, Databricks2-4 Years \u2013 Experience ETL/ELT2-4 Years \u2013 Experience building complex reportsexperience may include a combination of work experience and educationEducationMinimum Required:Bachelor's Degree in Computer Science or Computer EngineeringPreferred:Master's Degree in Computer Science or Computer EngineeringTypical Compensation RangePay Rate Type: Salary$78,619.75 - $113,998.64 / YearlyBenefitshttps://careers.niagarawater.com/us/en/benefits *Los Angeles County applicants only** Qualified applicants with arrest or conviction records will be considered for employment in accordance with the Los Angeles County Fair Chance Ordinance for Employers, the California Fair Chance Act, and any other applicable local and state laws.Any employment agency, person or entity that submits a r\u00e9sum\u00e9 into this career site or to a hiring manager does so with the understanding that the applicant's r\u00e9sum\u00e9 will become the property of Niagara Bottling, LLC. Niagara Bottling, LLC will have the right to hire that applicant at its discretion without any fee owed to the submitting employment agency, person or entity.Employment agencies that have fee agreements with Niagara Bottling, LLC and have been engaged on a search shall submit r\u00e9sum\u00e9 to the designated Niagara Bottling, LLC recruiter or, upon authorization, submit r\u00e9sum\u00e9 into this career site to be eligible for placement fees.Niagara Plant NameCORP-MAIN\n        ",
    "location": "Diamond Bar, CA"
  },
  "98": {
    "id": "4184196940",
    "Company": "AMEND Consulting",
    "title": "Data Engineer",
    "description": "About AMEND: AMEND is a management consulting firm based in Cincinnati, OH with areas of focus in operations, analytics, and technology, focused on strengthening the people, processes, and systems in organizations to generate a holistic transformation. Our three-tiered approach provides a distinct competitive edge and allows us to build strong relationships and create customized solutions for every client. We work each day to change lives and transform businesses, and we are constantly striving to make a positive impact on our community! The AMEND team continues to grow at a rapid pace, and our technical team will continue to be an important part of that journey.Overview:The Data Engineer consultant role is an incredibly exciting position in the fastest growing segment of AMEND. You will be working to solve real-world problems by designing cutting edge analytic solutions while surrounded by a team of world class talent. You will be entering an environment of explosive growth with ample opportunity for development. We are looking for individuals who can go into a client and optimize (or re-design) companies data architecture, who are the combination of a change agent, technical leader and passionate about transforming companies for the better. We need someone who is a problem solver, a critical thinker, and is always wanting to go after new things; you\u2019ll never be doing the same thing twice!Job Tasks:Create and maintain optimal data pipeline architectureAssemble large, complex data sets that meet functional / non-functional business requirementsIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sourcesBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metricsWork with stakeholders including the Executive, Product, Data, and Design teams to assist with data-related technical issues and support their data infrastructure needsDefine project requirements by identifying project milestones, phases, and deliverablesExecute project plan, report progress, identify and resolve problems, and recommend further actionsDelegate tasks to appropriate resources as project requirements dictateDesign, develop, and deliver audience training and adoption methods and materialsQualifications:Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Databricks and DBT experience is a plusExperience building and optimizing data pipelines, architectures, and data setsExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvementStrong analytic skills related to working with structured and unstructured datasetsBuild processes supporting data transformation, data structures, metadata, dependency, and workload managementA successful history of manipulating, processing, and extracting value from large, disconnected datasetsAbility to interface with multiple other business functions (internally and externally)Desire to build analytical competencies in others within the businessCuriosity to ask questions and challenge the status quoCreativity to devise out-of-the-box solutionsAbility to travel as needed to meet client requirementsWhat\u2019s in it for you?Competitive pay and bonusContinued education and individual development plansUnlimited VacationFull Health, Vision, Dental, and Life BenefitsPaid parental leavePTO for your Birthday3:1 charity matchAll this to say \u2013 we are looking for talented people who are excited to make an impact on our clients. If this job description isn\u2019t a perfect match for your skillset, but you are talented, eager to learn, and passionate about our work, please apply! Our recruiting process is centered around you as an individual and finding the best place for you to thrive at AMEND, whether it be with the specific title on this posting or something different. One recruiting conversation with us has the potential to open you up to our entire network of opportunities, so why not give it a shot? We\u2019re looking forward to connecting with you.*Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of employment Visa at this time.*\n",
    "location": "Cincinnati, OH"
  },
  "99": {
    "id": "4154582788",
    "Company": "FanDuel",
    "title": "Data Engineer",
    "description": "About FanduelFanDuel Group is the premier mobile gaming company in the United States. FanDuel Group consists of a portfolio of leading brands across mobile wagering including, America\u2019s #1 Sportsbook FanDuel Sportsbook, its leading iGaming platform FanDuel Casino, the industry\u2019s unquestioned leader in horse racing and advance-deposit wagering, FanDuel Racing and its daily fantasy sports product.In addition, FanDuel Group operates FanDuel TV, its broadly distributed linear cable television network and FanDuel TV+, its leading direct-to-consumer OTT platform. FanDuel Group has a presence across all 50 states and Puerto Rico with approximately 17 million customers and 31 retail locations.The company is based in New York with offices in Los Angeles, Atlanta and Jersey City, as well as in Canada, Scotland, Ireland, Portugal, Romania and Australia.FanDuel Group is a subsidiary of Flutter Entertainment, the world's largest sports betting and gaming operator with a portfolio of globally recognized brands and traded on the New York Stock Exchange (NYSE: FLUT).THE ROSTERAt FanDuel, we give fans a new and innovative way to interact with their favorite games, sports and teams. We\u2019re dedicated to building a winning team and we pride ourselves on being able to make every moment mean more, especially when it comes to your career. So, what does \u201cwinning\u201d look like at FanDuel? It\u2019s recognition for your hard-earned results, a culture that brings out your best work\u2014and a roster full of talented coworkers. Make no mistake, we are here to win, but we believe in winning right. That means we\u2019ll never compromise when it comes to looking out for our teammates. From creatives professionals to cutting edge technology innovators, FanDuel offers a wide range of career opportunities, best in class benefits, and the tools to explore and grow into your best selves. At FanDuel, our principle of \u201cWe Are One Team\u201d runs through all our offices across the globe, and you can expect to be a part of an exciting company with many opportunities to grow and be successful.THE POSITIONOur roster has an opening with your name on itFanDuel is looking for an experienced Data Engineer with a deep understanding of large-scale data handling and processing best practices in a cloud environment to help us build scalable systems. As our data is a key component of the business used by almost every facet of the company, including product development, marketing, operations, and finance. It is vital that we deliver robust solutions that ensure reliable access to data with a focus on quality and availability.Our competitive edge comes from making decisions based on accurate and timely data and your work will provide access to that data across the whole company. Looking ahead to the next phase of our data platform we are keen to do more with real time data processing and working with our data scientists to create machine learning pipelinesWe're seeking dynamic professionals to join our business-critical Risk and Trading data team at FanDuel, where you'll harness cutting-edge technologies to support risk analysis, pricing models, and trading operations. Collaborate with a broad array of Risk, Trading and Fraud stakeholders and deploy crucial data assets to advance operational and machine learning use cases.THE GAME PLANEveryone on our team has a part to playCreating and maintain optimal data pipelinesImplementing data pipelines required in the data warehouse and data lake in batch or real-time using data transformation technologies.Identifying and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalabilityDeploying data models and views with large datasets that meet functional / non-functional business requirementsDelivering data integration solutions to downstream marketing and campaign softwareDelivering quality production-ready code in an agile environmentDelivering test plans, monitoring, debugging and technical documents as a part of development cycleCreating data tools for analytics and working with stakeholders across all departments to assist with data-related technical issues and supporting their data infrastructure needsAdhere to engineering and operational excellenceTHE STATSWhat we're looking for in our next teammate2-4+ years of experience writing Python, SQL and DBTProficiency in relational databases and data warehouses (Delta Lake knowledge is a plus)Show proficiency understanding complex ETL processesUnderstanding of Risk and/or Fraud terminologyKnowledge of data integrity and relational rulesUnderstanding of AWSAbility to quickly learn new technologies is criticalProficiency with agile or lean development practicesPlayer BenefitsWe treat our team rightFrom our many opportunities for professional development to our generous insurance and paid leave policies, we\u2019re committed to making sure our employees get as much out of FanDuel as we ask them to give. Competitive compensation is just the beginning. As part of our team, you can expect:An exciting and fun environment committed to driving real growthOpportunities to build really cool products that fans loveCareer and professional development resources to help you refine your game plan for owning and driving your career and developmentBe well, save well and live well - with FanDuel Total Rewards your benefits are one highlight reel after anotherFanDuel is an equal opportunities employer and we believe, as one of our principal states, \u201cWe Are One Team!\u201d We are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, creed, sex, national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, gender expression, Veteran status, or another other characteristic protected by state, local or federal law. We believe FanDuel is strongest and best able to compete if all employees feel valued, respected, and included. We want our team to include diverse individuals because diversity of thought, diversity of perspectives, and diversity of experiences leads to better performance. Having a diverse and inclusive workforce is a core value that we believe makes FanDuel stronger and more competitive as One Team!The applicable salary range for this position is $108,000 - $142,000, which is dependent on a variety of factors including relevant experience, location, business needs and market demand. This role may offer the following benefits: medical, vision, and dental insurance; life insurance; disability insurance; a 401(k) matching program; among other employee benefits. This role may also be eligible for short-term or long-term incentive compensation, including, but not limited to, cash bonuses and stock program participation. This role includes paid personal time off and 14 paid company holidays. FanDuel offers paid sick time in accordance with all applicable state and federal laws.\n",
    "location": "Jersey City, NJ"
  },
  "100": {
    "id": "4073150825",
    "Company": "Cond\u00e9 Nast",
    "title": "Data Engineer I",
    "description": "Cond\u00e9 Nast is a global media company producing the highest quality content with a footprint of more than 1 billion consumers in 32 territories through print, digital, video and social platforms. The company\u2019s portfolio includes many of the world\u2019s most respected and influential media properties including Vogue, Vanity Fair, Glamour, Self, GQ, The New Yorker, Cond\u00e9 Nast Traveler/Traveller, Allure, AD, Bon App\u00e9tit and Wired, among others.Job DescriptionLocation:New York, NYCond\u00e9 Nast is a premier media company renowned for producing the highest qualitycontent for the world's most influential audiences, attracting more than 100 million consumers across its industry-leading print, digital and video brands.Cond\u00e9 Nast is home to many of the world's most-celebrated magazine and website brands.The company's reputation for excellence is the result of our commitment to publishing the best consumer, trade, and lifestyle content. Our brands include Vogue, Epicurious, Vanity Fair, The New Yorker, Wired, and many more. Passion is the core of our philosophy at Cond\u00e9 Nast. Our mission is not only to inform readers but to ignite and nourish their passions.The Data Solutions Engineering team have a wide range of responsibilities and play a critical role in shaping how Cond\u00e9 Nast enables its business using data. The team is responsible for building data pipelines, data products and tools that enable our Data Scientists, Analysts in various business units, Business Intelligence Engineers and Executives to solve challenging use cases in our industry. We are seeking a Data Engineer who will build and maintain data pipelines across business areas such as subscriptions, video, clickstream, commerce, social and advertising within Cond\u00e9 Nast. If you are looking for a challenging environment and to work with a world class team of data engineers in a well balanced environment and seasoned company, come join us:RESPONSIBILITIES:Design, build and test batch and streaming data pipelines capable of processinglarge volumes of dataBuild efficient code to transform raw data into datasets for analysis, reporting and machine learning modelsCollaborate with other data engineers to implement a shared technical visionParticipate in the entire software development lifecycle, from concept to releaseMINIMUM QUALIFICATIONS:Applicants should have a degree (B.S. or higher) in Computer Science or a relateddiscipline or relevant professional experience2.5+ years of distributed data processing experience designing scalable &automated software systemsExperience in processing structured and unstructured data into a form suitable for analysis and reportingExperience with data modelling, batch data pipeline design and implementationExperience building batch or real-time data pipelinesProficiency in Python/PySpark or ScalaProficiency in SQLExperience with data processing frameworks such as Apache Spark (we useDatabricks)Experience in cloud-based infrastructures such as AWS or GCPExposure to orchestration platforms such as Airflow (we use Astronomer)Proven attention to detail, critical thinking, and the ability to work independentlywithin a cross-functional teamComfortable with CI/CD (we use GitHub Actions) PipelinesExperience with Git version control, and other software adjacent toolsTerraform is used Infra as service tool.Salay Range: $119K - $137KWhat happens next?If you are interested in this opportunity, please apply below, and we will review your application as soon as possible. You can update your resume or upload a cover letter at any time by accessing your candidate profile.Cond\u00e9 Nast is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, age, familial status and other legally protected characteristics.\n",
    "location": "New York City Metropolitan Area"
  },
  "101": {
    "id": "4172514773",
    "Company": "CVS Health",
    "title": "Data Engineer - EDW",
    "description": "Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand \u2014 with heart at its center \u2014 our purpose sends a personal message that how we deliver our services is just as important as what we deliver.Our Heart At Work Behaviors\u2122 support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.Position SummaryUnderstands the Enterprise data systems and acquires knowledge on the relevant processes need for project delivery.Participate in project estimation process and provide inputs to Tech Lead.Participate in Agile scrum activities/project status meetings on regular basis.Participate in User story grooming/Design discussion with technical lead.Analyzes complex Data structure from disparate data sources and design large scale data engineering pipeline.Uses strong programming skills to build robust data pipelines for ETL (Extract / Transform / Load) processes, designs database systems and develops tools for data processing.Perform all Data Engineering job activities EDW/ETL project development/testing and deployment activities.Work closely with the developers on the ETL Jobs/Pipelines development.Create the Project process/automation by integrating the involved components.Documents data engineering processes, workflows, and systems for reference and knowledge-sharing purposes.Implements data quality checks and validation processes to ensure the accuracy, completeness, and consistency of the data.Be a team player and work with team members for Business solution and implementation.Required Qualifications1+ years of Experience in executing Data warehousing ETL projects.1+ years of Experience with Python1+ years of Experience with SQL1+ years of hands-on Experience with bash shell scripts, UNIX utilities & UNIX Commands1+ years of hands-on Experience with a major cloud platform (GCP, AWS, Azure)Preferred QualificationsGCP Experience - BigQuery, Cloud SQL, Python, Cloud composer/Airflow , Cloud Storage & Dataflow/Data FusionHands-on experience building and deploying data transformation and processing solutions using Teradata utilities (BTEQ, TPT, FastLoad & SQL Queries).GCP - Data Engineer certification strongly preferred.Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.Strong problem-solving skills and critical thinking abilityStrong collaboration and communication skills within and across teamsKnowledge in Flask, JavaScript, HTML , CSS, DjangoKnowledge in BI Tools MicroStrategy, TableauMust understand software development methodologies including waterfall and agile.Health Care/PBM domain experienceExcellent communication and presentation skillsEducation Bachelor's degree preferred/specialized training/relevant professional qualification. Computer Science or Equivalent Years of Relevant ExperienceAnticipated Weekly Hours40Time TypeFull timePay RangeThe Typical Pay Range For This Role Is$72,100.00 - $144,200.00This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company\u2019s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (\u201cPTO\u201d) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.For more detailed information on available benefits, please visit Benefits | CVS HealthWe anticipate the application window for this opening will close on: 03/31/2025Qualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state and local laws.\n        ",
    "location": "Richardson, TX"
  },
  "102": {
    "id": "4090708276",
    "Company": "PMA Companies",
    "title": "Data Engineer",
    "description": "We are seeking a Data Engineer to join our IT department. This role will focus on leveraging Databricks, Azure Data Factory and related tool sets to enhance our systems integration capabilities within the commercial insurance domain. The ideal candidate will have strong technical problem-solving skills, a deep understanding of Databricks, and a strong understanding of data.Data Pipeline DevelopmentDesign, develop, and maintain scalable data pipelines using Databricks.Ensure data quality, integrity, and consistency across all data pipelines.Data IntegrationIntegrate data from multiple sources, including Azure Data Lake, SQL databases, and any APIs.Leveraging Databricks to integrate with different Large and Small Language Models.Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.ETL ProcessesDevelop and maintain ETL (Extract, Transform, Load) processes using Azure Databricks.Implement Data Quality and Audit, Balance and Control to the data workflows.Maintain and optimize data workflow performance.Collaboration And CommunicationWork closely with cross-functional teams to understand business requirements and translate them into technical solutions.Provide technical support and guidance to team members and stakeholders.Performance OptimizationOptimize data processing performance on Azure Databricks.Implement best practices for data storage, processing, and retrieval.Security and Compliance:Ensure data security and compliance with industry standards and regulations.Implement data governance policies and procedures.Documentation and Reporting:Document data engineering processes, workflows, and best practices.Generate reports and dashboards to provide insights into data pipeline performance and data quality.Continuous Improvement:Stay updated with the latest industry trends and technologies in data engineering and Azure Databricks.Identify opportunities for process improvements and implement best practicesDemonstrate commitment to Company\u2019s Code of Business Conduct and Ethics, and apply knowledge of compliance policies and procedures, standards and laws applicable to job responsibilities in the performance of work.RequirementsBachelor\u2019s degree in Computer Science, Engineering, or a related field.Proven experience as a Data Engineer with a focus on Azure Databricks.Strong knowledge of Azure Databricks and its ecosystem.Experience with data pipeline and workflow management tools.Proficiency in programming languages such as PySpark, Python, Scala, or SQL.Familiarity with big data technologies (e.g., Spark).Experience with Azure services such as Azure Data Lake, Azure SQL Database, and analytics capabilities available in Azure.Knowledge of data warehousing concepts and best practices.Excellent problem-solving skills and attention to detail.Strong communication and collaboration skills.Ability to work in a fast-paced, dynamic environment.\n",
    "location": "Blue Bell, PA"
  },
  "103": {
    "id": "4188263740",
    "Company": "Johnson & Johnson",
    "title": "Data Engineer",
    "description": "Johnson & Johnson Family of CompaniesTitle: Data EngineerLocation: Titusville, NJ - Hybrid schedule (3 days onsite)Duration: 1 yearPay Rate: $48.89Benefits on offer for this contract position: Health Insurance, Life insurance, 401K and Voluntary BenefitsPlease note that this is a contract role providing services to the Johnson & Johnson Family of Companies through external staffing partners of Kelly OCG. If you are selected for this role, you will be employed by a contract staffing supplier and will not be a member of the Johnson & Johnson Family of Companies.Summary:Data Engineer to support our Manufacturing Science and Technology team. This position will be integrated within the Process Science, Modeling, and Data group on a contract basis for an initial 12-month period. The role focuses on utilizing Data Engineering methodologies to enable the analytics and modeling needed to enhance various advanced therapy manufacturing processes. The successful candidate will collaborate with multidisciplinary teams, showcasing a strong analytical mindset and exceptional problem-solving abilities.This role offers a unique opportunity to work with forward-thinking professionals dedicated to making a transformative impact in the field of advanced therapies manufacturing. This Data Engineer position offers a chance to contribute to cutting-edge advancements in manufacturing science within a globally recognized organization. If you are passionate about data engineering and eager to make a significant impact, we encourage you to apply.Responsibilities:Design, implement, and optimize data pipelines and ETL processes to ensure efficient data flow across systems.Develop and maintain code development tools, infrastructure, and CI/CD data pipelines to facilitate automated software deployment in a manufacturing environment.Write and evaluate code for quality assurance, conducting thorough testing and performance analysis to enhance existing systems and applications.Collaborate with software development teams to create seamless integrations with internal and external back-end systems.Construct and maintain robust data architectures, including databases and real-time data processing systems.Identify and evaluate new data acquisition opportunities to enhance the manufacturing process and analytics capabilities.Design data processes for modeling, mining, and production, ensuring reliable data outputs for analytical needs.Propose and implement strategies to improve data reliability, efficiency, and quality across all data systems.Qualifications:Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.Demonstrated experience in DevOps and DataOps, with proven capabilities in developing tools and frameworks for automated data pipelines.Proficient in Python, with experience using testing and code analysis tools such as pytest and pylint for maintaining code quality.Hands-on experience with CI/CD practices, version control systems, and Agile methodologies, particularly with Atlassian tools like Confluence and Jira.Familiarity with database programming, both SQL and No-SQL based, to ensure efficient data handling.Experience building and managing cloud data systems and data lakes, particularly in environments like Microsoft Azure and Databricks.Strong verbal and written communication skills, with the ability to convey complex technical concepts to non-technical stakeholders.Proven ability to effectively collaborate within cross-functional teams, driving initiatives that support operational excellence.\n",
    "location": "Titusville, NJ"
  },
  "104": {
    "id": "4115019763",
    "Company": "Atlassian",
    "title": "Data Engineer",
    "description": "OverviewAtlassian is looking for a Data Engineer to join our Data Engineering team and build world-class data solutions and applications that power crucial decisions throughout the organisation. We are looking for an open-minded, structured thinker who is passionate about building systems at scale. You will enable a world-class data engineering practice, drive the approach with which we use data, develop backend systems and data models to serve the needs of insights and play an active role in building Atlassian\u2019s data-driven culture.Working at AtlassianAtlassians can choose where they work \u2013 whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.ResponsibilitiesData is a BIG deal at Atlassian. We ingest over 180 billion events each month into our analytics platform and we have dozens of teams across the company driving their decisions and guiding their operations based on the data and services we provide.The data engineering team manages several data models and data pipelines across Atlassian, including finance, growth, product analysis, customer support, sales, and marketing. You'll join a team that is smart and very direct. We ask hard questions and challenge each other to constantly improve our work.As a Data Engineer, you will apply your technical expertise to build analytical data models that support a broad range of analytical requirements across the company. You will work with extended teams to evolve solutions as business processes and requirements change. You'll own problems end-to-end and on an ongoing basis, you'll improve the data by adding new sources, coding business rules, and producing new metrics that support the business.QualificationsBachelor\u2019s/Master's degree or equivalent in a STEM field with a minimum 2+ Years of Experience in Data Engineering or related field.Expertise in Python or other modern programming languages.Working knowledge of relational databases and query authoring via SQL.Experience designing data models for optimal storage, retrieval and dashboarding to meet product and business requirements.Experience building scalable data pipelines using Spark or Spark-SQL with Airflow scheduler/executor framework or similar scheduling tools.Experience building real-time data pipelines using a micro-services architecture.Experience working with AWS data services or similar Apache projects (Spark, Flink, Hive, and Kafka).Understanding of Data Engineering tools/frameworks and standards to improve the productivity and quality of output for Data Engineers across the team.Well-versed in modern software development practices (Agile, TDD, CICD).CompensationAt Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:Zone A: $140,100 - $186,800Zone B: $126,100 - $168,200Zone C: $116,300 - $155,100This role may also be eligible for benefits, bonuses, commissions, and equity.Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.Our Perks & BenefitsAtlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit go.atlassian.com/perksandbenefits to learn more.About AtlassianAt Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.To learn more about our culture and hiring process, visit go.atlassian.com/crh .\n",
    "location": "New York, NY"
  },
  "105": {
    "id": "4179801877",
    "Company": "City of Boston",
    "title": "Junior Data Engineer",
    "description": "OverviewThis is a provisional appointment.Job Description Brief Job Description (essential functions of the job): The Department of Innovation and Technology is looking for a Junior Data Engineer to join our growing Citywide Analytics Team.As part of a highly collaborative group using data to improve all aspects of city government, the Junior Data Engineer will work on expanding our data infrastructure, developing ETL pipelines and open datasets, and connecting our analysts and data scientists to the data they need. Successful candidates will be able to work closely and collaboratively across the Analytics Team as well as with data owners within city departments. They will have familiarity with data engineering principles and processes and be able to work on a wide range of projects and with a variety of tools. Loving city government is preferred!The Citywide Analytics team is a diverse group of analysts, data scientists, project managers, developers, and engineers with a range of skills and backgrounds, working together to use data to improve life for those who live and work in Boston. The Junior Data Engineer will fulfill the responsibilities listed below and bring a collaborative spirit that will strengthen and expand the team\u2019s overall range of abilities.Responsibilities Process new datasets, including finding syntax or logical errors in the data and working with data owners to understand the specifics of the data.  Set up and update automated extract, transform, and load (ETL) workflows.  Automate existing processes and integrate new data sets.  Maintain and curate the documentation surrounding the team\u2019s data inventory - including adding new documentation, working with data owners to provide more thorough documentation, and updating the documents to reflect any changes to the dataset.  Provide support to more senior engineers working on automation or integration efforts.  Maintain a diagram of existing data pipelines and their dependencies, including datasets and reports. Work closely with engineers and analysts to maintain the most updated version of the diagram and include new work on it.  Work closely with data owners to resolve any potential errors or confusion identified by analysts working on the data.  Assist other team members in locating the most appropriate dataset to use for their project, including both spatial and tabular data.  Help users install tools such as ArcGIS and SQL database clients on their desktops and set new users up with correct access levels. Maintain the install files for the latest approved version.  Assist department personnel with the writing of queries for reports and analytics, making sure the business logic assumptions made in the query are aligned with the data.  Provide oral and written reports and presentations clearly, concisely, and effectively.  Perform related work as required. Minimum Entrance Qualifications At least two (2) years of full-time, or equivalent part-time, experience working with data analysis projects in a business, government, or nonprofit setting.  A bachelor's degree in economics, computer science, social science, engineering, mathematics, public policy, business, or a related field is preferred and may be substituted for up to two (2) years of the required experience.  Knowledge of Python or shell scripting.  Experience cataloging or curating knowledge bases or writing documentation is preferred.  Experience cleaning up and standardizing datasets from various source systems.  Knowledge of and experience using SQL.  Knowledge of Github or other code management repository tools is preferred but not required.  Some knowledge of GIS concepts and experience working with spatial data is preferred, but not required. Includes experience with ArcGIS product suite and/or knowledge of PostGIS.  Strong analytical, problem-solving, and communication skills, including mapping of specific business needs to appropriate solutions in a data-focused context.  Demonstrated success in learning new skills as needed to succeed in a fast-paced environment.  Talent for explaining complex subject matter to both technical and non-technical audiences.  Ability to exercise good judgment and focus on detail as required by the job. Boston Residency Required Terms: Union/Salary Plan/Grade: SENA/MM1-6Hours per week: 35\n        ",
    "location": "Boston, MA"
  },
  "106": {
    "id": "4177859066",
    "Company": "Western Governors University",
    "title": "Data Engineer",
    "description": "If you\u2019re passionate about building a better future for individuals, communities, and our country\u2014and you\u2019re committed to working hard to play your part in building that future\u2014consider Craft Education as the next step in your career.Craft Education is on a mission to solve skilled labor shortages by powering work-based learning pathways for all.At Craft, we\u2019re revolutionizing the apprenticeship degree, combining on-the-job learning with accredited instruction to create innovative educational pathways that accommodate working professionals and meet employer needs. Our flagship product - Craft Connect - helps organizations administer apprentice degree programs and address mission-critical data and reporting needs. Through the same platform, Craft is also transforming how on-the-job learning converts into academic credits.Our team of technology, education and workforce professionals also provides technical assistance to organizations looking to launch and manage apprenticeship degree programs. We are working tirelessly to accelerate the expansion of these programs along with the data infrastructure that underpins them.If you\u2019re looking to join the work-based learning revolution, we\u2019d love to talk with you. At Craft, you\u2019ll have the opportunity to solve hard problems in a high-growth startup environment and make a lasting impact on the future of education and workforce development. We couldn\u2019t be more excited to advance this work as a team of innovative, collaborative and mission-oriented professionals - we hope you\u2019ll consider joining us.The salary range for this position takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.At WGU, it is not typical for an individual to be hired at or near the top of the range for their position, and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range is:Pay Range: $87,200.00 - $130,800.00Job DescriptionJob SummaryThe principal function of the Data Engineer is to build robust ETL/ ELT pipelines to make data available to Data Analysts, Data Scientists, and other internal business units. The Data Engineer will be responsible for coding and working through the development cycle of ETL/ELT jobs and BI reporting. They will understand business requirements and design optimal ETL/ELT process for data acquisition, transformation, and publication for ease of analysis.The Data Engineer will follow agile development methodology for timely delivery of accurate data. They must have coding proficiency to write unit tests for pipeline functionality as well as data quality \u2013 this includes job monitoring, alerting, and code versioning and deployment. This position is also expected to develop, modify, and deploy formal and ad hoc reports.Essential Functions and Responsibilities: Develop and build ETL/ELT data pipelines for use in data analysis.Create and maintain optimal data pipeline architecture.Keep our data separated and secure across multiple cloud environments.Assemble large, complex data sets that meet functional / non-functional business requirements.Deliver ad hoc and analytical reports to internal users and teams.Monitor and maintain ETL/ELT jobs and troubleshoot load issues.Manage change requests/ticket queues for analytical reports and ETL/ELT jobs.Perform data/technology discovery from new sources and third-party applications for data ingestion.Create complex reports and dashboards Metabase.Ingest and transform structured, semi structured and unstructured data from sources including relational databases, NoSQL, external APIs, JSON, XML, delimited files, and more.Work and deliver in agile methodology for new development projects. Deliver efficient and effective solutions on time.Performs other related duties as assigned.Knowledge, Skill and Abilities:Ability to analyze and understand data source and design a data model for data capture and ETL/ ELT.Ability to identify bugs and apply fix and check data quality via process/pipeline audits.Ability to work with team members, as well as cross-team for product delivery.Ability to work in agile environment with timely delivery of ETL/ ELT pipelines and reports.Ability to deal with a variety of variables under limited standardization.Ability to interpret various instructions.Ability to perform basic math, use decimals to compute ratios & percentages, and to draw and interpret graphs.Ability to prepare memos, reports, and essays using proper punctuation, spelling, and grammar.Ability to communicate clearly, effectively and appropriately with all audiences.Ability to work independently, is self-motivated, and a strong team player.Use of industry best practices for code development, testing, implementation and documentationOrganizational or Student Impact:Works on a variety of technical projects of moderate scope with some instruction.Uses discretion to prioritize work and evaluate problem-solving approaches.Limit errors to prevent impact to client operations, costs, or schedules.Problem Solving & Decision Making:This position requires general supervision on all work.May help lead/coordinate small-medium scope projects.Guidance is required around project scopes and methodology.Work generally reviewed for accuracy.Communication & Influence:Communicates with contacts both within the department and function on matters that may require some explanation or interpretation.May work to influence parties within the department at an operational level regarding policies and best practices.Leadership & Talent Management:May provide guidance and assistance to more junior technical professionals.Job Qualifications:Minimum Qualifications:B. S. in Business, Management Information Systems, Computer Science, or a related field, or an equivalent combination of experience and training.2 years of related experienceKnowledge and Exposure to tools and concepts like: JiraConfluenceGitHubData IntegrityValidation and testingRelational SQL and NoSQL databasesObject-oriented/object function scripting languages: Python, Java, ScalaBig data tools: Hadoop, Spark, Kafka, Databricks, etc.Exposure to analytical reporting tools, preferably MetabasePreferred Qualifications:2 years of prior relevant work experience or advanced degrees in Data Integration, Big Data, or Business IntelligenceEarly-stage company exposurePhysical Requirements:Prolonged periods sitting at a desk and working on a computer.Must be able to lift up to 15 pounds at times.Position & Application DetailsFull-Time Regular Positions (classified as regular and working 40 standard weekly hours): This is a full-time, regular position (classified for 40 standard weekly hours) that is eligible for bonuses; medical, dental, vision, telehealth and mental healthcare; health savings account and flexible spending account; basic and voluntary life insurance; disability coverage; accident, critical illness and hospital indemnity supplemental coverages; legal and identity theft coverage; retirement savings plan; wellbeing program; discounted WGU tuition; and flexible paid time off for rest and relaxation with no need for accrual, flexible paid sick time with no need for accrual, 11 paid holidays, and other paid leaves, including up to 12 weeks of parental leave.How to Apply: If interested, an application will need to be submitted online. Internal WGU employees will need to apply through the internal job board in Workday.Additional InformationDisclaimer: The job posting highlights the most critical responsibilities and requirements of the job. It\u2019s not all-inclusive.Accommodations: Applicants with disabilities who require assistance or accommodation during the application or interview process should contact our Talent Acquisition team at recruiting@wgu.edu.Equal Employment Opportunity: All qualified applicants will receive consideration for employment without regard to any protected characteristic as required by law.\n        ",
    "location": "Salt Lake City Metropolitan Area"
  },
  "107": {
    "id": "3975498338",
    "Company": "Care Access",
    "title": "Data Engineer",
    "description": "What We DoCare Access is a unique, multi-specialty network of research sites which operates as one connected team of physician investigators, nurse coordinators, and operations managers. Our goal is to engage every healthcare professional in clinical research and to make clinical trials a care option for every patient. By removing this bottleneck, Care Access is helping accelerate the approval and delivery of critical and life-saving therapies.Who We AreWe care. Our people are the engines behind our mission: to revolutionize access to clinical trials for the benefit of patients everywhere. We care for one another, find new ideas to accelerate medicine, and seed a long-term impact for generations.Position OverviewOur technology team is searching for a data engineer to continue building and maintaining our data architecture. Your central responsibility as the data engineer will be to maintain and optimize our organizational data platform. Your duties may include understanding and framing data requirements, building platform and interfaces with various applications, supporting migration, troubleshooting data issues, and supporting governance and maintenance efforts. To succeed in this role, you should know how to examine new data system requirements, implement pipelines, and attend end-to-end enterprise data needs. The ideal candidate will also have proven experience constructing data architectures leveraging data lakes/marts, warehouses, databases.What You'll Be Working OnDuties include but not limited to:Assist in the design, configuration, and implementation of effective data platform solutions and pipelines to store and retrieve company data across diverse applications, particularly with Azure stack, integrating with AWS (Amazon Web Services) and Google BigQuery.Support database implementation procedures to ensure they comply with internal and external regulations (21 CFR Part 11).Help install and organize information systems to guarantee company functionality, producing detailed topology. Assist in the migration of data from legacy systems to innovative solutions.Monitor system performance by performing regular tests, troubleshooting, and integrating new features.Address requirements from stakeholders to ensure data systems meet organizational needs.Provide support and training to staff members. Respond to system problems in a timely manner.Physical And Travel RequirementsThis is a remote position with less than 10% travel requirements. Occasional planned travel may be required as part of the role.What You BringKnowledge, Skills, and Abilities:Training and Education: Undergraduate education in Information Systems, Computer Science, or Engineering; OR relevant certifications for database administration or big data systems.Experience: 2-4 years of experience in building, developing, deploying, and monitoring data systems to perform ETL/ELT processes.Technical Proficiency:\u25cb Strong knowledge of Azure data products and standard languages: Data Factory, Data Lake, Azure SQL Databases, Power BI, SQL, Python.\u25cb Working experience with Databricks and Spark.\u25cb Comfortable transforming data in standard formats: JSON, Parquet, HL7.\u25cb Working knowledge of API connections.Soft Skills: Excellent communication and technical writing skills.Certifications/Licenses, Education, And Experience2-4 years of experience working in the role of a data engineer or similar roles.Experience using software development lifecycle, cloud infrastructure, database query, ETL, and continuous deployment.Benefits (US Full-Time Employees Only)Paid Time Off (PTO) and Company Paid Holidays100% Employer paid medical, dental, and vision insurance plan optionsHealth Savings Account and Flexible Spending AccountsBi-weekly HSA employer contributionCompany paid Short-Term Disability and Long-Term Disability401(k) Retirement Plan, with Company MatchDiversity & InclusionWe serve patients and researchers from diverse cultures and communities around the world. We are stronger and better when we build a team representing the people we aim to support. We maintain an inclusive culture where people from a broad range of backgrounds feel valued and respected as they contribute to our mission. We value diversity and believe that unique contributions drive our success.At Care Access, every day, we are advancing medical breakthroughs. We\u2019re uniting standard patient care with cutting-edge treatments and research. Our work brings life-changing therapies to those in need and paves the way for newer and greater treatments to reach the world. We\u2019re proud to advance these breakthroughs and work with the big players while engaging with thephysicians and caring for patients.We are an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law.Care Access is unable to sponsor work visas at this time.Employment StatementCare Access complies with all employment laws and regulations with respect to its employment practices, terms and conditions of employment, and pay equity and wages. Care Access does not engage in any unfair or forced labor practice and does not tolerate, under any circumstances, the use of any form of forced or involuntary labor, child labor, or human trafficking. This extends to suppliers, partners, or other third parties with whom Care Access does business. Care Access values and promotes the protection of human rights everywhere.\n        ",
    "location": "United States"
  },
  "108": {
    "id": "4170191904",
    "Company": "Codeworks IT Careers ",
    "title": "Data Engineer",
    "description": "Codeworks is an IT Services firm headquartered in SE Wisconsin, known for our strong commitment to quality and for our direct client relationships.Who We Are Looking For:We are seeking a motivated mid-level Data Engineer for a 6-month contract, with potential for long-term engagement or full-time hire. The ideal candidate will be responsible for designing and developing data solutions, with a focus on SQL and SSIS. You should be self-motivated, have strong technical skills, and be able to work independently with moderate supervision.Responsibilities:Design and implement ETL solutions using SQL and SSIS.Develop and maintain data models and structures for relational/non-relational databases.Collaborate with cross-functional teams to validate requirements and design solutions.Troubleshoot and optimize data processes.Write and optimize SQL queries for data extraction and transformation.Document data solutions and processes in accordance with standards.Participate in iterative development cycles (Agile/Scrum).Qualifications:Bachelor\u2019s degree or 3+ years of relevant experience.Strong SQL skills with experience in SSIS and data integration.Experience with data modeling and database design.Familiarity with Visual Studio, SSDT, and scripting languages (C#, Python).Solid analytical, troubleshooting, and problem-solving abilities.Strong communication skills, both written and verbal.Experience working in an iterative delivery process.Knowledge of database administration tasks is a plus.About Codeworks: Codeworks has over 25 years of experience serving Fortune 1000 companies in Wisconsin as well as our client's national locations. Our recruiting team excels at evaluating, advising, and connecting IT professionals with new opportunities that will satisfy their expectations regarding income and opportunity for growth. At Codeworks, we're committed to diversity, equity, and inclusion in our workforce and beyond. We believe in equal opportunities and value the unique perspectives that every individual brings to our team. Join us in creating an inclusive, innovative, and collaborative workplace where your talents can thrive. Codeworks is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws.  Codeworks, LLC discloses that the anticipated hourly pay range for this position is between $55 and $75. This range is subject to change based on job-related factors, including client requirements where applicable.All full time Codeworks employees are eligible to enroll in the company\u2019s medical, dental, vision, and life insurance plans. Additionally, employees can participate in Codeworks\u2019 401(k) retirement plan.\n",
    "location": "Brookfield, WI"
  },
  "109": {
    "id": "4186568788",
    "Company": "Textron GSE",
    "title": "Data Engineer",
    "description": "Data Engineer(Job Number331236)DescriptionTextron Specialized Vehicles Inc. is a leading global manufacturer of golf cars, utility and personal transportation vehicles, snowmobiles, side-by-sides, all-terrain vehicles, professional turf-care equipment, and ground support equipment. Textron Specialized Vehicles markets products under several different brands. Its vehicles are found in environments ranging from golf courses to factories, airports to planned communities, and theme parks to hunting preserves.ResponsibilitiesAnalyze, manipulate, or process large sets of data using statistical software.Partner with functional and technology teams to develop and maintain mappings/rules between business applications & business data and interfaces.Identify business problems or management objectives that can be addressed through data analysis.Document data standards, semantic layer mapping, data flows and conceptual flows.Create graphs, charts, or other visualizations to convey the results of data analysis using specialized software.Perform statistical analysis to provide recommendations on best courses of action to business leaders.Design, implement, and maintain ELT pipelines that provide transformed data from source systems to analytics tools.Collaborate with data professionals in maintaining and building data models for the enterprise data lakehouse.Collaborate with business users to ensure data is available for self-service analytics.Adhere to all IT change control and security processes when implementing BI solutions and clearly communicate project status, risks, and issues to key stakeholders.QualificationsQualifications:Education: Bachelor\u2019s degree in computer science, engineering, mathematics, or similar field requiredYears of Experience: P1- 0+ years of experience required, previous internship experience preferred // P2-2 or more years of related experience requiredSoftware Knowledge: Proficiency in SQL; experience using Power BI or similar visualization tools; knowledge of dimensional modeling and star schema techniques preferredExperience with the following resources: Azure Data Factory, Databricks Delta Lake, Azure Log Analytics, Qlik Replicate (formerly Attunity), and/or Azure Synapse)EEO StatementTextron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.Recruiting Company: Textron Specialized VehiclesPrimary Location: US-Georgia-AugustaJob Field: Information TechnologySchedule: Full-timeJob Level: Individual ContributorShift: First ShiftJob Posting: 03/19/2025, 4:40:42 PMWorksite: Onsite\n        ",
    "location": "Augusta, GA"
  },
  "110": {
    "id": "4182391173",
    "Company": "Monogram Foods",
    "title": "Associate Data Engineer",
    "description": "Data EngineerJoin Our Team at Monogram Foods!Are you passionate about ensuring food safety and quality in a fast-paced, innovative environment? Do you thrive on leading transformative initiatives and inspiring teams to achieve excellence? If so, Monogram Foods invites you to embark on a rewarding career journey with us!At Monogram Foods, we\u2019re not just creating food products, we\u2019re crafting experiences that bring families together. Since 2004, we\u2019ve been at the forefront of food manufacturing, delivering exceptional products like meat snacks, appetizers, sandwiches, bacon, USDA baked goods, and more. With 12 state-of-the-art facilities across six states, we\u2019re proud to be a leader in co-manufacturing, private label and food service channels.When you join Monogram Foods, you join a family of over 4,000 talented individuals dedicated to innovation, teamwork, and excellence.Why Choose Monogram Foods?Innovation at the Core: Collaborate on cutting-edge food solutions that set industry benchmarks.Career Advancement: With our rapid growth and diverse product lines, you\u2019ll have endless opportunities to develop and excel.Culture of Collaboration: Be part of a supportive team that values your contributions and celebrates your successes.Competitive Benefits: We offer a comprehensive benefits package, including competitive pay, health coverage, retirement plans, and professional development programs.Your RoleAre you passionate about data and innovation? Monogram Foods is looking for an Associate Data Engineer which plays a crucial role in building and maintaining data processing pipelines and analytics solutions within the Monogram Foods IT team. This position supports the company's co-manufacturing, private label and food service channels across the United States. The Associate Engineer, Data will leverage Azure cloud tools to ingest, transform and model data, providing critical business insights. This role involves close collaboration with business analysts and stakeholders to understand and fulfill their data requirements.Essential Job Duties & ResponsibilitiesIntegrate data from diverse sources using Azure Data Factory.Transform data utilizing Azure data flows or compute services.Monitor and optimize ETL processes and data pipelines to ensure performance, reliability, and scalability.Collaborate with business analysts, stakeholders, and other teams to define and address data needs.Develop and execute SQL queries, stored procedures, and views to support data processing and reporting.Contribute to the migration of data and processes to Microsoft Fabric, maximizing its data management and analyticscapabilities.Utilize Python, PySpark and other programming languages for data processing and analytics within Azure services. Education & ExperienceThe Ideal Candidate will have the Following Qualifications:Bachelor's degree in computer science, information technology, mathematics, or a related field.0-2 years of experience in data engineering or analytics.Proficiency with Azure Data Factory, Azure SQL Analytics, and SQL Server is preferred.Experience with Microsoft Fabric is a plus.Demonstrated skills in programming languages such as SQL, Python, or Scala, with knowledge of parallel processingand data architecture patterns.Experience with Logic Apps, Azure Storage, and REST APIs is a plus.Must be 18 years or older. Competencies & SkillsStrong understanding of data engineering principles and practices.Proficiency in using Azure cloud services for data processing and analytics.Ability to write and optimize SQL queries and stored procedures.Experience with ETL processes and data pipeline development.Problem-solving and analytical skills.Effective communication and collaboration skills.Aptitude for learning new technologies.\n",
    "location": "Memphis, TN"
  },
  "111": {
    "id": "4175181313",
    "Company": "Intelliswift - An LTTS Company",
    "title": "Data Engineer (Gaming)",
    "description": "Job Title: Data Engineer - Gaming Location: Remote USA Duration: 9 Months (can be extended)Looking for 5+ years of experience in Data Engineering + GamingWork with remote data analyst teams, production, and game designers  Must-Have Skills Python and SQL Experience Able to work with a wide variety of data sourcesWrite queriesBuilding dashboardsETL and data modelingVisualization tools such as Tableau, Power BIPlus: Game Industry Domain experiencePlus: Data governanceDatabase Standards & Best Practices Use dashboarding templates Nice-to-have Skills: Live ops game industry expConsumer-facing product experienceCoordinate to provide dashboard data engineering support\n",
    "location": "United States"
  },
  "112": {
    "id": "4187127810",
    "Company": "UST",
    "title": "Data Engineer",
    "description": "Role DescriptionData EngineerLead II - Enterprise SolutionsWho We AreBorn digital, UST transforms lives through the power of technology. We walk alongside our clients and partners, embedding innovation and agility into everything they do. We help them create transformative experiences and human-centered solutions for a better world.UST is a mission-driven group of 29,000+ practical problem solvers and creative thinkers in more than 30 countries. Our entrepreneurial teams are empowered to innovate, act nimbly, and create a lasting and sustainable impact for our clients, their customers, and the communities in which we live.With us, you\u2019ll create a boundless impact that transforms your career\u2014and the lives of people across the world.Visit us at UST.com.You AreWe are looking for a talented Data Engineer with expertise in Data Warehouse Appliances, SQL Queries, Data Modeling, and Informatica to join our growing team. As a Data Engineer, you will be responsible for designing, building, and maintaining robust data pipelines, ensuring seamless integration of data from various sources, and managing data storage and processing frameworks. You will work with cutting-edge data warehouse technologies to create scalable and efficient data architectures that support business intelligence and analytics initiatives.The Opportunity Data Warehouse Design & Implementation: Design, implement, and optimize data warehouse solutions using Data Warehouse Appliances such as Teradata, Snowflake, Amazon Redshift, or similar technologies. Ensure efficient data storage, retrieval, and processing capabilities. ETL Development with Informatica: Build and manage ETL processes using Informatica to automate the extraction, transformation, and loading of data into the data warehouse. Ensure data is transformed in line with business needs. SQL Query Optimization: Write complex SQL queries to extract, manipulate, and analyze large volumes of data across relational and cloud-based data sources. Optimize queries for performance in large-scale environments. Data Modeling: Design and implement data models (conceptual, logical, and physical) to ensure data is structured efficiently for both storage and analytics. Collaborate with data architects and analysts to create flexible and scalable data models. Data Integration: Integrate and consolidate data from multiple sources (e.g., transactional systems, APIs, cloud storage, and flat files) into a centralized data warehouse for analysis and reporting. Data Pipeline Development: Develop, maintain, and optimize robust data pipelines that can handle large volumes of data and support real-time or batch processing needs. Performance Tuning & Optimization: Monitor and optimize the performance of data pipelines and ETL workflows, ensuring efficient data processing, low latency, and high scalability. Data Quality & Governance: Ensure the accuracy, consistency, and reliability of data within the data warehouse by implementing data validation and data quality checks. Collaboration & Communication: Work closely with data scientists, business analysts, and other technical teams to understand data requirements and deliver solutions that support data-driven decision-making. Documentation & Best Practices: Maintain comprehensive documentation for ETL workflows, data models, and data integration processes. Follow industry best practices and company standards for data architecture and engineering.This position description identifies the responsibilities and tasks typically associated with the performance of the position. Other relevant essential functions may be required.What You Need Bachelor\u2019s degree in Computer Science, Information Systems, Engineering, or a related field (or equivalent experience). Proven experience as a Data Engineer, Data Architect, or similar role in a data engineering capacity. Strong expertise in Data Warehouse Appliances such as Teradata, Snowflake, Redshift, or similar platforms. Extensive experience writing SQL queries to extract and analyze data from relational databases and cloud environments. In-depth experience with Informatica for developing ETL processes. Proficiency in Data Modeling, including designing and implementing logical, physical, and conceptual data models for data warehousing. Experience with cloud-based data platforms (e.g., AWS, Azure, GCP) is a plus. Strong knowledge of data pipeline development, batch processing, and real-time data integration. Excellent understanding of data governance and data quality practices. Experience with version control systems (e.g., Git) and collaborative development practices. Ability to optimize and troubleshoot complex SQL queries and ETL processes for performance. Strong problem-solving skills and the ability to work in a collaborative, cross-functional team environment. Good communication skills, both verbal and written, with the ability to present complex data engineering concepts to both technical and non-technical stakeholders. Preferred Skills: Familiarity with tools like Apache Kafka, Airflow, or other data orchestration frameworks. Knowledge of data analytics and reporting tools (e.g., Tableau, Power BI, or similar). Familiarity with big data technologies (e.g., Hadoop, Spark, Hive) is a plus. Experience with automation and scripting languages (e.g., Python, Shell scripting).Compensation can differ depending on factors including but not limited to the specific office location, role, skill set, education, and level of experience. UST provides a reasonable range of compensation for roles that may be hired in various U.S. markets as set forth below.Role Location: WashingtonCompensation Range: $87,000-$131,000BenefitsFull-time, regular employees accrue a minimum of 10 days of paid vacation per year, receive 6 days of paid sick leave each year (pro-rated for new hires throughout the year), 10 paid holidays, and are eligible for paid bereavement leave and jury duty. They are eligible to participate in the Company\u2019s 401(k) Retirement Plan with employer matching. They and their dependents residing in the US are eligible for medical, dental, and vision insurance, as well as the following Company-paid Employee Only benefits: basic life insurance, accidental death and disability insurance, and short- and long-term disability benefits. Regular employees may purchase additional voluntary short-term disability benefits, and participate in a Health Savings Account (HSA) as well as a Flexible Spending Account (FSA) for healthcare, dependent child care, and/or commuting expenses as allowable under IRS guidelines. Benefits offerings vary in Puerto Rico.Part-time employees receive 6 days of paid sick leave each year (pro-rated for new hires throughout the year) and are eligible to participate in the Company\u2019s 401(k) Retirement Plan with employer matching.Full-time temporary employees receive 6 days of paid sick leave each year (pro-rated for new hires throughout the year) and are eligible to participate in the Company\u2019s 401(k) program with employer matching. They and their dependents residing in the US are eligible for medical, dental, and vision insurance.Part-time temporary employees receive 6 days of paid sick leave each year (pro-rated for new hires throughout the year).All US employees who work in a state or locality with more generous paid sick leave benefits than specified here will receive the benefit of those sick leave laws.What We BelieveWe proudly embrace the values that have shaped UST since day one. We build our culture of Humility, Humanity, and Integrity. These values inspire us to nurture a people-first, human centric culture that fosters belonging, prioritizes sustainable solutions, and keeps our people and clients at the forefront of all decisions.HumilityWe will listen, learn, be empathetic and help selflessly in our interactions with everyone.HumanityThrough business, we will better the lives of those less fortunate than ourselves.IntegrityWe honor our commitments and act with responsibility in all our relationships.Equal Employment Opportunity StatementUST is an Equal Opportunity Employer.All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other applicable characteristics protected by law. We will consider qualified applicants with arrest or conviction records in accordance with state and local laws and \u201cfair chance\u201d ordinances.UST reserves the right to periodically redefine your roles and responsibilities based on the requirements of the organization and/or your performance.#USTSkillsData Warehouse Appliances,Sql Queries,Data Modeling,Informatica\n",
    "location": "Issaquah, WA"
  },
  "113": {
    "id": "4179989260",
    "Company": "Workday",
    "title": "Software Development Engineer- Data Engineer",
    "description": "Your work days are brighter here.At Workday, it all began with a conversation over breakfast. When our founders met at a sunny California diner, they came up with an idea to revolutionize the enterprise software market. And when we began to rise, one thing that really set us apart was our culture. A culture which was driven by our value of putting our people first. And ever since, the happiness, development, and contribution of every Workmate is central to who we are. Our Workmates believe a healthy employee-centric, collaborative culture is the essential mix of ingredients for success in business. That\u2019s why we look after our people, communities and the planet while still being profitable. Feel encouraged to shine, however that manifests: you don\u2019t need to hide who you are. You can feel the energy and the passion, it's what makes us unique. Inspired to make a brighter work day for all and transform with us to the next stage of our growth journey? Bring your brightest version of you and have a brighter work day here.At Workday, we value our candidates\u2019 privacy and data security. Workday will never ask candidates to apply to jobs through websites that are not Workday Careers.Please be aware of sites that may ask for you to input your data in connection with a job posting that appears to be from Workday but is not.In addition, Workday will never ask candidates to pay a recruiting fee, or pay for consulting or coaching services, in order to apply for a job at Workday.About The TeamWorkday is the cloud-based SaaS company providing industry-leading software for HR, financials, workforce planning and employee learning to more than 40% of the Fortune 500. We are the Machine Learning Product team at Workday. Our focus is on the application of machine learning and statistical analysis to Workday\u2019s products to serve our end users. We use diverse datasets to build data-driven products, which help the world\u2019s largest organizations uncover insights and make strategic decisions about their people, finances, and business. We routinely work on data with high velocity, volume and variety, and we employ a modern machine learning distributed computing and big data software stack to deal with these challenges.This is an opportunity to be part of a growth team focused on Data Engineering and ML Development Tools. We build ML capabilities into our products, and you would be building part of the next generation of Workday technology. We believe predictive products can be as ground-breaking to the next generation of technology as mobile was to the last.As a Software Engineer III you will help develop ML powered features and experiences for every user across our HR & Talent product portfolio. You will work closely with ML engineers and other software teams to deliver critically important infrastructure, frameworks, tooling, and platform, that enable machine learning across Workday\u2019s product ecosystem. You will apply modern ML Ops, Devops, and data engineering stacks to enable development, training, deployment, and lifecycle management of a variety of ML capabilities; supervised and unsupervised, deep learning and classical. You will be responsible for the design & development of new APIs/microservices deployed at scale.You will use Workday\u2019s vast computing resources on rich, exclusive datasets to deliver value that transforms the way our end-users experience WD. We will challenge you to apply your best creative thinking, analysis, problem-solving, and technical abilities to make an impact on thousands of enterprises and millions of people.About The RoleWork with multi-functional teams to deliver scalable, secure and reliable solutionsEffectively engage with data scientists, software engineers, ML engineers, PMs and architects in requirements elaboration and drive technical solutionsDevelop features from end to end including infrastructure as code. Design and build developer tools that enable ML capabilities.Build systems and dashboards to monitor service & ML health.Participate in architecture reviews, code reviews and technology evaluation.Help research, evaluate, prototype and drive adoption of new ML tools with reliability and scale in mindAbout YouBasic Qualifications(REQUIRED)5 or more years of software engineering experience with tools similar to Python, Java, Scala, AWS (CloudFormation, Glue, Lambda, S3, EMR, StepFunctions, etc.)Bachelor\u2019s and/or Master\u2019s degree, preferably in CS, or equivalent experience2+ years experience with tools like Cloudformation/Terraform or similar toolsSolid experience with designing & developing of new APIs/microservices deployed at scale1 or more years of experience working on initiatives related to data engineering or ML1 or more years of experience with production support on call rotations to resolve outages and other operational issuesOther QualificationsImplementation and operation of distributed systemsExperience with Data Engineering and/or ML systems with ability to think across layers of the stackExperience with tools like Databricks, Sagemaker, & Apache SparkWorkday Pay Transparency StatementThe annualized base salary ranges for the primary location and any additional locations are listed below. Workday pay ranges vary based on work location. As a part of the total compensation package, this role may be eligible for the Workday Bonus Plan or a role-specific commission/bonus, as well as annual refresh stock grants. Recruiters can share more detail during the hiring process. Each candidate\u2019s compensation offer will be based on multiple factors including, but not limited to, geography, experience, skills, job duties, and business need, among other things. For more information regarding Workday\u2019s comprehensive benefits, please click here.Primary Location: USA.OR.BeavertonPrimary Location Base Pay Range: $128,800 USD - $193,200 USDAdditional US Location(s) Base Pay Range: $122,400 USD - $217,200 USDIf performed in Colorado, the pay range for this job is $128,800 - $193,200 USD based on min and max pay range for that role if performed in CO.The application deadline for this role is the same as the posting end date stated as below:03/24/2025Our Approach to Flexible WorkWith Flex Work, we\u2019re combining the best of both worlds: in-person time and remote. Our approach enables our teams to deepen connections, maintain a strong community, and do their best work. We know that flexibility can take shape in many ways, so rather than a number of required days in-office each week, we simply spend at least half (50%) of our time each quarter in the office or in the field with our customers, prospects, and partners (depending on role). This means you'll have the freedom to create a flexible schedule that caters to your business, team, and personal needs, while being intentional to make the most of time spent together. Those in our remote \"home office\" roles also have the opportunity to come together in our offices for important moments that matter.Pursuant to applicable Fair Chance law, Workday will consider for employment qualified applicants with arrest and conviction records.Workday is an Equal Opportunity Employer including individuals with disabilities and protected veterans.Are you being referred to one of our roles? If so, ask your connection at Workday about our Employee Referral process!,\n",
    "location": "Atlanta, GA"
  },
  "114": {
    "id": "4190015615",
    "Company": "Royal Caribbean Group",
    "title": "Data Analytics Engineer",
    "description": "Journey with us! Combine your career goals and sense of adventure by joining our incredible team of employees at Royal Caribbean Group. We are proud to offer a competitive compensation and benefits package, and excellent career development opportunities, each offering unique ways to explore the world.We are proud to be the vacation-industry leader with global brands \u2014 including Royal Caribbean International, Celebrity Cruises and Silversea Cruises \u2014 the most innovative fleet and private destinations, and the best people. Together, we are dedicated to turning the vacation of a lifetime into a lifetime of vacations for our guests.Royal Caribbean Group\u2019s Revenue Planning & Analysis has an exciting career opportunity for a full time Analyst  Engineer ,Data reporting to the Sr. Mgr, RM Dev & SystemsThis position will work on-site in Miami, Florida. Position SummaryThe Revenue Management Development and Systems team is responsible for all data, decision tools, reporting and information needs for the Revenue Planning and Revenue Management teams. This position assists in supporting the team\u2019s database, jobs, and reports. Serves as contact person for the department to help explain the tools and data results. Responsibilities include the design, development, and implementation of data processes, reporting, and analytics solutions. This individual can work under minimal supervision. They are responsible for interacting with various departments within Royal Caribbean to find, consolidate, and manipulate data from multiple large data sets; to analyze and understand results; and to create reports.Essential Duties And Responsibilities Meet with business stakeholders to clarify and document reporting requirements  Works with management & other team members to understand & clarify data analysis and reporting needs  Meet with technical stakeholders to perform code reviews and elicit feedback  Assist in developing, implementing, and maintaining business intelligence reporting and user tools  Recommend data architecture and engineering structures necessary to support reports  Develop new reporting by creating new queries or coding new processes. Aggregates large data sets in SQL, Python, and other analytical tools for analysis. Develops data strategies, specifically around data structures, identifying critical information, as well as the tools used to retrieve and analyze the data. Performs research and analysis on large data sets - data exploration, trending, etc.  Continuously search for potential business improvements, revenue opportunity and efficiency gains.  Clearly explain tools and results to teams and management. Develop and conduct clear, concise presentations, communicating technical methods and impacts in business terms. Performs other duties as required. This job description in no way states or implies that these are the only duties to be performed by the employee occupying this position. Employees will be required to perform any other job-related duties assigned by their supervisor or management.Qualifications, Knowledge And Skills Bachelor's degree preferably in Mathematics, Statistics, Computer Science, Economics, Analytics, Business, Engineering or BS/BA with combination thereof and related analytic work experience or relevant certifications.  2 years of work experience in data analysis, data mining, business case development or other related analytical projects.  Equivalent combination of education and experience will be considered  Strong proficiency in query/reporting tools, SQL, Python or other development tools.  Understanding of forecasting, data cleansing and transformations  Experience working with databases, including Oracle, Databricks, or SQL Server Strong quantitative, analytical, and problem-solving skills. Strong quantitative, analytical, and problem-solving skills.  Demonstrated ability to develop and implement applications using programming languages  Experience with SQL required  Experience in supporting and interpreting reporting using business Intelligence tools (Azure, Synapse, and Databricks experience a plus).  Ability to handle dynamic workload, balancing short- and long-term projects.  Ability to communicate clearly and effectively to business and technical audiences in both spoken and written formats. We know there's a lot to consider. As you go through the application process, our recruiters will be glad to provide guidance, and more relevant details to answer any additional questions. Thank you again for your interest in Royal Caribbean Group. We'll hope to see you onboard soon!It is the policy of the Company to ensure equal employment and promotion opportunity to qualified candidates without discrimination or harassment on the basis of race, color, religion, sex, age, national origin, disability, sexual orientation, sexuality, gender identity or expression, marital status, or any other characteristic protected by law. Royal Caribbean Group and each of its subsidiaries prohibit and will not tolerate discrimination or harassment.\n",
    "location": "Miami, FL"
  },
  "115": {
    "id": "4133415618",
    "Company": "Republic National Distributing Company",
    "title": "College Intern, Data Engineer",
    "description": "Republic National Distributing Company (RNDC) is a family-owned business with roots extending before Prohibition that has evolved into one of the nation's largest wine and spirits wholesalers. Our success is grounded in our core values of Family, Service, Accountability, Honesty, and Professionalism. We offer a vibrant, inclusive culture and workplace experience for individuals who want a career that makes them feel accomplished and engaged. RNDC values the health and well-being of our associates, inside and outside the office, offering dynamic health and wellness benefits that supply exceptional care and value. RNDC is geared toward growing our footprint and our people. Join our team of energetic professionals who believe in many happy hours and are experts in our craft.**Summary**RNDC's IT College Intern program provides students with hands-on experience in a dynamic and innovative technology environment. Interns will work alongside IT professionals, contributing to projects such as system support, data analysis, software development, cybersecurity, and troubleshooting technical issues. This role is ideal for students pursuing a degree in Information Technology, Computer Science, or a related field who are eager to enhance their technical skills, gain industry insight, and explore career opportunities within a leading beverage distributor. Candidates should have strong problem-solving abilities, familiarity with IT tools and systems, and a willingness to learn and adapt in a fast-paced setting.**In this role, you will****Data Engineer Intern** Design and optimize data pipelines and Snowflake queries for efficient data handling. Build and model large data sets to meet business needs and ensure data reliability. Create data tools, tag data for organizational visibility, and improve data quality. Adhere to privacy policies and Agile development processes while supporting analytics teams.**What you bring to RNDC**Data Engineer College Intern Enrolled in Computer Science or related field, with data engineering knowledge. Familiar with SQL, Python/Java, Snowflake, Apache Airflow, and big data tools like Kafka. Experience with Agile, CI/CD, and GitHub.**What's in it for you** 401(k) with company matching Medical, dental and vision benefits\\* Paid Time Off Program \u2013 work your way up to 5 weeks of PTO a year with the ability to carryover unused PTO Paid volunteer time Paid parental leave Paid caregivers leave Fertility benefits Paid training Company paid life insurance, short-term disability, and company-paid holidays Associate resource groups, and diversity, equity, and inclusion programs available for all associates\\*Participation in these programs is subject to applicable wait periods and all plan and program terms and eligibilityCOVID-19 ConsiderationsWe follow CDC Guidelines and have a fun and safe environment for our teams.**Bonus if you bring**Republic National Distributing Company and National Distributing Company are Equal Opportunity/Affirmative Action employers. It is our policy not to discriminate against any Employee or Applicant. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, age, status as a protected veteran, among other things, or status as a qualified individual with disability. This policy of nondiscrimination in employment includes but is not limited to: recruitment, hiring, placement, promotion, transfer, employment advertising or solicitations, compensation, layoff or termination of employment.RNDC is committed to providing reasonable accommodation to people with disabilities throughout the job application and interview process, to the point of undue hardship. If you require an accommodation during the application or interview process, please click here.\n        ",
    "location": "Atlanta, GA"
  },
  "116": {
    "id": "4141900202",
    "Company": "Steer Health",
    "title": "AI Data Engineer",
    "description": "About Steer HealthAt Steer Health, we\u2019re revolutionizing healthcare with AI-driven solutions that empower providers to deliver exceptional patient care. Our mission is to transform healthcare workflows through intelligent automation, predictive analytics, and seamless integrations. By leveraging cutting-edge technologies like LLMs, FHIR APIs, and agentic AI, we\u2019re building the future of healthcare efficiency and patient engagement.\u00a0\u00a0About the RoleSteer Health is seeking a talented **Backend Engineer** with expertise in AI/ML and healthcare technologies to design and implement **AgenticAI workflows** that redefine clinical and operational processes. You\u2019ll build scalable backend systems that integrate FHIR-compliant APIs, LLM-driven automation, and conversational AI to solve real-world healthcare challenges. If you\u2019re passionate about Python, AI workflows, and making a tangible impact in healthcare, this role is for you.\u00a0\u00a0---Key Responsibilities-FastAPI to enable seamless data exchange across EHRs, patient portals, and AI agents.\u00a0\u00a0- Architect AI-driven workflows using tools like RAGFlow or similar platforms to automate tasks such as clinical documentation, prior authorization, and patient triage.\u00a0\u00a0- Develop and fine-tune LLM-based solutions (e.g., GPT, Claude) with PyTorch, focusing on healthcare-specific use cases like diagnosis support or patient communication.\u00a0\u00a0- Integrate Dialogflow for conversational AI agents that power chatbots, voice assistants, and virtual health aides.\u00a0\u00a0- Collaborate on prompt engineering to optimize LLM outputs for accuracy, compliance, and clinical relevance.\u00a0\u00a0- Optimize backend systems for performance, scalability, and security in HIPAA-compliant environments.\u00a0\u00a0- Partner with cross-functional teams (data scientists, product managers, clinicians) to translate healthcare needs into technical solutions.\u00a0\u00a0---Qualifications- 3+ years of backend engineering experience, with expertise in Python and frameworks like FastAPI or Flask.\u00a0\u00a0- Hands-on experience with **PyTorch/TensorFlow** and deploying ML models in production.\u00a0\u00a0- Familiarity with AI workflow tools (e.g., RAGFlow, Airflow, Kubeflow) and orchestration of LLM pipelines.\u00a0\u00a0- Experience integrating Dialogflow or similar platforms for conversational AI.\u00a0\u00a0- Strong understanding of LLMs (training, fine-tuning, and deployment) and prompt engineering best practices.\u00a0\u00a0- Knowledge of cloud platforms (AWS/GCP/Azure) and containerization (Docker, Kubernetes).\u00a0\u00a0- Passion for healthcare innovation and improving patient/provider experiences.\u00a0\u00a0---Preferred Qualifications- Experience in healthcare tech (EHR integrations, HIPAA compliance, HL7/FHIR).\u00a0\u00a0- Contributions to open-source AI/healthcare projects.\u00a0\u00a0- Familiarity with **LangChain**, **LlamaIndex**, or agentic workflow frameworks.\u00a0\u00a0---Why Join Steer Health?- Impact: Your work will directly enhance healthcare delivery for millions of patients.\u00a0\u00a0- Innovation: Build with the latest AI/ML tools in a fast-paced, forward-thinking environment.\u00a0\u00a0- Growth: Lead projects at the intersection of AI and healthcare, with opportunities for advancement.\u00a0\u00a0- Culture: Collaborative, mission-driven team with flexible work policies.\u00a0\u00a0\n",
    "location": "Dallas, TX"
  },
  "117": {
    "id": "4118016758",
    "Company": "Tential Solutions",
    "title": "Data Engineer",
    "description": "We are Tential, a highly ranked and extremely successful IT solutions and staffing with offices in Tampa, FL, Annapolis, MD, and Charlotte, NC. Tential is your trusted technology and transformation partner. We lead with capabilities to deliver right-sized solutions across data, applications, AI, customer engagement and talent. With our unwavering commitment, radical candor and deep understanding of culture, we partner with financial services, accounting, and healthcare leaders across the US to modernize technology and help them stay competitive. Tential \u2013 empowering change that wins.Job Requirements Extensive Experience with cloud based Big Data technologies like Hadoop, Hive, Spark, AWS EMR and Airflow. Experience in one or more programming languages like Java, Scala, and Python. Experience in solving complex problems using SQL. Experience with developing enterprise grade solutions in an iterative or Agile environment. Clear, effective communication with strong interpersonal skills. Ability to push the frontier of technology and independently pursue better alternatives. Ability to maintain focus and develop proficiency in new skills rapidly. Ability to utilize problem solving skills in a fast-paced environment Experience in AWS Cloud is requiredEqual Opportunity Statement \u2013 Please know that Tential does not and will not discriminate in employment opportunities or practices on the base of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factors.\n",
    "location": "Rockville, MD"
  },
  "118": {
    "id": "4126255312",
    "Company": "Skanska",
    "title": "Azure Data Engineer",
    "description": "Skanska is searching for a dynamic Azure Data Engineer. This is a great opportunity to start a career with a company that builds things that matter and values its team. We are proud to share our culture of diversity and inclusion. This is a hybrid role, requiring a minimum of two days in-person at the Charlotte, North Carolina or Durham, NC office. Our work makes a clear contribution to society and the environment around us. We build in many different verticals. Whether we are building schools to provide inspiring spaces for learning, roads to connect communities, or hospitals to care for patients, it all contributes to our purpose - we build for a better society.Skanska's values \u2014Be Better Together, Act Ethically and Transparently, Commit to Customer, and Care for Life\u2014are deeply engrained in how we work, which is why our values support and drive our D&I efforts.The Data Engineer will help build, optimize, and maintain data pipelines that support our data analytics and business intelligence initiatives. Data Engineers design, develop, and maintain scalable ETL/ELT pipelines using Azure Data Factory (ADF), SSIS, and other Azure services (Azure Data Lake, Azure SQL Database, etc.). They optimize SQL queries and ensure effective data storage, performance and retrieval strategies. Data Engineers are also responsible for automating data integration processes, including data cleansing, transformation, and loading across systems.Data Engineer Required Qualifications:3+ years of hands-on experience in data engineering or related roles with a focus on Azure Cloud technologies.1+ years experience with data modeling, schema design, and best practices for relational, NoSQL, and data warehouse technologies in Azure.Advanced level of experience with SQL queries from scratch with good understanding of RDBMS concepts to optimize queries as per requirements.1+ years of experience in Microsoft Azure, including key components like Azure Data Lake, Azure Data Factory, Azure Blob Storage, Azure Synapse and Azure SQLDemonstrated experience designing, developing, migrating, and maintaining scalable data pipelines and ETL/ELT processes in SSIS and Azure Data Factory, and utilizing Azure data and analytics resources.Bachelor\u2019s Degree - Computer Science, Data Engineering, or a related field; or equivalent experience and minimum 7 years prior relevant experience.Currently, the company is not considering applicants for this position who now or in the future require employment sponsorship by the company.Our Investment in you:We believe that Benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That\u2019s why we provide an array of options (including medical, dental, and vision insurance plans), expert guidance, and always-on tools, that are personalized to meet the needs of your reality - to help support you physically, financially, and emotionally through the big milestones and in your everyday life. Please visit the compensation and Benefits summary on our careers site for more details.As a Skanska community, our values ground us, and our diversity of experience propels us forward. No matter what your career stage, there\u2019s a place for you to thrive here and partner with us in shaping how our world lives, moves, and connects.At Skanska, we Care for Life. And we\u2019re committed to supporting your whole health and peace of mind through inclusive and personalized total rewards.We\u2019re committed to your success by developing you in your role and supporting your career growthCompensation and financial well-being - Competitive base salary, excellent bonus program, 401k, & Employee ownership program. Come work with us and join a winning team!Background Check RequiredSkanska is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion or creed, sex, sexual orientation, gender identity, national origin or citizenship status, disability, status as a protected veteran, or any other protected characteristics under federal, state, or local law.Skanska Equal Employment OpportunitySkanska uses knowledge & foresight to shape the way people live, work, and connect. More than 135 years in the making, we\u2019re one of the world\u2019s largest development and construction companies. We operate in select markets throughout the Nordics, Europe and the United States. Skanska in the U.S. is headquartered in New York City with 29 offices around the country. In 2022, construction in the U.S. generated $6.9 billion in revenue, and as a developer in the U.S., Skanska has invested a total of $3.5 billion in commercial and multi-family projects. Together with our customers and the collective expertise of our 6,500+ teammates in the U.S. and 27,000+ globally, we create innovative and sustainable solutions that support healthy living beyond our lifetime.Skanska's Applicant Privacy Policy for California ResidentsSearch Firm and Employment Agency Disclaimer Search Firm and Employment Agency Disclaimer Skanska USA Human Resources (\u201cSkanska HR\u201d) provides HR services to the Skanska business units within the U.S.A. including Skanska USA Civil Inc., Skanska USA Building Inc., Skanska USA Commercial Development Inc. and Skanska Infrastructure Development Inc. (collectively \u201cSkanska USA\u201d). As such, Skanska HR is the sole authorized representative of Skanska USA to execute any agreements with search firms, employment agencies or any employment vendor (\u201cVendor\u201d). As a condition precedent to any entitlement for payment, a Vendor shall have both (1) Skanska USA Placement Agreement, and (2) an Engagement Job Order executed by an authorized Skanska HR representative. Absent the properly executed documents, Skanska HR shall have no obligation to make payment to the Vendor. Verbal or written communications from any employee of Skanska USA business units shall not be considered binding obligations. All resumes whether unsolicited or solicited shall be considered property of Skanska HR.\n",
    "location": "Charlotte, NC"
  },
  "119": {
    "id": "4131389287",
    "Company": "Wolfspeed",
    "title": "Data Engineer",
    "description": "At Wolfspeed, we do amazing things in a human way.We know that the achievements of our organization are due to the passion, hard work and creativity of our employees. We celebrate different perspectives to foster excellence across our organization, and our goal is to make diversity a foundation of what we do. We are proudly building an environment where you can bring your authentic self to work.Enjoy doing things that people say can\u2019t be done? Innovation is at the center of everything we do.Hate red tape? We remove roadblocks instead of creating them.Working parent? We provide childcare assistance and paid parental leave.Student? We offer continuing education assistance.Looking for community? There are many ways to get involved, from Employee Resource Groups to local outreach.Here\u2019s the Gist: You will work to support the Mohawk Valley Fab data analysis needs, particularly relating to Yield , Integration and Process engineering. In this role, you will support the data needs of various Defectivity reduction teams, automate data analysis for yield and build infrastructure to automate data reports. You will work with IT teams to ensure data validity and help automate data collation activities.The Day-to-Day Establish standardized reporting of defect/yield metrics with easy access across the organization. Engage with IT to provide visualization tools to convert data in to actionable information.Working with a cross functional team to develop centralized data repository and application for servicing data to engineering teams in sites within USA and across the globe. Research, evaluate, and prototype different big data architectures for our back-end data needs. Apply machine learning and its concepts to solve cycle time, productivity, wafer and die yield problems. Expand functionality of in-house built yield management system. Provide training to other Wolfspeed personnel where appropriate to improve over-all capabilities and knowledge to use in-house developed Yield Engineering tools.This Job is Right for You if You Have (Minimum Requirements):Bachelors degree in a STEM field; with good working knowledge of data systems3+ years of back-end software engineering experience in Java/C++/C# and Oracle/SQL Server.Familiarity with Git version control.Excellent written, verbal, interpersonal skillsAble to adjust to changing priorities, schedules, business needs.Strong background in data validation and testingThis role may require additional duties and/or assignments as designated by management.To put it legally \u2013Wolfspeed is an equal opportunity employer. We recruit, employ, train, compensate and promote regardless of race, sex, religion, color, national origin, disability, age, veteran status, gender identity, sexual orientation and other protected status as required by law.The posted salary range is what Wolfspeed reasonably expects to pay for this position. Actual pay for a hired applicant will be determined based on the individual\u2019s job-relevant qualifications, experience, and other determinative factors. In the event it is determined that a different job level upon hire is warranted then that range will be communicated to the hired applicant as soon as reasonably practicable.Compensation Range:$75,000.00 - $103,000.00We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, national origin, disability status, protected veteran status, or any other characteristic protected by law.\n",
    "location": "Marcy, NY"
  },
  "120": {
    "id": "3945491630",
    "Company": "Ovative Group",
    "title": "Data and Software Engineer",
    "description": "About Ovative Group:Ovative Group is the premier independent media and measurement firm in the United States. We help trailblazers in fast-growing, customer-centric organizations across industries reinvent their marketing and measurement programs. We leverage our media, measurement, and consulting capabilities to help brands like Domino's, Facebook, The Home Depot, CVS, Disney, and UnitedHealth Group transform their media and marketing programs. Our proprietary approach to measuring and optimizing marketing investment decisions,\u202fEnterprise Marketing Return (EMR),\u202fis disrupting the industry and setting the gold standard for customer and marketing strategy, activation, and measurement.\u202fRecognized\u202feight\u202fconsecutive years on Star Tribune\u2019s list of Top 150 Workplaces and\u202ffive\u202fyears on Inc. 5000\u2019s list of the fastest-growing private companies in America, we pride ourselves in always overdelivering for our clients, our teams, and our communities.\u202f\u202f\u202fAbout the Role:We are seeking a Data & Software Engineer to join our rapidly growing product and engineering development team. Our company specializes in enterprise-level media measurement and optimization across various industries. In this role, you will be an integral part of a cross-functional team responsible for creating, optimizing, and maintaining scalable software and data solutions to enhance performance, stability, and scalability. You will work under the guidance of experienced team members and closely collaborate with stakeholders throughout the entire software development lifecycle, from concept to deployment.The ideal candidate will have a strong foundation in iterative development practices, familiarity with version control systems like GitHub, and a passion for developing both conceptual and pragmatic problem-solving skills. Your attention to detail and communication skills, both written and oral, will enable you to work directly with a variety of users, understand their objectives, and contribute to translating them into technical requirements and solutions. As a valuable team member, you will be encouraged to learn and grow in a supportive environment, actively participate in team activities, and set the foundation for your professional development.Responsibilities:Assist in designing, developing, testing, and deploying software solutions that align with business and technical requirements.Contribute to the effort of identifying opportunities for automation with a focus on the operational stability of software applications and systemsCollaborate with your team to support translating business goals and user requirements into detailed, actionable technical requirementsContribute to documentation and technical requirements, including software designs, evaluation plans, test results, technical manuals and formal recommendations and reportsContribute to the creation and maintenance of robust, performant solutions that provide high customer impact at scaleProvide technical assistance to teammates as needed through collaboration and by sharing your expertiseFollow established software and product engineering best practices, including code quality, documentation, deployment, and testing.Explore new technologies and tools under guidance to enhance software development.Troubleshoot tier 1 software issues with support from senior team members as needed.Participate in technical knowledge sharing with other team members.Stay current with emerging technology and software engineering innovations and continuously enhance skills.Requirements:3+ years of relevant data & software engineering development experienceProficient working with ETL/ELT tooling for large data sets (e.g., Airbyte)Proficient utilizing SQL, Python, and command lineFamiliarity with cloud-based platforms (i.e. GCP, AWS)Preferred:Experience working with APIs for data retrievalExperience working with data warehouses and big data tools (e.g., BigQuery, Databricks)Experience creating data/table architectureExperience implementing QA processes and QA automationExperience integrating data models within softwareExperience working with marketing, analytics and customer dataPay TransparencyAt Ovative, we offer a transparent view into three core components of your total compensation package: Base Salary, Annual Bonus, and Benefits. The salary range for this position below is inclusive of an annual bonus.\u202fActual offers are made with consideration for relevant experience and anticipated impact. Additional benefits information is provided below.For our Sr. Analyst positions, our compensation ranges from $62,000 to $93,000, which is inclusive of a 15% bonus.Benefits Of Working At Ovative GroupWe provide strong, competitive, holistic benefits that understand the importance of your life inside and out of work.Culture:Culture matters and we\u2019ve been recognized as a Top Workplace for eight years running because of it. We demand trust and transparency from each other. We believe in doing the hard and complicated work others put off. We\u2019re open in communication and floor plan. We\u2019re flat \u2013 our interns sit next to VPs, our analysts work closely with senior leaders, and our CEO interacts with every single person daily. Put together, these elements help foster an environment where smart people can support each other in performing to their highest potential.Compensation and Insurance:We strive to hire and retain the best talent. Paying fair, competitive compensation, with a large bonus incentive, and phenomenal health insurance is an important part of this mix.We\u2019re rewarded fairly and when the company performs well, we all benefit.Tangible amenities we enjoy:Access to all office spaces in MSP, NYC, and CHI Frequent, paid travel to our Minneapolis headquarters for company events, team events, and in-person collaboration with teams. Flexible paid vacation policy 401k match program Top-notch health insurance options Monthly stipend for your mobile phone and data plan Sabbatical program Charitable giving via our time and a financial match program Shenanigan\u2019s Day Working at Ovative won\u2019t be easy, but if you like getting your hands dirty, driving results, and being surrounded by the best talent, it\u2019ll be the most rewarding job you\u2019ll ever have. If you think you can make us better, we want to hear from you!\n        ",
    "location": "Chicago, IL"
  },
  "121": {
    "id": "4151100142",
    "Company": "Baker Tilly US",
    "title": "Associate Data Engineer",
    "description": "OverviewBaker Tilly is a leading advisory, tax and assurance firm, providing clients with a genuine coast-to-coast and global advantage in major regions of the U.S. and in many of the world\u2019s leading financial centers \u2013 New York, London, San Francisco, Los Angeles, Chicago and Boston. Baker Tilly Advisory Group, LP and Baker Tilly US, LLP (Baker Tilly) provide professional services through an alternative practice structure in accordance with the AICPA Code of Professional Conduct and applicable laws, regulations and professional standards. Baker Tilly US, LLP is a licensed independent CPA firm that provides attest services to its clients. Baker Tilly Advisory Group, LP and its subsidiary entities provide tax and business advisory services to their clients. Baker Tilly Advisory Group, LP and its subsidiary entities are not licensed CPA firms.Baker Tilly Advisory Group, LP and Baker Tilly US, LLP, trading as Baker Tilly, are independent members of Baker Tilly International, a worldwide network of independent accounting and business advisory firms in 141 territories, with 43,000 professionals and a combined worldwide revenue of $5.2 billion. Visit bakertilly.com or join the conversation on LinkedIn, Facebook and Instagram.Please discuss the work location status with your Baker Tilly talent acquisition professional to understand the requirements for an opportunity you are exploring.Baker Tilly is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status, gender identity, sexual orientation, or any other legally protected basis, in accordance with applicable federal, state or local law.Any unsolicited resumes submitted through our website or to Baker Tilly Advisory Group, LP, employee e-mail accounts are considered property of Baker Tilly Advisory Group, LP, and are not subject to payment of agency fees. In order to be an authorized recruitment agency (\"search firm\") for Baker Tilly Advisory Group, LP, there must be a formal written agreement in place and the agency must be invited, by Baker Tilly's Talent Attraction team, to submit candidates for review via our applicant tracking system.ResponsibilitiesJob Description:Baker Tilly has an incredible career opportunity for an Associate Data Engineer to join our growing Enterprise Technology team.The Associate Data Engineer role will be focused specifically on Data Warehousing, Data Management, BI and Data Analytics. All supporting the need to define the businesses strategy and bring light and understanding to the vast amounts of data that Baker Tilly utilizes.Assemble large, complex data sets that meet functional / non-functional business requirements.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure technologiesIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etcUtilize development best practices including technical design reviews, implementing test plans, monitoring/alerting, peer code reviews, and documentationWork with our developers to implement data solutions in SQL Server (both on-premises and Azure instances)Work with the team to develop our Databricks data lakehouse environment for firmwide usage within applications, data integrations, and reporting solutions.QualificationsStrong understanding of data modeling, algorithms, and data transformation techniques.Well versed in BI and data analytics, SQL, Python, R, the MS Stack, Azure and other cloud servicesDatabricks Lakehouse Platform and Boomi MDH skills are preferred Experience developing, deploying and supporting high-quality, fault-tolerant data pipelinesHave hands on experience in Microsoft business intelligence technologies.Exhibit responsibility and accountability toward quality completion of projects and consistently hitting project timelines.Outstanding customer service skills following proper business requirements and human resources expectationsDisciplined to be able to work in a variety of business environments.Maintained a Bachelor\u2019s degree in Computer Science, Engineering, Math, Information Technology, or other related discipline or 4 + years of commensurate experience.\n",
    "location": "Greater Tampa Bay Area"
  },
  "122": {
    "id": "4185039901",
    "Company": "Spring & Bond",
    "title": "Associate Data Engineer",
    "description": "About us: Spring & Bond is a digital media agency and consultancy specializing in helping pharmaceutical and medical device manufacturers create robust, omnichannel media strategies for both healthcare professionals (HCPs) and consumer audiences. We emphasize transparency and client empowerment through comprehensive services, including customer journey planning, media strategy and activation, technology evaluation, in-house capability development, and training.What you\u2019ll do:We are looking for an Associate Data Engineer to help build and maintain our data infrastructure, ensuring data quality and accessibility for our analytics and media operations teams. Success in this position will involve creating efficient ETL pipelines, validating data accuracy, and collaborating with stakeholders to understand their data needs. This role is crucial in supporting data-driven decision-making across the organization and reports to the Data Engineering Manager.Your Responsibilities:Design, develop, and maintain ETL pipelines to ingest and transform data from various sources. Implement data validation processes to ensure data accuracy and consistency throughout the data lifecycle, using tools like Regex. Write and optimize SQL queries for data extraction, aggregation, and analysis. Develop and maintain Python scripts and Pandas dataframes for data manipulation and analysis. Utilize AWS Lambda functions to automate data processing tasks. Manage code using Git for version control and collaborative development. Collaborate with data analysts, media strategists, and other stakeholders to understand their data requirements and provide solutions. Communicate technical concepts to non-technical stakeholders and translate business needs into technical specifications. Troubleshoot and resolve data-related issues, identifying areas for improvement and efficiency gains. Document data processes, pipelines, and transformations for knowledge sharing and maintainability. Work with vendors to ensure seamless data integration and resolve any data delivery issues. Apply critical thinking skills to analyze complex data problems and develop innovative solutions. Your experience:Bachelor's degree in Computer Science, Data Science, or a related field preferred, but not required. Equivalent experience will be considered. 1+ years of experience in a data engineering or related role. Strong SQL skills, including the ability to write complex queries using JOINs and aggregate functions. Proficiency in Python and data manipulation libraries such as Pandas. Experience with data validation techniques and tools. Familiarity with AWS cloud services, particularly S3 and Lambda. Experience with Git for code version control. Detail-oriented with a focus on data accuracy and quality. Organized with a systematic approach to managing data workflows. Comfortable working in an ambiguous environment and able to independently drive projects forward. Nimble and able to adapt to changing priorities. Benefits:Remote-first team environmentCoverage for medical, dental, and vision insurance for you and your dependentsDisability insurance plan Matching 401K Parental leave Other fun health & wellness perksSpring & Bond is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.Spring & Bond is a woman-owned business.Not everyone will match the above qualifications 100%. If your experiences don\u2019t perfectly align, but you think you\u2019d be a great addition to our team, we\u2019d still love to hear from you.Compensation Range: $80K - $100K\n",
    "location": "New York, NY"
  },
  "123": {
    "id": "4177854770",
    "Company": "Western Governors University",
    "title": "Data Engineer",
    "description": "If you\u2019re passionate about building a better future for individuals, communities, and our country\u2014and you\u2019re committed to working hard to play your part in building that future\u2014consider Craft Education as the next step in your career.Craft Education is on a mission to solve skilled labor shortages by powering work-based learning pathways for all.At Craft, we\u2019re revolutionizing the apprenticeship degree, combining on-the-job learning with accredited instruction to create innovative educational pathways that accommodate working professionals and meet employer needs. Our flagship product - Craft Connect - helps organizations administer apprentice degree programs and address mission-critical data and reporting needs. Through the same platform, Craft is also transforming how on-the-job learning converts into academic credits.Our team of technology, education and workforce professionals also provides technical assistance to organizations looking to launch and manage apprenticeship degree programs. We are working tirelessly to accelerate the expansion of these programs along with the data infrastructure that underpins them.If you\u2019re looking to join the work-based learning revolution, we\u2019d love to talk with you. At Craft, you\u2019ll have the opportunity to solve hard problems in a high-growth startup environment and make a lasting impact on the future of education and workforce development. We couldn\u2019t be more excited to advance this work as a team of innovative, collaborative and mission-oriented professionals - we hope you\u2019ll consider joining us.The salary range for this position takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.At WGU, it is not typical for an individual to be hired at or near the top of the range for their position, and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range is:Pay Range: $87,200.00 - $130,800.00Job DescriptionJob SummaryThe principal function of the Data Engineer is to build robust ETL/ ELT pipelines to make data available to Data Analysts, Data Scientists, and other internal business units. The Data Engineer will be responsible for coding and working through the development cycle of ETL/ELT jobs and BI reporting. They will understand business requirements and design optimal ETL/ELT process for data acquisition, transformation, and publication for ease of analysis.The Data Engineer will follow agile development methodology for timely delivery of accurate data. They must have coding proficiency to write unit tests for pipeline functionality as well as data quality \u2013 this includes job monitoring, alerting, and code versioning and deployment. This position is also expected to develop, modify, and deploy formal and ad hoc reports.Essential Functions and Responsibilities: Develop and build ETL/ELT data pipelines for use in data analysis.Create and maintain optimal data pipeline architecture.Keep our data separated and secure across multiple cloud environments.Assemble large, complex data sets that meet functional / non-functional business requirements.Deliver ad hoc and analytical reports to internal users and teams.Monitor and maintain ETL/ELT jobs and troubleshoot load issues.Manage change requests/ticket queues for analytical reports and ETL/ELT jobs.Perform data/technology discovery from new sources and third-party applications for data ingestion.Create complex reports and dashboards Metabase.Ingest and transform structured, semi structured and unstructured data from sources including relational databases, NoSQL, external APIs, JSON, XML, delimited files, and more.Work and deliver in agile methodology for new development projects. Deliver efficient and effective solutions on time.Performs other related duties as assigned.Knowledge, Skill and Abilities:Ability to analyze and understand data source and design a data model for data capture and ETL/ ELT.Ability to identify bugs and apply fix and check data quality via process/pipeline audits.Ability to work with team members, as well as cross-team for product delivery.Ability to work in agile environment with timely delivery of ETL/ ELT pipelines and reports.Ability to deal with a variety of variables under limited standardization.Ability to interpret various instructions.Ability to perform basic math, use decimals to compute ratios & percentages, and to draw and interpret graphs.Ability to prepare memos, reports, and essays using proper punctuation, spelling, and grammar.Ability to communicate clearly, effectively and appropriately with all audiences.Ability to work independently, is self-motivated, and a strong team player.Use of industry best practices for code development, testing, implementation and documentationOrganizational or Student Impact:Works on a variety of technical projects of moderate scope with some instruction.Uses discretion to prioritize work and evaluate problem-solving approaches.Limit errors to prevent impact to client operations, costs, or schedules.Problem Solving & Decision Making:This position requires general supervision on all work.May help lead/coordinate small-medium scope projects.Guidance is required around project scopes and methodology.Work generally reviewed for accuracy.Communication & Influence:Communicates with contacts both within the department and function on matters that may require some explanation or interpretation.May work to influence parties within the department at an operational level regarding policies and best practices.Leadership & Talent Management:May provide guidance and assistance to more junior technical professionals.Job Qualifications:Minimum Qualifications:B. S. in Business, Management Information Systems, Computer Science, or a related field, or an equivalent combination of experience and training.2 years of related experienceKnowledge and Exposure to tools and concepts like: JiraConfluenceGitHubData IntegrityValidation and testingRelational SQL and NoSQL databasesObject-oriented/object function scripting languages: Python, Java, ScalaBig data tools: Hadoop, Spark, Kafka, Databricks, etc.Exposure to analytical reporting tools, preferably MetabasePreferred Qualifications:2 years of prior relevant work experience or advanced degrees in Data Integration, Big Data, or Business IntelligenceEarly-stage company exposurePhysical Requirements:Prolonged periods sitting at a desk and working on a computer.Must be able to lift up to 15 pounds at times.Position & Application DetailsFull-Time Regular Positions (classified as regular and working 40 standard weekly hours): This is a full-time, regular position (classified for 40 standard weekly hours) that is eligible for bonuses; medical, dental, vision, telehealth and mental healthcare; health savings account and flexible spending account; basic and voluntary life insurance; disability coverage; accident, critical illness and hospital indemnity supplemental coverages; legal and identity theft coverage; retirement savings plan; wellbeing program; discounted WGU tuition; and flexible paid time off for rest and relaxation with no need for accrual, flexible paid sick time with no need for accrual, 11 paid holidays, and other paid leaves, including up to 12 weeks of parental leave.How to Apply: If interested, an application will need to be submitted online. Internal WGU employees will need to apply through the internal job board in Workday.Additional InformationDisclaimer: The job posting highlights the most critical responsibilities and requirements of the job. It\u2019s not all-inclusive.Accommodations: Applicants with disabilities who require assistance or accommodation during the application or interview process should contact our Talent Acquisition team at recruiting@wgu.edu.Equal Employment Opportunity: All qualified applicants will receive consideration for employment without regard to any protected characteristic as required by law.\n        ",
    "location": "Salt Lake City Metropolitan Area"
  },
  "124": {
    "id": "4142560487",
    "Company": "Modis",
    "title": "Big Data Engineer",
    "description": "Akkodis is seeking a Big Data Engineer for a Contract job with a client in Plano, TX(Hybrid). Responsibilities include developing processes to interact with HDFS and Oracle, creating workflows using Autosys, and ensuring ML/AI models are production-ready while working in an agile environment.Rate Range: $60/hour to $70/hour; The rate may be negotiable based on experience, education, geographic location, and other factors.Big Data Engineer Job Responsibilities IncludeDevelop and enhance application components for supporting ML/AI models and data ingestion processes with a focus on code resiliency and stability.Build processes to interact with HDFS and Oracle using Python/PySpark and Oracle PL/SQL.Create, schedule, and manage workflows and jobs using Autosys.Collaborate with development teams to contribute to story refinement and deliver data requirements throughout the delivery life cycle.Leverage architecture components to integrate, clean, transform, and control data as per acceptance criteria.Develop and execute test plans to identify issues, errors, and underlying causes, ensuring data accuracy and reliability.Drive complex information technology projects, ensuring on-time delivery and adherence to team delivery and release processes.Identify, define, and document data engineering requirements for deployment, maintenance, and business functionality.Work independently with strong analytical skills, while also being an excellent team player in a fast-paced agile environment.Contribute to data resiliency capabilities and ensure that ML/AI models are ready for production.Desired QualificationsBS/master\u2019s in computer science, Engineering, or a related field.10+ years of relevant experience in Big Data engineering and development.Expertise in Oracle PL/SQL, Hadoop ecosystem, Hive Tables, and Python/PySpark.Experience in creating workflows and scheduling jobs using Autosys, with a strong background in machine learning and data resiliency.If you are interested in this role, then please click APPLY NOW. For other opportunities available at Akkodis, or any questions, feel free to contact me at mudit.rajpal@akkodisgroup.com.Pay Details: $60.00 to $70.00 per hourBenefit offerings available for our associates include medical, dental, vision, life insurance, short-term disability, additional voluntary benefits, EAP program, commuter benefits and a 401K plan. Our benefit offerings provide employees the flexibility to choose the type of coverage that meets their individual needs. In addition, our associates may be eligible for paid leave including Paid Sick Leave or any other paid leave required by Federal, State, or local law, as well as Holiday pay where applicable.Equal Opportunity Employer/Veterans/DisabledTo read our Candidate Privacy Information Statement, which explains how we will use your information, please navigate to https://www.akkodis.com/en/us/candidate-privacy-policyRequirementsThe Company will consider qualified applicants with arrest and conviction records in accordance with federal, state, and local laws and/or security clearance requirements, including, as applicable:The California Fair Chance ActLos Angeles City Fair Chance OrdinanceLos Angeles County Fair Chance Ordinance for EmployersSan Francisco Fair Chance Ordinance\n",
    "location": "Plano, TX"
  },
  "125": {
    "id": "4190002631",
    "Company": "Capital One",
    "title": "Data Engineer",
    "description": "Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you\u2019ll have the opportunity to be on the forefront of driving a major transformation within Capital One.What You\u2019ll Do: Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies  Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems  Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake  Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community  Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment  Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance Basic Qualifications: Bachelor\u2019s Degree  At least 1.5 years of experience in application development (Internship experience does not apply)  At least 1 year of experience in big data technologies Preferred Qualifications:  3+ years of experience in application development including Python, SQL, Scala, or Java  1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)  2+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)  1+ years experience working on real-time data and streaming applications  1+ years of experience with NoSQL implementation (Mongo, Cassandra)  1+ years of data warehousing experience (Redshift or Snowflake)  2+ years of experience with UNIX/Linux including basic commands and shell scripting  1+ years of experience with Agile engineering practices At this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, or another type of work authorization).The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.McLean, VA: $133,000 - $151,800 for Data EngineerCandidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter.This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.comCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).\n        ",
    "location": "McLean, VA"
  },
  "126": {
    "id": "4168614554",
    "Company": "Entergy",
    "title": "AI Data Engineer",
    "description": "Work Place Flexibility:  Hybrid Legal Entity:  Entergy Services, LLC This is a hybrid position located in New Orleans, LA. Relocation assistance and sponsorship is not provided. ***Job Summary/PurposeThe Data Engineer II is an integral member of the product delivery teams, executing technical designs for analytics projects to deliver analytic products to business customers. The Data Engineer II will implement and test the overall architectural solution by ingesting, cleaning, and organizing data into information used by the analytics product. The Data Engineer II\u2019s key knowledge set includes system agile development life cycle, integration architectures, technology tools, database architectures, data management principles, data modelling techniques and methodologies, communication and problem solving.Job Duties/Responsibilities Analyze data sources, design and evaluate feasible data pipeline solutions.  Support and adhere to the development of data management policies, standards and procedures.  Define integrated views of data drawing together data from across the enterprise, both in real-time and as extracts.  Package deployments and collaborate with IT to migrate code between Development, Quality Assurance (QA), and Production environments.  Support test automation, troubleshooting ETL job functionality, validating data, as well as create test data and table structures through use of SQL.  Design and develop data pipelines or ETL processes.  Must be able to work in an agile, rapid delivery environment. Minimum RequirementsMinimum education required of the positionBachelor's degreeMinimum Experience Required Of The Position  2+ years of experience: ETL experience  Technologies such as Informatica, SQL programming and hands-on experience in Python, R, APIs, Spark, Python or Kafka  Big data technologies  Data visualization tools Minimum Knowledge, Skills And Abilities Required Of The Position Proficient in developing and utilizing tools that prepare, extract and manipulate data to create connected datasets (data warehousing) that can be utilized for analytics purposes.  As a minimum this must include strong SQL skills and proficiency of an ETL tool (Informatica, SSIS, Talend, etc).  Experience discussing complex and new topics in a simple, easy-to-understand way.  Demonstrated teamwork skills.  Strong, demonstrated writing and presentation skills Primary Location: Louisiana-New Orleans Louisiana : New OrleansJob Function: Information TechnologyFLSA Status: ProfessionalRelocation Option: No Relocation OfferedUnion description/code: NON BARGAINING UNITNumber of Openings: 1Req ID: 117967Travel Percentage:Up to 25%An Equal Opportunity Employer, Minority/Female/Disability/Vets. Please click here to view the EEI page, or see statements below.EEO Statement: The Entergy System of Companies provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a protected veteran in accordance with applicable federal, state and local laws. The Entergy System of Companies complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment including, but not limited to, recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.The Entergy System of Companies expressly prohibits any form of unlawful employee harassment based on race, color, religion, sex, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of the Entergy System of Company employees to perform their expected job duties is absolutely not tolerated.Accessibility: Entergy provides reasonable accommodations for online applicants. Requests for a reasonable accommodation may be made orally or in writing by an applicant, employee, or third party on his or her behalf. If you are an individual with a disability and you are in need of an accommodation for the recruiting process please click  here  and provide your name, contact number, the accommodation requested and the requisition number that you are requesting the accommodation for. Employee Services will contact you regarding your request.Additional Responsibilities: As a provider of essential services, Entergy expects its employees to be available to work additional hours, to work in alternate locations, and/or to perform additional duties in connection with storms, outages, emergencies, or other situations as deemed necessary by the company. Exempt employees may not be paid overtime associated with such duties. Equal Opportunity The non-confidential portions of the affirmative action program for individuals with disabilities and protected veterans shall be available for inspection upon request by any employee or applicant for employment. Please contact HRCompliance@entergy.com to schedule a time to review the affirmative action plan during regular office hours.Working ConditionsAs a provider of essential services, Entergy expects its employees to be available to work additional hours, to work in alternate locations, and/or to perform additional duties in connection with storms, outages, emergencies, or other situations as deemed necessary by the company. Exempt employees may not be paid overtime associated with such duties.Please note: Authorization to work in the United States is a precondition to employment in this position. Entergy will not sponsor candidates for work visas for this position.\n",
    "location": "New Orleans, LA"
  },
  "127": {
    "id": "4115023391",
    "Company": "Atlassian",
    "title": "Data Engineer",
    "description": "OverviewAtlassian is looking for a Data Engineer to join our Data Engineering team and build world-class data solutions and applications that power crucial decisions throughout the organisation. We are looking for an open-minded, structured thinker who is passionate about building systems at scale. You will enable a world-class data engineering practice, drive the approach with which we use data, develop backend systems and data models to serve the needs of insights and play an active role in building Atlassian\u2019s data-driven culture.Working at AtlassianAtlassians can choose where they work \u2013 whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.ResponsibilitiesData is a BIG deal at Atlassian. We ingest over 180 billion events each month into our analytics platform and we have dozens of teams across the company driving their decisions and guiding their operations based on the data and services we provide.The data engineering team manages several data models and data pipelines across Atlassian, including finance, growth, product analysis, customer support, sales, and marketing. You'll join a team that is smart and very direct. We ask hard questions and challenge each other to constantly improve our work.As a Data Engineer, you will apply your technical expertise to build analytical data models that support a broad range of analytical requirements across the company. You will work with extended teams to evolve solutions as business processes and requirements change. You'll own problems end-to-end and on an ongoing basis, you'll improve the data by adding new sources, coding business rules, and producing new metrics that support the business.QualificationsBachelor\u2019s/Master's degree or equivalent in a STEM field with a minimum 2+ Years of Experience in Data Engineering or related field.Expertise in Python or other modern programming languages.Working knowledge of relational databases and query authoring via SQL.Experience designing data models for optimal storage, retrieval and dashboarding to meet product and business requirements.Experience building scalable data pipelines using Spark or Spark-SQL with Airflow scheduler/executor framework or similar scheduling tools.Experience building real-time data pipelines using a micro-services architecture.Experience working with AWS data services or similar Apache projects (Spark, Flink, Hive, and Kafka).Understanding of Data Engineering tools/frameworks and standards to improve the productivity and quality of output for Data Engineers across the team.Well-versed in modern software development practices (Agile, TDD, CICD).CompensationAt Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:Zone A: $140,100 - $186,800Zone B: $126,100 - $168,200Zone C: $116,300 - $155,100This role may also be eligible for benefits, bonuses, commissions, and equity.Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.Our Perks & BenefitsAtlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit go.atlassian.com/perksandbenefits to learn more.About AtlassianAt Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.To learn more about our culture and hiring process, visit go.atlassian.com/crh .\n",
    "location": "Mountain View, CA"
  },
  "128": {
    "id": "4157966208",
    "Company": "SOLV Energy",
    "title": "Data Engineer (Bend, OR)",
    "description": "SOLV Energy is an engineering, procurement, construction (EPC) and solar services provider for utility solar, high voltage substation and energy storage markets across North America. Job Description Summary:As a Data Engineer, you will play a crucial role in supporting our data engineering team. Your primary responsibility will be to help design, develop, and maintain data pipelines and ETL (Extract, Transform, Load) processes. You will work to ensure efficient data flow, data quality, and system reliability.This position is required to be onsite at the Bend, OR office.Job Description: This job description reflects management's assignment of essential functions; it does not prescribe or restrict the tasks that may be assignedPosition Responsibilities and Duties:Data Pipeline DevelopmentCollaborate with the senior data engineer to design and implement data pipelines that extract, transform, and load data from various sources into our data warehouse.Optimize data pipelines for performance, scalability, and reliability.Monitor and troubleshoot pipeline issues, ensuring data consistency and accuracy.Document ETL/ELT processes and data lineage.Database ManagementAssist in data model design and schema creation.Perform routine database maintenance tasks such as backups, indexing, and query optimization.Data Quality AssuranceValidate data quality by implementing data validation checks and monitoring data anomalies.Administer change management policies and procedures and conduct tabletop exercises to assess ETL process effectiveness.Work closely with data analysts and business stakeholders to understand data requirements and ensure accurate data delivery.Collaboration and CommunicationCommunicate effectively with cross-functional teams, including data scientists, analysts, and software engineers.Assist in documenting technical specifications, data catalogs, and process workflows.Collaborate with the Software Development and Business Applications teams to implement changes based on new requirements, ensuring robust design is maintained.Continuous Learning and Skill DevelopmentStay up to date with industry trends, best practices, and emerging technologies related to data engineering.Participate in training sessions and workshops to enhance your technical skills.Minimum Skills or Experience Requirements:Bachelor\u2019s degree in Computer Science, Data Engineering, or equivalent technical degree3+ years within a professional business data engineering roleKnowledge of business intelligence/reporting technologyExperience with SQL and relational databases (e.g., PostgreSQL, MySQL)Experience with cloud solution technologies such as Azure or AWSKnowledge of dimensional modeling and schema designFamiliarity building data warehouses and data martsExposure to version control systems (e.g., Git)Experience working with diverse corporate functions and understanding diverse data needsExcellent verbal and written communication skillsProficient use of Microsoft office tools including Word, Excel, Outlook, SharePoint, Power BIExperience using task management tools like but not limited to; Jira, Confluence, MS Teams, or othersStrong problem-solving skills and attention to detailEagerness to learn and grow in a dynamic data engineering environmentEntrepreneurial spiritSOLV Energy Is an Equal Opportunity EmployerAt SOLV Energy we celebrate the power of our differences. We are committed to building diverse, equitable, and inclusive workplaces that improve our communities. SOLV Energy prohibits discrimination and harassment of any kind against an employee or applicant based on race, color, age, religion, sex, sexual orientation, gender identity or expression, marital status, national origin, or ethnicity, mental or physical disability, veteran status, parental status, or any other characteristic protected by law.Benefits:Employees (and their families) are eligible for medical, dental, vision, basic life and disability insurance. Employees can enroll in our company\u2019s 401(k) plan and are provided vacation, sick and holiday pay.Compensation Range:$103,430.00 - $129,288.00Pay Rate Type:SalarySOLV Energy does not accept unsolicited candidate introductions, referrals or resumes from third-party recruiters or staffing agencies. We require all third-party recruiters to communicate exclusively with our internal talent acquisition team. SOLV Energy will not pay a placement fee to any third-party recruiter or agency that has not coordinated their recruiting activity with the appropriate member of our internal talent acquisition team.In addition, candidate introductions or resumes can only be submitted to our internal talent acquisition recruiting team if a signed vendor agreement is already on file and the third-party recruiter or agency has received formal instructions from our internal talent acquisition team to submit candidates for a particular job posting.Any unsolicited candidate introductions, referrals or resumes sent by third-party recruiters to SOLV Energy or directly to any of our employees, or received through our website or career portal, will be considered property of SOLV Energy and will not be eligible for a placement fee. In the event a third-party recruiter submits a resume or refers a candidate without a previously signed vendor agreement, SOLV Energy explicitly reserves the right to pursue and hire the candidate(s) without financial liability to such third-party recruiter.If you\u2019re interested in a meaningful career with a brighter future, join the SOLV Energy Team.\n",
    "location": "Bend, OR"
  },
  "129": {
    "id": "4168665595",
    "Company": "Delta Air Lines",
    "title": "IT Data Engineer",
    "description": "United States, Georgia, AtlantaInformation Technology 26-Feb-2025Ref #: 27712How you'll help us Keep Climbing (overview & key responsibilities)Delta Air Lines, Inc. has multiple openings for IT Data Engineers in Atlanta, Georgia.Key ResponsibilitiesAdministrate and support data integration and analytics tools. Perform server patching activities to support OS and security upgrades and vulnerabilities. Work with business users and developers to enable access to BI tools and data platforms. Manage database connection set up for Informatica workflows within developer tool and server. Deploy system administration, operational support, and problem resolution for SAS GRID v9.4, Linux platform. Create and maintain continuity of business plans for SAS and Informatica applications, documenting recovery requirements and procedures in Linux. Perform database connection set up for multiple relational database types including Oracle, SQL Server, Teradata, SAP Hana, DB2, AWS Aurora, Redshift, and Postgres. Rewrite ETL Informatica Jobs to Python Programming Language and AWS. Telecommuting permitted 2 to 3 days per week.Benefits and Perks to Help You Keep ClimbingBenefitsOur culture is rooted in a shared dedication to living our values \u2013 Care, Integrity, Resilience, Servant Leadership, and Teamwork \u2013 every day, in everything we do. At Delta, our people are our success. At the heart of what we offer is our focus on Sharing Success with Delta employees. Exploring a career at Delta gives you a chance to see the world while earning great compensation and benefits to help you keep climbing along the way:Competitive salary, industry-leading pro\ufb01t sharing program, and performance incentives 401(k) with generous company contributions up to 3% Paid time off including paid personal timeDomestic and International space-available flight privileges for employees and eligible family membersCareer development programs to achieve your long-term career goals World-wide partnerships to engage in community service and innovative goals created to focus on sustainability and reducing our carbon footprintBusiness Resource Groups created to connect employees with common interests to promote inclusion, provide perspective and help implement strategiesRecognition rewards and awards through the platform Unstoppable TogetherAccess to over 500 discounts and specialty savings through Deltaperks such as car and hotel rentals, discounts on tickets, and more.What You Need To Succeed (minimum Qualifications)A Bachelor\u2019s degree in Computer Science, Computer Engineering, Information Systems, or related field, and 3 years of experience in systems engineering. In the alternative, will accept a Master\u2019s Degree in Computer Science, Computer Engineering, Information Systems, or related field, and 1 year of experience in systems engineering.Must Have Work Experience With Linux Shell scripts, cronjobs, and Linux commands  Using SQL to support relational database connectivity in Teradata and other Database Management Systems (DBMS)including Oracle, DB2LUW, DB2 z/OS, Sybase, MS SQL Server, and SAP HANA  Working with business users and developers to understand their requirements using the data integration and analytic tools including Informatica, Linux shell scripting, Python, and AWS Air Flow; and  Identifying engineering problems and finding solutions including conducting problem analysis, use of operating system tools, such as Linux, to investigate potential causes, use of operating system knowledge to narrow possible problems, and communication of potential solutions. Airline Experience Preferred.What will give you a competitive edge (preferred qualifications)< Go back\n        ",
    "location": "Atlanta, GA"
  },
  "130": {
    "id": "4189605476",
    "Company": "Joinrs US",
    "title": "[Remote] Data Engineer ",
    "description": "This position is in SNC Sierra Nevada Corporation, a global leader in aerospace and national securityJoinrs AI's summary of the opportunity: SNC seeks a full-time Data Engineer II with a Bachelor's degree in engineering and 2 years of experience. You will design and develop cloud-based data pipelines and transformations. We offer medical, dental, vision plans, 401(k) with 150% match, life insurance, 3 weeks paid time off, and tuition reimbursement.The selection process will be fully managed by SNC Sierra Nevada Corporation. \u2014SNC can only hire in these states: AL, AK, AZ, CA, CO, FL, GA, HI, ID, IL, IN, KS, KY, LA, MD, MA, MI, MN, MS, MO, MT, NE, NV, NH, NJ, NM, NC, OH, OK, OR, PA, SC, TN, TX, UT, VA, WA, WV, WI.As a Data Engineer II, you'll be involved in designing, developing, documenting, testing, and debugging new and existing cloud-based data pipelines and transformations. In this role, you will collaborate in design meetings and work closely with a team of engineers to develop processes and structures that ingest data from multiple sources into our cloud-based Enterprise Data Warehouse (EDW).You will leverage a variety of tools within the EDW to implement and continuously improve data integration, master data management, data lifecycle management, data security, data quality management, metadata management, and reporting and analytics. Additionally, you'll be responsible for performing defect corrections, which includes analysis, design, and coding.This role offers a unique opportunity to work on fulfilling and cutting-edge technology projects that are central to our data strategy. If you're passionate about data and eager to make a significant impact, this is the perfect role for you!As SNC's corporate team, we provide the company and its business areas with strategic direction and business support spanning executive management, finance and accounting, operations, human resources, legal, IT, information security, facilities, marketing, and communications.Responsibilities:You are a smart, collaborative person who takes ownership and gets things done, curious about cloud-based tools, architectures, and languages, capable of breaking technical requests into manageable tasks, and comfortable asking for help and raising concerns as needed.You have foundational, hands-on experience with at least one cloud platform (AWS, Azure, GCP), at least one programming language (e.g. Python, Java), and demonstrated experience with SQL data modeling.You will leverage strong technical skills to build, implement, and improve cloud-based data pipelines, design and implement data models that ensure data accuracy, completeness, and consistency, develop maintainable infrastructure, DevOps, and standards for a robust EDW, troubleshoot and resolve issues related to production cloud resources, and learn and adapt to new tools and techniques.You will leverage strong communication skills to consult with business clients to gather technical requirements for data ingestions and models, communicate solution design pros and cons to the team technical lead, explain technical problems to leadership, business, and other developers, and document processes and standards.Qualifications You Must Have:Bachelor's Degree in a related field with at least 2 years of relevant experienceHigher education may substitute for relevant experienceRelevant experience may be considered in lieu of required educationWorking SQL knowledge and experience working with relational databasesExperience with AWS cloud services: S3, Redshift, GlueExperience building data pipelines, architectures, and data setsExperience performing root cause analysis on data to answer specific business questions or issuesExperience with object-oriented scripting languages and frameworksOperational responsibilities (schedules, monitoring, logging, alerting, error handling, etc.)Qualifications We Prefer:Experience performing root cause analysis on data to answer specific business questions or issuesExperience with additional cloud servicesExperience with cloud security best practicesExperience with object-oriented scripting languages and frameworks: Python (BOT03), PySpark, Java, etcMaster Data Management (MDM) Solution experienceStrong presentation, facilitation, collaborarion, negotiation, and influencing skills.Estimated Starting Salary Range: $108,496.89 - $149,183.22. SNC considers several factors when extending job offers, including but not limited to candidates\u2019 key skills, relevant work experience, and education/training/certifications.SNC offers a generous benefit package, including medical, dental, and vision plans, 401(k) with 150% match up to 6%, life insurance, 3 weeks paid time off, tuition reimbursement, and more.IMPORTANT NOTICE:To conform to U.S. Government international trade regulations, applicant must be a U.S. Citizen, lawful permanent resident of the U.S., protected individual as defined by 8 U.S.C. 1324b(a)(3), or eligible to obtain the required authorizations from the U.S. Department of State or U.S. Department of Commerce.Learn more about the background check process for Security Clearances.SNC is a global leader in aerospace and national security committed to moving the American Dream forward. We\u2019re known and respected for our mission and execution focus, agility, and disruptive and rapid innovation. We provide leading edge technologies and transformative solutions that support our nation\u2019s most critical security needs. If you are mission-focused, thrive in collaborative environments, and want to make our country stronger with state-of-the-art technologies that safeguard freedom, join our team!As an Equal Opportunity Employer, we welcome our employees to bring their whole selves to their work. SNC is committed to fostering an inclusive, accepting, and diverse environment free of discrimination. Employment decisions are made without regarding to race, color, age, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran or other characteristics protected by law. Contributions to SNC come in many shapes and styles, and we believe diversity in our workforce fosters new and greater ways to dream, innovate, and inspire.\n",
    "location": "United States"
  },
  "131": {
    "id": "4175542510",
    "Company": "Matlen Silver",
    "title": "Data Engineer",
    "description": "Job Title: Data Engineer\u00a0 Duration: 18 Month W2 Contract\u00a0 Location: Chicago, IL Required Pay Scale: $70-$75/hour Job Summary:Hadoop Ecosystem (hadoop, hive, spark)Kafka (building ETL pipelines, not building infrastructure)Elastic SearchScripting experience (doesn't matter what kind)strong SQL experienceend to end design experienceWe are seeking a skilled Data Engineer responsible for developing and delivering data solutions to accomplishtechnology and business goals and initiatives. Key responsibilities include performing code design and deliverytasks associated with the integration, cleaning, transformation, and control of data in operational and analyticaldata systems. Job expectations include working with stakeholders and Product and Software Engineering teams toaid with implementing data requirements, analyzing performance, and researching and troubleshooting dataproblems within system engineering domains.Requirements:\u2022 Strong SQL Skills \u2013 one or more of MySQL, HIVE, Impala, SPARK SQL\u2022 Data ingestion experience from message queue, file share, REST API, relational database, etc. and experiencewith data formats like json, csv, xml\u2022 Experience working with SPARK Structured steaming\u2022 Experience working with Hadoop/Big Data and Distributed Systems\u2022 Experience working with admin/prod support/debugging\u2022 Working experience with Spark, Sqoop, Kafka, MapReduce, NoSQL Database like HBase, SOLR, CDP or HDP,Cloudera or Hortonworks, Elastic Search, Kibana, etc.\u2022 Hands on programming experience in at least one of Scala, Python, PHP, or Shell Scripting, to name a few\u2022 Performance tuning experience with spark /MapReduce and/or SQL jobs\u2022 Experience and proficiency with Linux operating system is a must\u2022 Experience in end-to-end design and build process of Near-Real Time and Batch Data Pipelines\u2022 Experience working in Agile development process and deep understanding of various phases of the SoftwareDevelopment Life Cycle\u2022 Experience using Source Code and Version Control systems like SVN, Git, Bit Bucket etc.\u2022 Experience working with Jenkins and Jar management\u2022 Self-starter who works with minimal supervision and the ability to work in a team of diverse skill sets\u2022 Ability to comprehend customer requests and provide the correct solution\u2022 Strong analytical mind to help take on complicated problems\u2022 Desire to resolve issues and dive into potential issues\u2022 Ability to adapt and continue to learn new technologies is important About Matlen Silver Experience Matters. Let your experience be driven by our experience. For more than 40 years, Matlen Silver has delivered solutions for complex talent and technology needs to Fortune 500 companies and industry leaders. Led by hard work, honesty, and a trusted team of experts, we can say that Matlen Silver technology has created a solutions experience and legacy of success that is the difference in the way the world works. \u00a0 Matlen Silver is an Equal Opportunity Employer and considers all applicants for all positions without regard to race, color, religion, gender, national origin, age, sexual orientation, veteran status, the presence of a non-job-related medical condition or disability, or any other legally protected status. If you are a person with a disability needing assistance with the application or at any point in the hiring process, please contact us at email and/or phone at: info@matlensilver.com // 908-393-8600 \u00a0\n        ",
    "location": "Chicago, IL"
  },
  "132": {
    "id": "4161055903",
    "Company": "Interwell Health",
    "title": "Data Engineer",
    "description": "Interwell Health\u2019s Data Engineer role is responsible for corporate claims data acquisition, file loading and structure development. The ideal candidate will have a strong background in data engineering and will be responsible for designing and developing database systems, processes, and documentation. This includes collecting data, analyzing data, designing algorithms, drawing flowcharts, and implementing code, and maintaining the EDW databases.The work you will do:Collaborates effectively with the development groups to deliver projects to the satisfaction of the client in a timely fashion.Maintains current knowledge of the latest and newest technologies in the areas of specific relevancy to the Data Architecture group for utilization as appropriate within the department. Identifies and researches enhancement options and process improvements, making suggestions to senior managers as needed and drive continued improvement and innovation.Assist with schema design, code review, SQL query tuning.Write and deploy SQL code.Guide and mentor junior teammates acting as a resource with respect to the definition of development processes, methodologies and frameworks and other technical aspects of the projects and provides direction and assistance regarding working with users and the process and issues encountered with user interaction.Become skilled with the architecture and technology supporting the Datawarehouse, to design and develop accordingly.Ensure the distribution of knowledge for processes being designed and built within the team to ensure assigned jobs are completed accurately and according to schedule and that all deadlines are met.Ensure the design of sufficient documentation for easy and smooth hand-over of projects.Participate in the formulation and design of methodologies.Trains in developing tools that are available with the Datawarehouse and Lakehouse infrastructure and help with development, where appropriate.Monitor and troubleshoot data pipelines to identify and resolve performance issues, bottlenecks, and data anomalies.The skills and qualifications you need:3-5 years related experience in data engineering with bachelor\u2019s degree; or a master\u2019s degree with 3 years\u2019 experience.Ability to architect simple and complex solutions related to integration with other applications and design.Excellent communication skills, both verbal and written.Strong presentation skills. Ability to present formally to users and customers during gathering requirements, workshops, and feedback sessions.Experience working under tight deadlines while maintaining high product relevance and quality.Azure/SQL focusStrong understanding of data lake and Lakehouse architecture principles, data modeling techniques, and data warehousing concepts.3-5 years\u2019 experience using Azure Data Factory pipelines and Databricks.5 years\u2019 experience working with Azure databases.Extensive experience with MS SQL stack. (SSIS and ADF)Extensive experience with Azure - Cloud migration.Python experience.\n",
    "location": "United States"
  },
  "133": {
    "id": "4184059539",
    "Company": "Emerson",
    "title": "Data Engineer",
    "description": "Job DescriptionWe are seeking a highly skilled Data Engineer with 5 to 8 years of experience to join our Data Management team. The candidate must have extensive experience in Azure technologies and sophisticated Python skills. Experience with Oracle technologies would be an added advantage. This role involves crafting, building, and optimizing data pipelines, ensuring data security, and enabling analytical capabilities across the organization.IN THIS ROLE, YOUR RESPONSIBILITIES WILL BE: Develop, test, and maintain high-quality software products by using ground breaking technologies and best programming practices.Design, develop, and maintain scalable and high-performance data pipelines using Azure Data Factory, Azure Synapse Analytics, and other Azure services.Develop and maintain ETL/ELT processes to ensure the quality, consistency, and accuracy of data.Implement data integration solutions across on-premise, hybrid, and cloud environments.Ensure the security, availability, and performance of enterprise data platforms.Work with relational (SQL Server, Azure SQL) and non-relational databases (Azure Cosmos DB, etc.).Build, test, and deploy data solutions using Azure DevOps, version control, and CI/CD pipelines.Develop and implement data governance policies and practices to ensure data integrity and security.Perform code reviews and ensure the quality of work from the other developers are as per the standards.Collaborate with multi-functional teams, including designers, developers, and quality assurance engineers to build, refine, and improve software products.Optimize and solve large datasets using Python and Azure cloud-native technologies.Build and maintain comprehensive user documentation. WHO YOU ARE: A competent professional characterized by having a strong instinct to act, consistently pursuing self-development to enhance skills and knowledge. You possess a drive for achieving results, demonstrating a commitment to perfection in their endeavors. Tech-savvy professional adept at demonstrating technological tools to streamline processes and enhance productivity. You excel in collaborative efforts, recognizing the value of collaboration and optimally contributing to collective goals.FOR THIS ROLE, YOU WILL NEED:  Bachelor\u2019s degree in computer science or a related field  5+ years of experience as a Data Engineer, focusing on Azure cloud services with a Solid understanding of Python/PySpark programming language. Strong hands-on experience with Azure Data Factory, Azure Synapse Analytics, Azure SQL Database, and Azure Storage.Experience with Azure Databricks and PySpark is highly desirableStrong SQL skills, including experience with data modeling, sophisticated queries, and performance optimization.Expertise in Delta Tables and Delta live tables Familiarity with version control systems (e.g., Git) and CI/CD pipelines (Azure DevOps).Knowledge of Data Lake Architecture, Data Warehousing, and Data Modeling principles.Strong problem-solving, communication, and collaboration skills.In-depth understanding of the Python software development stacks, ecosystems, frameworks, and tools such as NumPy, SciPy, Pandas, sci-kit-learn and PyTorch.Writing scalable code using Python programming language, Multi-process architecture Integrating and running notebooks into Data Pipelines  Able to work in the US without need for sponsorship now or in the future PREFERRED QUALIFICATIONS THAT SET YOU APART:  Experience in ODI/OBIEE and PL/SQL would be a plus.  Experience in implementing Medallion Architecture  Understanding of Dimensional Modeling and Data Mesh Architecture is an added  Experience with RESTful APIs, Data APIs, and event-driven architecture  Familiarity with standard processes in data governance, lineage, security, and privacy.  Strong analytical, communication, and teamwork skills.  Microsoft Certified: Azure Solutions Architect Authority  Microsoft Certified: Azure Fundamentals Our Culture & Commitment to YouAt Emerson, we prioritize a workplace where every employee is valued, respected, and empowered to grow. We foster an environment that encourages innovation, collaboration, and diverse perspectives\u2014because we know that great ideas come from great teams. Our commitment to ongoing career development and growing an inclusive culture ensures you have the support to thrive. Whether through mentorship, training, or leadership opportunities, we invest in your success so you can make a lasting impact. We believe diverse teams, working together are key to driving growth and delivering business results.We recognize the importance of employee wellbeing. We prioritize providing flexible, competitive benefits plans to meet you and your family\u2019s physical, mental, financial, and social needs. We provide a variety of medical insurance plans, with dental and vision coverage, Employee Assistance Program, 401(k), tuition reimbursement, employee resource groups, recognition, and much more. Our culture offers flexible time off plans, including paid parental leave (maternal and paternal), vacation and holiday leave.Learn more about our  Culture & Values . About UsWHY EMERSONOur Commitment to Our PeopleAt Emerson, we are motivated by a spirit of collaboration that helps our diverse, multicultural teams across the world drive innovation that makes the world healthier, safer, smarter, and more sustainable. And we want you to join us in our bold aspiration.We have built an engaged community of inquisitive, dedicated people who thrive knowing they are welcomed, trusted, celebrated, and empowered to solve the world\u2019s most complex problems \u2014 for our customers, our communities, and the planet. You\u2019ll contribute to this vital work while further developing your skills through our award-winning employee development programs. We are a proud corporate citizen in every city where we operate and are committed to our people, our communities, and the world at large. We take this responsibility seriously and strive to make a positive impact through every endeavor.At Emerson, you\u2019ll see firsthand that our people are at the center of everything we do. So, let\u2019s go. Let\u2019s think differently. Learn, collaborate, and grow. Seek opportunity. Push boundaries. Be empowered to make things better. Speed up to break through. Let\u2019s go, together.Work AuthorizationEmerson will only employ those who are legally authorized to work in the United States. This is not a position for which sponsorship will be provided. Individuals with temporary visas such as E, F-1(including those with OPT or CPT) , H-1, H-2, L-1, B, J or TN, or who need sponsorship for work authorization now or in the future, are not eligible for hire.Equal Opportunity EmployerEmerson is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to sex, race, color, religion, national origin, age, marital status, political affiliation, sexual orientation, gender identity, genetic information, disability or protected veteran status. We are committed to providing a workplace free of any discrimination or harassment.Accessibility Assistance or AccommodationIf you have a disability and are having difficulty accessing or using this website to apply for a position, please contact: idisability.administrator@emerson.com .About EmersonEmerson is a global leader in automation technology and software. Through our deep domain expertise and legacy of flawless execution, Emerson helps customers in critical industries like life sciences, energy, power and renewables, chemical and advanced factory automation operate more sustainably while improving productivity, energy security and reliability.With global operations and a comprehensive portfolio of software and technology, we are helping companies implement digital transformation to measurably improve their operations, conserve valuable resources and enhance their safety.We offer equitable opportunities, celebrate diversity, and embrace challenges with confidence that, together, we can make an impact across a broad spectrum of countries and industries. Whether you\u2019re an established professional looking for a career change, an undergraduate student exploring possibilities, or a recent graduate with an advanced degree, you\u2019ll find your chance to make a difference with Emerson. Join our team \u2013 let\u2019s go!No calls or agencies please.\n",
    "location": "Round Rock, TX"
  },
  "134": {
    "id": "4188347369",
    "Company": "MeridianLink",
    "title": "Data Engineer - #1696",
    "description": "Job SummaryWe are looking for an accomplished Data Engineer to join our quickly growing Analytics team. The hire will be responsible for expanding and improving our data and data pipeline architecture, as well as optimizing data flow and MDM for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.The Data Engineer will support database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company\u2019s data pipelines to support our next generation of products and data initiatives.Responsibilities Design, develop, and operate large scale data pipelines to support internal and external consumers Improve and automate internal processes Integrate data sources to meet business requirements Write robust, maintainable, well documented codeQualifications 2-4 years professional Data Engineering and Data warehousing experience Bachelor's degree is highly preferred Extremely strong implementation experience in Python, Parquet, Spark, Azure Databricks, Delta Lake, Databricks Data Warehouse. Databricks workflows, Delta Sharing and Unity Catalog. SQL development knowledge \u2013 Stored procedures, triggers, jobs, indexes, partitioning, pruning etc. Be able to write/debug complex SQL queries ETL/ELT and Data-warehousing techniques and best practice Implementation experience with building data pipelines and modelling Salesforce data, Netsuite data and Hubspot is a plus. Experience building, maintaining, and scaling ETL/ELT processes and infrastructure Implementation experience with various data modelling techniques Implementation experience working with a BI visualization tool (Sisense is a plus) Experience with CI/CD tools (Preferred Gitlab, Jenkins) Experience with cloud infrastructure (Azure strongly preferred) Experience working in a fast-paced product environment, with an attitude of getting the job done with the least amount of tech debtThe following experience is highly preferred: UI development frameworks such as java script, Django, REACT etc. Knowledge of being able to work with a variety of Ingestion patterns such as API/SQL servers etc. Knowledge of Master Data Management Prior Financial industry experience a plus. Be able to navigate ambiguity and pivot based on business priorities with ease. Strong communication, negotiating and estimating skills. Be a team player and should be able to collaborate well.At MeridianLink, we work together to design, implement, test, and deliver state-of-the-art web applications for the financial services industry, using the latest technologies including cloud computing, mobile development, responsive design, ASP.NET , JavaScript, C#, VB.NET , and SQL Server.OUR CULTUREOur low turnover is a testament to our wonderful culture where people value the work they do and appreciate each other for their contributions. MeridianLink develops our employees so they can grow professionally by preferring to promote from within. We have an open door policy with direct access to executives; we want to hear your ideas and what you think. Our company believes that to be productive in the long term, we must have a genuine work-life balance. We understand that employees have families and full lives outside of the office. To that end, we honor their personal commitments.MeridianLink is an Equal Opportunity Employer. We do not discriminate on the basis of race, religion, color, sex, age, national origin, disability or any other characteristic protected by applicable law.MeridianLink runs a comprehensive background check, credit check and drug test as part of our offer process.MeridianLink has a wonderful culture where people value the work they do and appreciate each other for their contributions. We develop our employees so they can grow professionally by preferring to promote from within. We have an open-door policy with direct access to executives; we want to hear your ideas and what you think. Our company believes that to be productive in the long term, we must have a genuine work-life balance. We understand that employees have families and full lives outside of the office. To that end, we honor their personal commitments.MeridianLink is an Equal Opportunity Employer. We do not discriminate based on race, religion, color, sex, age, national origin, disability, or any other characteristic protected by applicable law.MeridianLink runs a comprehensive background check, credit check, and drug test as part of our offer process.Salary range of $94,500 - $133,400. [It is not typical for offers to be made at or near the top of the range.] The actual salary will be determined based on experience and other job-related factors permitted by law including geographical location.Meridianlink offers:Potential For Equity-Based AwardsInsurance coverage (medical, dental, vision, life, and disability)Flexible paid time offPaid holidays401(k) plan with company matchRemote workAll compensation and benefits are subject to the terms and conditions of the underlying plans or programs, as applicable and as may be amended, terminated, or superseded from time to time.\n",
    "location": "United States"
  },
  "135": {
    "id": "4178067645",
    "Company": "Adobe",
    "title": "Data Engineer",
    "description": "Our CompanyChanging the world through digital experiences is what Adobe\u2019s all about. We give everyone\u2014from emerging artists to global brands\u2014everything they need to design and deliver exceptional digital experiences! We\u2019re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.We\u2019re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!Be a part of Adobe\u2019s Experience Platform; our fastest growing business in the Experience Cloud! The Adobe Experience Platform manages petabytes of data on behalf of organizations allowing them to have a centralized and standardized data platform applying data science and machine learning to improve the design and delivery of rich, personalized experiences.Adobe Experience Platform is seeking a Software Development Engineer to join the operational intelligence team. We build scalable, performant services and tools to handle end to end customer lifecycle from provisioning everything needed for onboarding customers when they purchase AEP to analyzing customer usage data and behaviors to generate business critical insights.We are looking for innovative and passionate software engineers to build low latency & highly scalable fault tolerant systems.What you will do: Design and develop distributed services that are resilient, highly available and scalable.  Collaborate with business partners, architects, technical leads, product management and analysts to develop high-quality customer centric solutions.  Participate in all aspects of software development activities, including design, coding, code review, unit and integration testing, bug fixing, deploy and code/API documentation.  Own feature development from inception to production rollout and postmortem & contribute to the development of engineering processes.  Help evaluate innovative technologies and incorporate them into our stack. What you will need to succeed: B.S. or M.S. in Computer Science or equivalent engineering degree.  3+ years of software engineering experience having built highly maintainable, scalable systems with Scala/Java or comparable strongly typed language.  Experience with data transformation & ELT pipelines on large data sets using Databricks, SnowFlake, SQL, Python, Jupyter Notebooks.  Excellent data analysis, problem-solving skills & proficiency with data visualization tools (e.g. Power BI, Tableau, Looker) Experience with building & deploying machine learning models & ML pipelines such as Sklearn, Tensorflow, PyTorch, KubeFlow, MLFlow, SageMaker, or similar.  Ability to multi-task simultaneously different projects, having a positive outlook, motivated learner with strong interpersonal and written and verbal communication skills. What will help you stand out from the crowd Unending curiosity, thoroughness, tenacity and focus on designing and building complex software systems with excellent quality to address customer problems.  Experience developing backend distributed applications on Java/JVM and Spring (or similar framework).  Shown experience using structured, focused approaches to solving technical, data, and logical problems. Application Window NoticeIf this role is open to hiring in Colorado (as listed on the job posting), the application window will remain open until at least 12:01 AM Pacific Time, in compliance with Colorado pay transparency regulations. If this role does not have Colorado listed as a hiring location, no specific application window applies, and the posting may close at any time based on hiring needs.Our compensation reflects the cost of labor across several\u202f U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position\u202fis $113,400 -- $206,300 annually. Pay\u202fwithin this range varies by work location\u202fand may also depend on job-related knowledge, skills,\u202fand experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.Adobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and \u201cfair chance\u201d ordinances.Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other\u2019s employees.\n        ",
    "location": "San Jose, CA"
  },
  "136": {
    "id": "4151540873",
    "Company": "Underdog",
    "title": "Data Engineer",
    "description": "Hi, we\u2019re Underdog!We\u2019re the fastest-growing sports gaming company ever.We build innovative games and products for American sports fans.Founded in 2020, our team built four of today\u2019s most widely played fantasy games and recently launched our Underdog Sportsbook. We are the only sportsbook to ever launch on our own home grown technology, which allows us to build different and innovative experiences. We believe there\u2019s so much more to be built for sports fans, and we\u2019ll continue to win by building the best products and experiences for our customers.The opportunity in front of us to become the biggest company in our space is massive; after all, we\u2019re currently sitting in the fastest-growing consumer industry in the U.S. In just over two years, we reached a nearly $500 million valuation through some of the best investors in the game, including Mark Cuban, Kevin Durant, BlackRock, and SV Angel. We are many times larger now and our growth is not slowing down.At Underdog, we believe that sports are for everyone and are building a tomorrow for every fan. Want to help create that future? Join us.About the role and why it\u2019s unique:As a Data Engineer on the Data Platform team, you\u2019ll be architecting and developing Underdog\u2019s sports focused data systems and data pipelinesDesign, build, and optimize scalable data platform infrastructure using Terraform to support batch and real-time data processing and storage on the cloud using Kafka, Kinesis, RDS, and S3Implement and maintain monitoring, alerting, and logging mechanisms to ensure the health and performance of the data platformCollaborate with data scientists, engineers, and business stakeholders to understand data requirements and translate them into scalable technical solutionsLead code reviews, provide constructive feedback, and evangelize best practices to maintain code and data qualityResearch and keep up to date on emerging data technologies and trends and focus on iteratively implementing them into Underdog\u2019s data systemsWho you are:At least 3 years of experience building scalable and durable data pipelines and data systems on a cloud environment (e.g. AWS, GCP, Azure)Highly focused on delivering results for internal and external stakeholders in a fast-paced, entrepreneurial environmentExcellent leadership and communication skills with ability to influence and collaborate with stakeholdersStrong familiarity with distributed computing and data storage mechanisms on the cloud environment with hands-on experience in managing data infrastructureFamiliarity with containerization and orchestration technologies such as Docker, Kubernetes, or ECSExperience with data streaming frameworks such as Apache Kafka, Apache Flink, or KinesisAdvanced proficiency with Python and SQLAt least one year of experience with GolangExperience with DevOps practices such as CI/CD pipelines, and infrastructure-as-code tools (e.g. Terraform, CDK, CloudFormation)Even better if you have:Strong interest in sportsPrior experience in the sports betting industryKnowledge of ML concepts, with some experience in building inference systemsOur target starting base salary range for this position is between $135,000 and $150,000, plus target equity. The starting base salary will depend on a number of factors including the candidate\u2019s skills and experience, among other things.What we can offer you:Unlimited PTO (we're extremely flexible with the exception of the first few weeks before & into the NFL season)16 weeks of fully paid parental leaveA $500 home office allowanceA connected virtual first culture with a highly engaged distributed workforce5% 401k match, FSA, company paid health, dental, vision plan options for employees and dependentsThis position may require sports betting licensure based on certain state regulations.Underdog is an equal opportunity employer and doesn't discriminate on the basis of creed, race, sexual orientation, gender, age, disability status, or any other defining characteristic.\n",
    "location": "United States"
  },
  "137": {
    "id": "3890650480",
    "Company": "Demyst",
    "title": "Data Engineer (United States)",
    "description": "Our SolutionDemyst unlocks innovation with the power of data. Our platform helps enterprises solve strategic use cases, including lending, risk, digital origination, and automation, by harnessing the power and agility of the external data universe. We are known for harnessing rich, relevant, integrated, linked data to deliver real value in production. We operate as a distributed team across the globe and serve over 50 clients as a strategic external data partner. Frictionless external data adoption within digitally advancing enterprises is unlocking market growth and allowing solutions to finally get out of the lab. If you like actually to get things done and deployed, Demyst is your new home.The OpportunityAs a Data Engineer at Demyst, you will be powering the latest technology at leading financial institutions around the world. You may be solving a fintech's fraud problems or crafting a Fortune 500 insurer's marketing campaigns. Using innovative data sets and Demyst's software architecture, you will use your expertise and creativity to build best-in-class solutions. You will see projects through from start to finish, assisting in every stage from testing to integration.To meet these challenges, you will access data using Demyst's proprietary Python library via our JupyterHub servers, and utilize our cloud infrastructure built on AWS, including Athena, Lambda, EMR, EC2, S3, and other products. For analysis, you will leverage AutoML tools, and for enterprise data delivery, you'll work with our clients' data warehouse solutions like Snowflake, DataBricks, and more.Demyst is a remote-first company. The candidate must be based in the United States.ResponsibilitiesCollaborate with internal project managers, sales directors, account managers, and clients' stakeholders to identify requirements and build external data-driven solutionsPerform data appends, extracts, and analyses to deliver curated datasets and insights to clients to help achieve their business objectivesUnderstand and keep current with external data landscapes such as consumer, business, and property data.Engage in projects involving entity detection, record linking, and data modelling projectsDesign scalable code blocks using Demyst's APIs/SDKs that can be leveraged across production projectsGovern releases, change management and maintenance of production solutions in close coordination with clients' IT teamsRequirementsBachelor's in Computer Science, Data Science, Engineering or similar technical discipline (or commensurate work experience); Master's degree preferred1-3 years of Python programming (with Pandas experience)Experience with CSV, JSON, parquet, and other common formatsData cleaning and structuring (ETL experience)Knowledge of API (REST and SOAP), HTTP protocols, API Security and best practicesExperience with SQL, Git, and AirflowStrong written and oral communication skillsExcellent attention to detailAbility to learn and adapt quicklyBenefitsDistributed working team and cultureGenerous benefits and competitive compensationCollaborative, inclusive work culture: all-company offsites and local get togethers in BangaloreAnnual learning allowanceOffice setup allowanceGenerous paid parental leaveBe a part of the exploding external data ecosystemJoin an established fast growth data technology businessWork with the largest consumer and business external data market in an emerging industry that is fueling AI globallyOutsized impact in a small but rapidly growing team offering real autonomy and responsibility for client outcomesStretch yourself to help define and support something entirely new that will impact billionsWork within a strong, tight-knit team of subject matter expertsSmall enough where you matter, big enough to have the support to deliver what you promiseDemyst is committed to creating a diverse, rewarding career environment and is proud to be an equal opportunity employer. We strongly encourage individuals from all walks of life to apply.\n",
    "location": "United States"
  },
  "138": {
    "id": "4187551308",
    "Company": "NOV",
    "title": "Data Engineer",
    "description": "Key Responsibilities JOB DESCRIPTION Data Ecosystem DevelopmentDesign and develop a robust, high-performance data ecosystem.Build and launch reliable data pipelines to extract, transform, and load both large and small data sets.Optimize existing pipelines and ensure smooth operation of domain-related data flows.Implement data quality checks to maintain high data integrity.Leverage DevOps practices to develop automated CI/CD processes that expedite testing, deployment, and promotion of analytics models.Develop, test, deploy and maintain efficient streaming and batch data ingestion pipelines.Document and enforce architecture and coding standards for all supported platforms.Participate in all stages of the software development lifecycle\u2014from requirements gathering to design, development, testing, and support.Work collaboratively with the Analytics team and cross-functional partners to deliver innovative solutions.Proactively identify technology risks and seek guidance as needed.Embrace agile methodologies and adhere to established quality and management procedures.EducationBasic QualificationsBachelor\u2019s/Masters degree in Computer Science, Computer Engineering.Databricks or Data engineering certification will be preferred.Technical Experience4+ years of development experience using Python, PySpark or another modern programming language.Advanced knowledge in data modeling techniques such as Database Normalization, Entity RelationshipHands-on expertise in designing and managing data pipelines across diverse sources and targets (e.g. Databricks, SQL Server, Data Lakes, distributed-based systems, SQL and No-SQL databases).Proven experience in batch and real-time ETL pipelines implementation, and maintenance.Experience with common data engineering tools(e.g., Git, JIRA, Confluence)If you\u2019re ready to drive innovation and deliver impactful analytics solutions at NOV, we\u2019d love to hear from you.\n        ",
    "location": "Houston, TX"
  },
  "139": {
    "id": "4172317535",
    "Company": "Orion Innovation",
    "title": "Data Engineer",
    "description": "Orion Innovation is a premier, award-winning, global business and technology services firm. Orion delivers game-changing business transformation and product development rooted in digital strategy, experience design, and engineering, with a unique combination of agility, scale, and maturity. We work with a wide range of clients across many industries including financial services, professional services, telecommunications and media, consumer products, automotive, industrial automation, professional sports and entertainment, life sciences, ecommerce, and education.DutiesProven experience as a Data Engineer, with a strong focus on Azure Synapse Analytics.Proficiency in Python for data manipulation, scripting, and automation.Expertise in Azure services, including Azure Data Factory, Azure SQL Database, and Azure Storage.Hands-on experience with Azure AI Document Intelligence for document processing and data extraction.Create, manage, and optimize ETL processes to ensure efficient data flow and high data quality.Strong understanding of data warehousing concepts, ETL processes, and data modeling techniques.Monitor and troubleshoot data systems, ensuring optimal performance and reliability.Implement best practices for data security, metadata management, and compliance with organizational policies.Excellent problem-solving skills and ability to work in a fast-paced, collaborative environment.Effective communication and interpersonal skills.Act with integrity, professionalism, and personal responsibility to uphold the firm\u2019s respectful and courteous work environment.Benefits: It is recommended that employers include a general description of any health or compulsory benefits for compliance with Pay Transparency laws.SkillsPython (more than 3 years\u2019 experience with libraries to extract data from PDF, MS office files).Azure Synapse to create data Python pipeline jobs and clusters (REQUIRED)Good knowledge of API integrationExpertise in Azure services, including Azure Data Factory, Azure SQL Database, and Azure StorageProven experience as a Data Engineer, with a strong focus on Azure Synapse Analytics.Proficiency in Python for data manipulation, scripting, and automation.Hands-on experience with Azure AI Document Intelligence for document processing and data extraction.Create, manage, and optimize ETL processes to ensure efficient data flow and high data quality.Strong understanding of data warehousing concepts, ETL processes, and data modeling techniques.Monitor and troubleshoot data systems, ensuring optimal performance and reliability.Implement best practices for data security, metadata management, and compliance with organizational policies.Excellent problem-solving skills and ability to work in a fast-paced, collaborative environment.Effective communication and interpersonal skills.For emerging skill sets, years of experience may matter less than depth of knowledge & expertise but traditionally may have 5-10 years of experienceEducation: A university degree is preferredCertifications & Licenses: Microsoft Certified Database AdministratorOrion is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, citizenship status, disability status, genetic information, protected veteran status, or any other characteristic protected by law.Candidate Privacy PolicyOrion Systems Integrators, LLC And Its Subsidiaries And Its Affiliates (collectively, \u201cOrion,\u201d \u201cwe\u201d Or \u201cus\u201d) Are Committed To Protecting Your Privacy. This Candidate Privacy Policy (orioninc.com) (\u201cNotice\u201d) ExplainsWhat information we collect during our application and recruitment process and why we collect it;How we handle that information; andHow to access and update that information.Your use of Orion services is governed by any applicable terms in this notice and our general Privacy Policy.\n        ",
    "location": "Montvale, NJ"
  },
  "140": {
    "id": "4154001792",
    "Company": "mthree",
    "title": "Data Engineer",
    "description": "**Looking for local candidates**Want to work in technology in the financial industry?We are looking for someone to be a part of a dynamic team for one of our clients as a Data Engineer.About mthree:Since 2010, mthree has been helping clients solve their business and technological challenges. We are a technology and business consultancy with a global workforce delivering significant business and IT projects in some of the largest financial services organizations worldwide.Core ServicesConsulting and AdvisoryManaged ServicesAlumni Graduate ProgramAlumni Pro ProgramWe have a global presence and are experts in delivering exceptional quality to our client base, providing consulting services across Risk, Regulation & Compliance; Vendor Products; Application Support; Application Development; Cyber & Information Security; Data Science and DevOps areas.Our Expert program offers experienced professionals access to top roles in tech, finance, aviation and insurance. Join us to work on groundbreaking technology projects, from international trading platforms to critical applications for leading airlines. We recruit professionals who are eager to fast-track their careers in technology or operations within prestigious global organizations.Primary Responsibilities: The Balance Sheet & Reg Technology within Finance Tech is looking for a Data Engineer who will work on front to back initiative to re-platform and renovate key streams of data based on Teradata and Snowflake. These applications include large scale computational processing of datasets, and data model, data pipeline development, warehouse reporting.You will be expected to work as a member of the development team and participate in all aspects of the development lifecycle. The candidate will be working on providing Data model design & ETL development (SQL & Internal ETL Framework, and/or Python) as well as very intensive business & data analysis out of our Finance Data Warehouse (FDW) built in Teradata. The ideal candidate will possess solid technical and problem-solving skills as well as the drive to learn more about the business and new technologies. Candidate should be open to work from front to back on applications and must have a sound grasp of development best practices and system architecture.Skills Required: 3+ years of experience in Database & ETL development (ETL/SQL/Data Modelling). Familiar with best practice on development.Python (or similar language) experience required.Java (or Scala) programing language is required.Working knowledge of UNIX/LinuxDemonstrates exceptional analytical and problem-solving skills.Strong communication, organizational, and collaboration skills.Ability to multi-task and work well under pressure Experience working on large and medium scale projects in an agile environment.Skills Desired: Experience with Snowflake, Azure cloud is preferred.High interest in Finance business & data analysis required.Eager to work with new technologies and apply them towards enterprise-level data solutions.Experience building high volume and resilient systems.Ability to multi-task and work against deadlines/priorities.Capable of working with a global team and thriving in a team development environment.At mthree, our values support courageous teammates, needle movers, and learning champions all while striving to support the health and well-being of all employees. We take great pride in celebrating the diversity of each individual who contributes to making mthree the company it is today and will be in the future. We value diversity both within mthree and with our partner companies, and we're proud to provide an environment where all our colleagues can flourish. That means promoting a strong culture of equality but, most importantly, inclusion.\n",
    "location": "New York City Metropolitan Area"
  },
  "141": {
    "id": "4177375383",
    "Company": "Overhaul",
    "title": "Data Engineer",
    "description": "Who We AreOverhaul is a supply chain integrity solutions company that allows shippers to connect disparate sources of data into the first fully transparent situational analysis engine designed for the logistics industry. Data that is transformed into critical insights can instantly trigger corrective actions, impacting everything from temperature control to handling requirements or package-level tracking, ensuring cargo arrives at its destination safely, undamaged, and on time. We are a dynamic, innovative, and fun team who is highly committed to our customers\u2019 experiences and our Mission and Vision.Job SummaryWe are seeking an experienced Data Engineer to join our Data Platform team. The ideal candidate will be responsible for creating and maintaining data pipelines from a variety of data sources, designing and maintaining Bronze, Silver, and Gold data layers to provide clean, easily retrievable data for our internal and external users. These data pipelines will improve data accessibility and decision-making processes within Overhaul. This is fast growing team expanding into a variety of data roles including AI and Machine Learning, Data Science, and Analysis.This position is based in Austin, Texas, but remote work is available.Job DutiesDesign and maintain data pipelines using Azure Data Lake, Azure Synapse, Databricks, and SparkDesign and Maintain a Gold Layer interface to provide a clean, easy to use way to interact with our data through a variety of reporting tools. Create ML Pipelines to feed data and update a variety of models. Develop Semantic Models in Power BIDevelop ML models to enhance predictive analytics capabilities and support decision-making processes across various departments. Provide documentation and assistance to internal usersWork ExperienceExperience with Cloud based Data Lake ToolsExperience with designing and implementing data pipelinesPython and SparkSQL/Spark experiencePlusesAzure Data Tools, such as Data Lake, Synapse, or FabricPower BI ExperienceCertifications and EducationBS in Computer Science or Related field preferred, but not required Perks And BenefitsTop employee health and well- being benefits Care giver/adoption/family leaveUnlimited Vacation Policy Casual dress Rotating company \u2018Perks at Work\u2019 program Diversity and Inclusivity Statement Overhaul has always been, and always will be, committed to diversity and inclusion. Our Overhaul Culture Code\u2019s top listed commitment is to \u201cDiversity and Synergy.\u201d All aspects of employment will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law. We strongly encourage people from underrepresented groups to apply!\n        ",
    "location": "Austin, TX"
  },
  "142": {
    "id": "4188418028",
    "Company": "PPL Corporation",
    "title": "Data Engineer",
    "description": "Company Summary StatementLouisville Gas and Electric Company and Kentucky Utilities Company, part of the PPL Corporation (NYSE: PPL) family of companies, are regulated utilities that serve more than 1.3 million customers and have consistently ranked among the best companies for customer service in the United States. LG&E serves 334,000 natural gas and 433,000 electric customers in Louisville and 16 surrounding counties. KU serves 569,000 customers in 77 Kentucky counties and five counties in Virginia. LG&E and KU are major employers and active supporters of the communities they serve. They empower employees, community members and initiatives across their service territory through volunteerism and investments in organizations that support education, sustainability and wellbeing.OverviewThe main purpose of this position is to develop and maintain data reporting tools that are used by large groups of people to promote safe and reliable operations as well as inform business decision making. The Data Engineer will work as part of the Fleet Engineering team and leverage other company resources to integrate data science strategies to solve engineering problems.The position will be filled based on the successful candidates experience level: Engineer I, II, III or Principal.ResponsibilitiesDrive the collection of data and the refinement of existing data sources.Work with generation fleet to understand organizational processes and deliver relevant data solutions.Work with generation fleet to recommend improvements and efficiencies to improve utilization of data.Drive to optimize data workflows for performance, scalability, and cost efficiency.Implement and enhance the collection and calculation of key generation performance metrics, in addition to addressing data-related technical issues.Supports construction and hands-on development of software analysis tools, business intelligence (BI) insights, and other reporting solutions.Works with IT to troubleshoot and maintain existing data connections. This includes a working knowledge of ETL functions, APIs, database infrastructure, and SSIS processes.Supports the transition of implemented solutions to IT. Work closely with IT to facilitate knowledge transfer and develop documentation of standard processes that can be followed by others.Works with engineers of other disciplines to translate technical calculations into automated data reporting tools that can be used for reporting and decision making.Supports AVEVA (OSIsoft) PI systems across the fleet of generating sites. Serves as business proponent and helps configure points, set up notifications and reports, analyze data, develop calculations, maintain data connections, etc.Works with cross functional teams to implement new software solution to track unit outage events/data.Develops tools to automate existing reports that are widely distributed.Develops analysis tools that leverages data science to help drive power generation reliability improvement.Performs continuous verification of data accuracy.Supports the creation, modification, and updates to internal sharepoint pages.Identifies opportunities for improvement and/or suggests improved methods.Gathers, correlates, and analyses specific data using established procedures.Performs detailed and/or routine engineering assignments involving established calculations, tests, and analysis.Performs a variety of routine tasks to become familiar with the engineering methods, practices, and programs of the company.Supports operating and maintenance functions with problem identification, engineering analysis, and problem resolution activities.Prepares and presents reports giving conclusions of job related engineering activities.Assists other engineers with large projects, as needed.Assists with the development of goals, objectives, budgets, programs, and procedures.Plans and controls all activities so as to provide an accident-free work place.Conducts work activities with regard to environmental rules and regulations.Regularly reviews assignments to assure optimum efficiency and cost effectiveness.Occasionally directs the activities of coops, supervisors, and others as they perform various activities.Performs other duties as assigned, and complies with all policies and standardsQualificationsRequired Education ABET accredited degree in engineering or engineering technology is required.Preferred QualificationsABET Accredited Bachelor\u2019s degree in Computer EngineeringAbility to analyze and solve engineering problems from a business perspective.Ability to turn concepts into deliverables, translating requirements of internal customers into analytical tools.Ability to analyze and solve engineering problems from a business perspective.Proficiency in Python language, syntax, and management of associated development environments.Proficiency in the following is preferred: Microsoft Power BI, SharePoint, PowerPivot, R, C#, L, ETLs, APIs, TFS, Microsoft SQL Server Analysis Services (SSAS), Microsoft SQL Server Integration Services (SSIS), or SQL Server Reporting Services (SSRS).Ability to work with and advise a variety of personnel including engineers, management, technicians, contractors, co-op students, supervisors, consultants, colleagues, etc. This includes personnel is other lines of business (IT).Adaptable to varying priorities and customer needs, including time critical projects that require immediate attention, travel to the plants, and periodic overtime.Experience with AVEVA (OSIsoft) PI systems is preferred.\n",
    "location": "Louisville, KY"
  },
  "143": {
    "id": "4184873322",
    "Company": "Major League Soccer",
    "title": "Data Engineer",
    "description": "OverviewOur next generation data platform will enable the fans to have the best experience with Major League Soccer with personalized content and offerings through advanced analytics. This position will contribute to developing the future of our data platform by applying the latest capabilities of our cloud platform.This role is responsible for building scalable solutions for data ingestion, processing and analytics that crosscuts data engineering, architecture and software development. You will be accountable for design and implementation of cloud native solutions for data ingestion, processing and compute at scale.Responsibilities Design, implement, document and automate scalable production grade end to end data pipelines including API and ingestion, transformation, processing, monitoring and analytics capabilities while adhering to best practices in software development.  Build data-intensive application solutions on top of cloud platforms such as AWS, using state-of-the-art solutions including distributed compute, lake house, real-time streaming while enforcing standard methodologies for data modeling and engineering.  Work with infrastructure engineering team to setup infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources. Qualifications Bachelor\u2019s Degree in computer science or a related field required.  3+ years of overall experience, with 2+ years of related experience and a track record of building production software. Required Skills Hands-on experience building and delivering cloud native data solutions (AWS preferred)  Solid CS fundamentals with experience across a range of fields, with one or more areas of deep knowledge and experience in an advanced programming language.  Working experience of distributed processing systems including Apache Spark.  Working experience with MedallionLakehouse environment (AWS) ingesting data into Bronze / Silver / Gold layers  Hands-On Experience with AWS tech such as Glue, Athena. Kinesis, Lambda.  Hands-On Experience with lake house architecture, open table formats such as Hudi (or Iceberg), orchestration frameworks such as airflow, real time streaming with Apache Kafka and container technology.  Hands-On Experience with SQL, Python, pySpark.  Deep understanding of the best software practices and application of them in data engineering.  Familiarity with data science and machine learning workflows and frameworks  Ability to work independently, collaborate with cross-functional teams to complete projects, and lead integration of technical components with other teams as necessary.  High-level of commitment to a quality work product and organizational ethics, integrity and compliance  Ability to work effectively in a fast paced, team environment  Strong interpersonal skills and the ability to effectively communicate, both verbally and in writing  Demonstrated decision making and problem-solving skills  High attention to detail with the ability to multi-task and meet deadlines with minimal supervision  Proficiency in Word, Excel, PowerPoint and Outlook Total RewardsMajor League Soccer offers a competitive starting base salary of $90,000 - $115,000, based on individual qualifications, market financials, and operational business needs. We are committed to providing a Total Rewards package that attracts, supports, engages, and retains talent. Our benefits package includes comprehensive medical, dental, and vision coverage, a $500 wellness reimbursement, and generous PTO. We also prioritize career and professional development, offering on-the-job training, feedback, and ongoing educational opportunities.We believe in the power of in-person collaboration to fuel creativity, strengthen connections, and cultivate a vibrant workplace. As a result, employees are required to work from an MLS office at least four days a week. We understand the value of balance, so employees also have the flexibility of working remotely on Fridays, along with the option to take up to two additional remote flex days each month.At Major League Soccer, we are proud to be an equal opportunity employer. We value diversity and inclusion and believe that a diverse workforce enhances our ability to compete in the marketplace. We are committed to providing equal employment opportunities to all individuals regardless of race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law.We are dedicated to ensuring that individuals with disabilities are provided reasonable accommodation throughout the job application or interview process, essential job functions, and other benefits and privileges of employment. If you require accommodation, please contact us to request it.Join our team and be part of the Major League Soccer family, where we elevate the game and inspire greatness!\n        ",
    "location": "New York, NY"
  },
  "144": {
    "id": "4066232907",
    "Company": "Amazon",
    "title": "Data Engineer",
    "description": "DescriptionData EngineerCome build the future as a Data Engineer at Amazon, where you will be inspired working along best-in-class inventors and innovators! You will have the opportunity to create meaningful experiences that deliver on the ever-evolving needs of our customers, and your work will impact millions of people around the world.As an Amazon Data Engineer, you will solve unique and complex problems at a rapid pace, utilizing the latest technologies to create solutions that are highly scalable. You will find that there is an unlimited number of opportunities within Amazon, where developing your career across a wide range of teams is highly supported. We are committed to making your work experience as enjoyable as the experiences you\u2019ll be creating for our customers.Apply now and you will be eligible for Amazon Data Engineer positions that are based on your preferred location, team, and more. We\u2019re hiring across Amazon Stores in the United States and Canada.Teams with available positions include, but are not limited to: Consumer Technology: Build new generation features and products for amazon.com, constantly improving the Customer and Seller experience for billions around the globe. Whether building site wide features such as reviews and recommendations, category specific software for the likes of Pharmacy, Electronics, Digital Software and Video Games or seller infrastructure, there are a variety of complex problems to tackle using a range of technologies in the design of your technical solutions. Operations Technology: Shape the future of transportation planning and execution on a global scale, that impacts hundreds of fulfillment centers, thousands of Amazonians, and millions of customers across the world. Your technology will support thousands of operators worldwide to design, build and run the best-in-class Amazon transportation network. We are building intelligent software to make transportation more reliable, faster, and less costly, providing a better and less expensive experience for our customers. Human Resources Technology: Create a seamless experience for millions of Amazonians and/or candidates. Whether supporting technologies for onboarding, time and attendance, compensation, amazon.jobs, or recruiting, you\u2019ll deliver robust feature sets, elegant designs, intuitive user interfaces and systems that make it easy for Amazonians to excel at performing critical business functions.About UsWork/Life BalanceOur team puts a high value on work-life balance. It isn\u2019t about how many hours you spend at home or at work; it\u2019s about the flow you establish that brings energy to both parts of your life. We believe striking the right balance between your personal and professional life is critical to life-long happiness and fulfillment. We offer flexibility in working hours and encourage you to find your own balance between your work and personal lives.Mentorship & Career GrowthOur team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we\u2019re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign opportunities based on what will help each team member develop into a better-rounded contributor.Inclusive Team CultureHere at Amazon, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon\u2019s culture of inclusion is reinforced within our 14 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.Key job responsibilities Design, implement, and support a platform providing secured access to large datasets. Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation. Tune application and query performance using profiling tools and SQL. Analyze and solve problems at their root, stepping back to understand the broader context. Learn and understand a broad range of Amazon\u2019s data resources and know when, how, and which to use and which not to use. Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS. Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets. Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment.Basic Qualifications 3+ years of data engineering experience Proficient in SQL Experience with data modeling, warehousing and building ETL pipelines Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJSPreferred Qualifications Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases) Experience with big data technologies such as: Hadoop, Hive, Spark, EMRAmazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status.Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you\u2019re applying in isn\u2019t listed, please contact your Recruiting Partner.Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.Company - Amazon.com LLC - A03Job ID: A2820515\n        ",
    "location": "Austin, TX"
  },
  "145": {
    "id": "4185055857",
    "Company": "Farmers Insurance",
    "title": "Data Engineer",
    "description": "We are Farmers!We are\u2026 more than just your favorite commercials. \u202fAt Farmers, we strive to deliver peace of mind to our customers by providing protection and comprehensive advice and delivering in the moments of truth. That means having people who can help us meet changing customer and business needs. Farmers high-performance culture is focused on results and the people who achieve them. We hold ourselves and others accountable for sustainably growing the business and each other. We seek solutions, own our actions, and grow through discomfort. We see setbacks as opportunities while continuously asking ourselves how we impact our customers.Farmers is an award winning, equal opportunity employer, committed to the strength of a diverse workforce. We are dedicated to supporting the well-being of our people through our extensive suite of benefits, as well as the well-being of the communities we serve through employee volunteer programs and nonprofit partnerships. Helping others in their time of need isn\u2019t just our business \u2013 it\u2019s our culture!\u202f To learn more about our high-performance culture and open opportunities, check out www.Farmers.com/careers/corporate and be sure to follow us on Instagram, LinkedIn, and TikTok.Workplace: Hybrid ( ), Remote ( )Farmers believes in a culture of collaboration, creativity, and innovation, which thrives when we have the ability to work flexibly in a virtual setting as well as the opportunity to be together in person. Our hybrid work environment combines the best of both worlds with at least three (3) days in office and up to two (2) days virtual for employees who live within fifty (50) miles of a Farmers corporate office. Applicants beyond fifty (50) miles may still be considered.Job SummaryResponsible for acquiring, curating, and publishing data both on prem and in the cloud for analytical or operational uses for basic to moderate scenarios. Ensures the data is in a ready-to-use form that creates a single version of the truth across all data consumers, including business/technology users, reporting and visualization specialists and data scientists with coaching and support. Utilizes skills to translate business analytic requests/requirements into design, development, testing, deployment, and production maintenance tasks. Works with various technologies from big data, relational and non-relational databases, cloud environments, different programming languages and various reporting tools and is familiar with a few but requires training for some.What You'll DoConsults independently, works iteratively, and is viewed as an expert on data projects of a complex nature. Ability to understand design requirement and existing business processes to lead in the design, development, and implementation of data products in partnership with business partners and data scientists.Produces complex data building blocks, data models, and data flows working with business partners and data scientists within personal lines and distribution domains to deliver requests such as dimensional data models, standard and ad hoc reporting, data feeds, dashboard reporting, ad hoc analysis, data profiling etc.Designs and manages highly complex projects while handling multiple complex cross-functional projects.Preps/cleanses data to optimize for downstream reporting using Farmers standards.Functions as an expert in serving as a SME for appropriate data sources and for how best to operationalize solutions working closely with business partners and data scientists.Acts as a conduit between business and IT teams for all aspects of the work. Create Dimensional Data Models, STTM\u2019s and other technical documents for offshore developers and oversee technical delivery of projects by conducting code reviews, documentation reviews etc.Performs other duties as assigned.What You'll Bring3-5 years of related experience required.Possesses strong technical aptitude, preferred. Strong verbal communication and listening skills, preferred. Extensive experience interacting with senior leadership stakeholders around the topics of data access, quality, appropriate use, metric definitions and standardization, preferred. Effectively coaches junior members and delivers constructive feedback, preferred. Practical knowledge of SQL, Dimensional modeling, Query tuning, ETL orchestration ,PowerBI, Python, preferred. Hands-on experience with multiple cloud platforms including AWS, Google, and Azure, preferred. Hand on experience with developing on SSIS and SSAS, preferred. Education RequirementsHigh School Diploma or equivalent required. Bachelor's degree preferred.Physical ActionsThis role, whether performed virtually or in an office setting, will include normal and customary distractions, noise, and interruptions. Sits or stands for extended periods of time, up to a full work shift. Occasionally reaches overhead and below the knees, including bending, twisting, pulling, and stooping. Occasionally moves, lifts, carries, and places objects and supplies weighing 0-10 pounds without assistance. Listens to, interprets, and differentiates auditory information (example others speaking) at normal speaking levels with or without correction. Visually verifies and reads information. Visually locates material, resources and other objects. Ability to continuously operate a computer for extended periods of time, up to a full work shift. Physical dexterity sufficient to use hands, arms, and shoulders repetitively to operate keyboard and other office equipment up to a full work shift.BenefitsFarmers offers a competitive salary commensurate with experience, qualifications and location.CA Only: $106,560 - $169,950CO Only: $99,920 - $146,520 HI/IL/MN/VT Only: $99,920 - $156,750MA Only: $99,920 - $156,750 o MD Only: $99,920 - $156,750NY/DC/Jersey City Only: $99,920 - $169,950Albany County: $106,560 - $146,520WA Only: $99,920 - $178,125 Bonus Opportunity (based on Company and Individual Performance)401(k)MedicalDentalVisionHealth Savings and Flexible Spending AccountsLife InsurancePaid Time OffPaid Parental LeaveTuition AssistanceFor more information, review \u201cWhat we offer\u201d on https://www.farmers.com/careers/corporate/#offerJob Location(s): R_US - United StatesAnticipated application deadline: At Farmers, the recruitment process is designed to ensure that we find the best talent to join our team. As part of this process, we typically close open positions within 8 to 21 days after posting. If you are interested in any of our open positions, we encourage you to submit your application promptly.Farmers will consider for employment all qualified applicants, including those with criminal histories, in accordance with the Los Angeles Fair Chance Initiative for Hiring Ordinance or other applicable law. Pursuant to 18 U.S.C. Section 1033, Farmers is prohibited from employing any individual who has been convicted of any criminal felony involving dishonesty or a breach of trust without prior written consent from the state Department of Insurance.Farmers is an Equal Opportunity Employer and does not discriminate in any employer/employee relations based on race, color, religion, gender, sexual orientation, gender expression, genetic information, national origin, age, disability, marital status, military and veteran's status, or any other basis protected by applicable discrimination laws.Want to learn more about our culture & opportunities? Check out www.Farmers.com/careers/corporate and be sure to follow us on Instagram, LinkedIn, and TikTok.\n",
    "location": "United States"
  },
  "146": {
    "id": "4150329250",
    "Company": "Revelio Labs",
    "title": "Data Engineer - Data Flow",
    "description": "Who We AreRevelio Labs provides workforce intelligence. We absorb and standardize hundreds of millions of public employment records to create the world\u2019s first universal HR database, allowing us to see current workforce composition and trends of any company. Our customers include investors, corporate strategists, HR teams, and governments.What We\u2019re Looking ForIf you\u2019re ambitious, talented, and ready to level up your data engineering skills, this role is for you. As our data feeds evolve from daily to real-time, the engineering challenges are growing. We\u2019re currently in the process of upgrading our core data flow from a Python/Snowflake-based system that runs on AWS to a Go/Kubernetes-based system that runs on our own hardware. The efficiency gains so far are startling. We\u2019re looking for someone with strong skills in Python and SQL who\u2019s ready to take on a new challenge.Experience And SkillsPythonSQLCloud computing1+ years in a data engineering or data science roleLocation:Our offices are based in New York City, but the position can be done remotely.SalaryThe pay range for this position in New York City is $90,000 - $130,000 per year. The salary range for performing this role outside of New York City may differ. Base pay offered may vary depending on job-related knowledge, skills, and experience. Additionally, you may be eligible to participate in our company\u2019s equity program, plus benefits, including medical, dental, vision, retirement, and other. The range above is for the expectations as laid out in the job description, however we are often open to a wide variety of profiles, and recognize that the person we hire may be more senior or have different experience than this job description as posted. If that ends up being the case, the updated salary range will be communicated to you as a candidate.Why You Should Work With UsWe are working on some of the most challenging and cutting-edge data science problems. You will work with a team of PhD data scientists with advanced domain expertise. Our startup is small and growing quickly, with enormous potential for growth.How You Should Reach UsPlease email your resume to recruiting@reveliolabs.com as a PDF file. Please include your GitHub and highlight any projects that you\u2019ve worked on that may be relevant.ApplyPlease find our CPRA Job Applicant Privacy Notice here.\n        ",
    "location": "New York, NY"
  },
  "147": {
    "id": "4185975235",
    "Company": "Kyndryl",
    "title": "Data Analytics Engineer",
    "description": "Who We AreAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward \u2013 always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.The RoleAre you ready to embark on an exhilarating journey as a Data Analytics Engineer? Join Kyndryl and become a driving force behind the transformative power of data! We're seeking an exceptionally talented individual to accelerate the competitive performance of our customers worldwide, establishing us as their unrivaled business and technology consulting partner.As a Data Analytics Engineer, your impact will be monumental. You will unleash the potential of data technology, collaborating with customers to envision and conceptualize strategic applications. By combining your deep understanding of business and technology, you will become a catalyst for success, delivering invaluable insights and recommendations that propel our customers' organizations forward.Prepare to immerse yourself in the world of data strategies and programs, crafting ingenious approaches to collecting, storing, analyzing, and visualizing data from diverse sources. Your technical expertise in a vast array of cutting-edge big data tools will empower you to develop groundbreaking solutions tailored to meet the unique requirements of each customer.But that's not all \u2013 your role as a Data Analytics Engineer at Kyndryl will transcend traditional boundaries. You will lead captivating workshops and engaging consulting engagements, helping customers forge data-driven strategies that reshape their future. Educating our customers about the latest Data Science technologies and frameworks will be second nature to you, enabling them to unlock the full potential of their data resources.Armed with an analytical approach, you will unveil strategies, assessments, recommendations, and comprehensive plans to address complex issues while seamlessly aligning both the technical and functional requirements of IT and the business. Your deep consulting skills and business acumen will be harnessed to analyze customer business issues, formulate hypotheses, and test conclusions, ultimately delivering data solutions that defy expectations.If you have a passion for turning data into a force for change and thrive in a dynamic, fast-paced environment, this is your chance to shine! Unleash your creativity, embrace innovation, and become a master of efficiency as you create solutions that revolutionize customer business and IT landscapes. Join Kyndryl and become part of a remarkable team that turns data into success stories.Your Future at KyndrylAs a Data Analytics Engineer at Kyndryl you will join the Kyndryl Consultant Profession, working with other Kyndryl Consultants, Architects, Project Managers, and cross-functional Technical Subject Matter Experts \u2013 presenting unlimited opportunities with unmatched support through our investment in your learning, training, and career growth.Kyndryl currently does not require employees to be fully vaccinated against COVID-19, however, if you are hired to work at a client, customer, or partner location, you may be required to show proof of vaccination to align with their respective COVID-19 vaccination policies. Those who believe they are eligible may apply for a medical or religious accommodation prior to the start of employment.Who You AreJoin our dynamic team at Kyndryl, where innovation and cutting-edge technology come together to solve complex data challenges. As a Data Analytics Engineer, you'll have the opportunity to collaborate with brilliant minds and shape the future of data solutions across industries. If you're passionate about transforming data into actionable insights, we\u2019d love to have you on board to drive our success and your career forward.Your Role and Responsibilities:Design and develop data pipelines to extract, transform, and load (ETL) data from various sources into data warehouses, lakes, or other data storage systems. Implement and maintain scalable data architectures and systems that support the ingestion, processing, and analysis of large volumes of structured and unstructured data. Work with AWS or Google cloud-based technologies to deploy and scale data solutions. Participate in code reviews and collaborate with team members to improve data pipeline performance, data structure optimization, and overall solution scalability. Document data pipeline processes and BI architecture for future reference, ensuring knowledge sharing and maintaining best practices. Design and develop reports, dashboards and generating insights utilizing multiple data sources in Amazon Quicksight and Google Looker Provide expertise on best practices, guidelines and guide projects to successful conclusions based on business goals. Create and demonstrate Proof of Concepts Work with data, evaluate, build and identify patterns; relate with business process and develop insightsMust be able to perform duties with moderate to low supervision. Own the technical relationship with the customer and operate as their trusted advisor. Drive a well-planned workshop session with the customers to help identify clear scope and requirement. Contribute to the growth of the organization by hiring, coaching, and mentoring others. Develop areas of depth in technical domains relevant to your interests and your customer's outcomes. Requirements:10+ years of experience in designing, building and implementing data solutions. Proficiency in developing pipelines using AWS and GCP services (AWS Glue, Lambda, Kinesis, databricks, Google dataflow, dataproc, pub/sub etc). Proficiency in Python and core librariesStrong experience with SQL and data modeling in relational databases Experience with cloud based data services (AWS Redshift, RDS, DynamoDB, BigQuery). Knowledge of data warehousing concepts and solutions (e.g., Snowflake, Amazon Redshift, Google BigQuery). Knowledge with data lakes and unstructured data formats. Expertise in data integration and working with APIs, flat files, and streaming data. Hands-on experience in Amazon Quicksight and/or Google LookerExperience in the design and development of dashboards, reports, business workflow using various data sources (on-prem & Cloud) Experience and exposure towards Devops and versioning toolsExperience in deployment patternsGood documentation, troubleshooting skills and data profiling knowledgeAbility to manage multiple priorities, and assess and adjust quickly to changing prioritiesRecommended ExperienceAssociate or Professional Certification in AWS or GCPExperience in Business Development, Project Leadership, and collaborating with teams on presales activitiesFamiliarity with daily scrums and comfortable working closely with leads, architects, scrum master and product owner to provide timely status updatesThe compensation range for the position in the U.S. is $107,400 to $204,000 based on a full-time schedule.Your actual compensation may vary depending on your geography, job-related skills and experience. For part time roles, the compensation will be adjusted appropriately. The pay or salary range will not be below any applicable state, city or local minimum wage requirement.There is a different applicable compensation range for the following work locations:California:$118,200 to $244,920Colorado: $107,400 to $204,000New York City: $128,880 to $244,920Washington: $118,200 to $224,520Washington DC:$118,200 to $224,520This position will be eligible for Kyndryl\u2019s discretionary annual bonus program, based on performance and subject to the terms of Kyndryl\u2019s applicable plans. You may also receive a comprehensive benefits package which includes medical and dental coverage, disability, retirement benefits, paid leave, and paid time off. Note: If this is a sales commission eligible role, you will be eligible to participate in a sales commission plan in lieu of the annual discretionary bonus program.Applications will be accepted on a rolling basis.EEO Language All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.Know Your Rights: Workplace Discrimination is IllegalPay Transparency Nondiscrimination ProvisionBeing YouDiversity is a whole lot more than what we look like or where we come from, it\u2019s how we think and who we are. We welcome people of all cultures, backgrounds, and experiences. But we\u2019re not doing it single-handily: Our Kyndryl Inclusion Networks are only one of many ways we create a workplace where all Kyndryls can find and provide support and advice. This dedication to welcoming everyone into our company means that Kyndryl gives you \u2013 and everyone next to you \u2013 the ability to bring your whole self to work, individually and collectively, and support the activation of our equitable culture. That\u2019s the Kyndryl Way.What You Can ExpectWith state-of-the-art resources and Fortune 100 clients, every day is an opportunity to innovate, build new capabilities, new relationships, new processes, and new value. Kyndryl cares about your well-being and prides itself on offering benefits that give you choice, reflect the diversity of our employees and support you and your family through the moments that matter \u2013 wherever you are in your life journey. Our employee learning programs give you access to the best learning in the industry to receive certifications, including Microsoft, Google, Amazon, Skillsoft, and many more. Through our company-wide volunteering and giving platform, you can donate, start fundraisers, volunteer, and search over 2 million non-profit organizations. At Kyndryl, we invest heavily in you, we want you to succeed so that together, we will all succeed.Get Referred!If you know someone that works at Kyndryl, when asked \u2018How Did You Hear About Us\u2019 during the application process, select \u2018Employee Referral\u2019 and enter your contact's Kyndryl email address.\n        ",
    "location": "Massachusetts, United States"
  },
  "148": {
    "id": "4181814701",
    "Company": "NielsenIQ",
    "title": "Data Engineer (Remote only in US)",
    "description": "Company DescriptionMRI-Simmons Research is a leading consumer insights company? We are an established organization, with a great brand, that is aggressively reinventing itself through investments in technology and people! The innovative private equity firm, Symphony Technology Partners, also backs us.Job DescriptionAs Data Engineer, you will have following key accountabilities: Design, implement and maintain scalable pipelines and architecture to collect, process, and store data from various sourcesUnit test and document solutions that meet product quality standards prior to release to QAIdentify and resolve performance bottlenecks in pipelines due to data, queries and processing workflows to ensure efficient and timely data deliveryImplement data quality checks and validations processes to ensure accuracy, completeness and consistency of data deliveryWork with Data Architect and implement best practices for data governance, quality and securityCollaborate with cross-functional teams to identify and address data needsEnsure technology solutions support the needs of the customer and/or organizationDefine and document technical requirementsQualificationsNow that we have introduced you to the position as Data Engineer, what skills, qualifications and experience should you, have? Strong PL/SQL, SQL development skillsProficient in multiple languages used in data engineering such as Python, JavaMinimum 3-5 years of experience in Data engineering working with Oracle and MS SQLExperience with data warehousing concepts and technologies including cloud-based services (e.g. Snowflake)Experience with cloud platforms like Azure and knowledge of infrastructureExperience with data orchestration tools (e.g. Azure Data Factory, DataBricks workflows)Understanding of data privacy regulations and best practicesExperience working with remote teamsExperience working on a team with a CI/CD processFamiliarity using tools like Git, JiraGreat problem-solving abilitiesGreat work ethicsBachelor's degree in Computer Science or Computer EngineeringAdditional InformationThis role has a market-competitive salary with an anticipated base compensation of the following range: $64,000 to $120,000. Actual salaries will vary depending on a candidate\u2019s experience, qualifications, skills, and location. This role might also be eligible for a Sales-based incentive or performance-based bonus. Other benefits include a flexible working environment, comprehensive health insurance, industry-leading parental leave, life insurance, education support, and more.Our BenefitsFlexible working environmentVolunteer time offLinkedIn LearningEmployee-Assistance-Program (EAP)About NIQNIQ is the world\u2019s leading consumer intelligence company, delivering the most complete understanding of consumer buying behavior and revealing new pathways to growth. In 2023, NIQ combined with GfK, bringing together the two industry leaders with unparalleled global reach. With a holistic retail read and the most comprehensive consumer insights\u2014delivered with advanced analytics through state-of-the-art platforms\u2014NIQ delivers the Full View\u2122. NIQ is an Advent International portfolio company with operations in 100+ markets, covering more than 90% of the world\u2019s population.For more information, visit NIQ.comWant to keep up with our latest updates?Follow us on: LinkedIn | Instagram | Twitter | FacebookOur commitment to Diversity, Equity, and InclusionNIQ is committed to reflecting the diversity of the clients, communities, and markets we measure within our own workforce. We exist to count everyone and are on a mission to systematically embed inclusion and diversity into all aspects of our workforce, measurement, and products. We enthusiastically invite candidates who share that mission to join us. We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, protected veteran status or any other protected class. Our global non-discrimination policy covers these protected classes in every market in which we do business worldwide. Learn more about how we are driving diversity and inclusion in everything we do by visiting the NIQ News Center: https://nielseniq.com/global/en/news-center/diversity-inclusion\n        ",
    "location": "Springfield, Illinois Metropolitan Area"
  },
  "149": {
    "id": "4182650954",
    "Company": "Insight Global",
    "title": "Junior Data Engineer",
    "description": "12 month contract to hire3 days onsite, 2 days remoteJOB DESCRIPTIONThe Junior Data Engineer will be an integral part of the product development cycle, responsible for creating features and developing tables and hydration procedures in a Databricks environment. This role requires a deep understanding of SQL, as it is the core language for interacting with data. They will be taking features to translate them into technical development by creating SQL tables, stored procedures, orchestration of these products. The engineer will also work with Python and PySpark to build and manage data transformations. Additionally, they will learn about the domain context to effectively build data transformations. Experience with Airflow for pipeline and orchestration, as well as AWS and streaming technologies like Kafka, is a plus.REQUIRED SKILLS AND EXPERIENCEExpertise in SQL, with the ability to write complex queries Experience with Databricks and Unity Catalog 1+ years of professional experience Pyspark and python\n",
    "location": "Greater St. Louis"
  },
  "150": {
    "id": "4175130795",
    "Company": "Medline Industries, LP",
    "title": "Data Engineer",
    "description": "The candidate should have a robust background in data modeling and ETL (Extract, Transform, Load) processes. The candidate should have essential hands-on experience with Advanced SQL and python. Proficiency in building Data Lake and pipelines using Azure is highly desirable. Additionally, knowledge and experience in Finance domain , MS Fabric Implementation and holding Azure certifications, are considered a plus.Development ResponsibilitiesResponsible for working with Finance and IT teams to design and implement data solutions. This includes responsibility for the method and processes used to translate business needs into functional and technical specifications.Design, develop, and maintain robust data models and ETL pipelines.Develop logical data models and physical database designs applied across multiple computing environments.Responsible for building new Data Lake in Azure, expanding and optimizing our data platform and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams.Responsible for designing and developing solutions in Azure big data frameworks/tools: Azure Data Lake, Azure Data Factory, FabricProcess ImprovementEnsure data quality and integrity across various data sources and systems.Maintain quality of data in the warehouse, ensuring integrity of data in the warehouse, correcting any data problemsRelevant Work Experience3-5 years of experience in software engineering, with a focus on data engineering.Strong experience in data modeling and ETL processes.Advanced Hands-on SQL and Python knowledge and experience working with relational databases for data querying and retrieval.Knowledge and experience of building data lake and enterprise analytics solutions with Microsoft Azure, including Azure Data Factory, Azure SQL Database and other Azure services.\n",
    "location": "Northfield, IL"
  },
  "151": {
    "id": "4190656582",
    "Company": "Brink\u2019s Inc",
    "title": "Data Engineer",
    "description": "Brinks Texas License #C00550About Brink'sThe Brink\u2019s Company (NYSE:BCO) is a leading global provider of cash and valuables management, digital retail solutions, and ATM managed services. Our customers include financial institutions, retailers, government agencies, mints, jewelers, and other commercial operations. Our network of operations in 52 countries serves customers in more than 100 countries.Brink\u2019s has been a trusted partner in securing commerce for more than 165 years. Together, every Brink\u2019s Team Member is committed to providing the highest levels of service and support to our customers. We take pride in our work, and we share a passion about our future. Learn why so many people have made the choice to join our team \u2013 and stay here.We believe that our team should be reflective of the customers we serve every day around the world. We believe in building partnerships that secure commerce and doing that requires fostering an inclusive culture that values people with diverse backgrounds, ideas, and perspectives. We build a sense of belonging, so all employees feel respected, safe, and valued, and we provide equal opportunity to participate and grow.Job DescriptionWe are seeking a Data Analyst/Engineer with strong data modeling and communication skills to collaborate closely with Process Engineers and Analysts. This role involves translating current and future system requirements into robust data models that will serve as the foundation for our target systems. Ideal candidates have 5\u20138 years of relevant experience, thrive in cross-functional settings, and possess the ability to convey technical concepts to both technical and non-technical stakeholders.Key Responsibilities Data Modeling & Architecture:Gather system requirements from Process Engineers and Analysts to design, implement, and refine logical and physical data models.Ensure models align with enterprise standards, performance goals, and data governance best practices. Collaboration & Communication:Work closely with cross-functional teams, including business stakeholders, to translate functional needs into actionable data solutions.Serve as the key liaison between business and technical teams, clearly articulating data requirements, project updates, and recommendations. Data Integration & Quality:Define data pipelines and integration strategies across diverse systems (on-premise and cloud), ensuring accurate data flow and quality.Establish and maintain data validation and transformation processes to guarantee data consistency and reliability. Analysis & Reporting:Perform data exploration, analysis, and reporting to drive insights and support decision-making.Develop best practices for data storage, retrieval, and security, including monitoring and alerting processes. Documentation & Standards:Create and maintain comprehensive technical documentation, including data models, data dictionaries, and data flow diagrams.Contribute to or lead the creation and enforcement of data governance frameworks, policies, and procedures.Required Qualifications Experience: 5\u20138 years of professional experience in data modeling, data engineering, or related fields. Technical Skills:Strong proficiency with data modeling tools and concepts (e.g., ER diagrams, dimensional modeling).Familiarity with SQL, Python, or similar languages for data manipulation and analysis.Solid understanding of relational databases, NoSQL databases, data warehousing, and ETL/ELT best practices.Soft SkillsExcellent verbal and written communication; ability to explain complex data concepts in clear, concise terms.Proven track record of partnering effectively with cross-functional teams.Strong problem-solving, organizational, and time-management abilities.Education Bachelor\u2019s degree in Computer Science, Information Systems, Engineering, or a related technical field (or equivalent professional experience).Preferred Qualifications Exposure to cloud-based data platforms (e.g., AWS, Azure and ServiceNow). Experience working with or implementing data governance practices. Advanced proficiency in data analytics tools (e.g., Power BI) or statistical programming Prior experience in a process-driven or manufacturing environment a plus.What\u2019s Next?Thank you for considering applying for a job at Brink\u2019s. To be considered for this position, you must complete the entire application process, which includes answering all prescreening questions and providing your eSignature.Upon completion of the application process, you will receive an email confirming that we have received your application. We will review all candidates and notify you of your status should we deem you fit for a job. Thank you again for your interest in a career at Brink\u2019s. For more information about future career opportunities, join our talent network, like our Facebook page or Follow us on X.Brink\u2019s is an equal opportunity/affirmative action employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, marital status, protected veteran status, sexual orientation, gender identity, genetic information, or history or any other characteristic protected by law. Brink\u2019s is also committed to providing a drug-free workplace. We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state, or local protected class.\n",
    "location": "Coppell, TX"
  },
  "152": {
    "id": "4148125121",
    "Company": "ProAg",
    "title": "Data Engineer",
    "description": "Grow with UsThis position is in our Maple Grove, MN office.ProAg has an exciting opportunity for a Data Engineer to join our data team. We are looking for individuals who want to embrace the advantages of data and dedicate importance to ensure our data structures are the easiest and most efficient in the industry. Bring your passion for data to help data solutions to help as we serve our farmers, agents, and re-insurers.You will be primarily responsible for the analysis, design, development, testing, implementation and maintenance of new and existing data structures. Responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. Supports our software developers, data architects, data analysts and data scientists on data initiatives.In This Exciting Opportunity You WillBuild the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources.Maintain optimal data pipeline architectureIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build analytics tools that utilize the data to provide actionable insights into operational efficiency and other key business performance metricsWork with the stakeholders and business teams to assist with data-related technical issues and supportWhat You\u2019ll Bring2-3 years of relevant and progressive professional experience in data analysis, design and development.Bachelor\u2019s degree in a related field or equivalent education and/or experience.Ability to work in a dynamic problem-solving environment and synthesize strategy, plans, and solutions.Demonstrated ability to deliver in a complex business environment.What We RepresentPart of something bigger: We offer a career with purpose as you support the farmers and ranchers who create food, fuel and fiber for the world.Personal connections: We are built on strong relationships and appreciation of your individuality.A team who cares: We look out for each other personally and professionally because we care about each other.Innovators by trade: We\u2019re committed to a brighter tomorrow for our team members and for agriculture.The best of both worlds: We combine personal connections with powerful resources, thanks to our culture and the backing of Tokio Marine HCC.The salary for this position is: $95,000 - $110,00 annually.While our nation weathers economic storms, ProAg, a member of the Tokio Marine HCC group of companies, is positioned as a financially strong and well-capitalized insurer. We\u2019re known for our quick response and fast, accurate claims settlement. We understand how important this is because many of us are farmers and ranchers ourselves. With more than 90 years of service to our agents & insureds, we stand committed to continuing the principles that ProAg was founded on: Integrity, Loyalty and Customer Service.The Tokio Marine HCC Group of Companies offers a competitive salary and employee benefit package. We are a successful, dynamic organization experiencing rapid growth and are seeking energetic and confident individuals to join our team of professionals. The Tokio Marine HCC Group of Companies is an equal-opportunity employer. Please visit www.tokiomarinehcc.com for more information about our companies.\n",
    "location": "Maple Grove, MN"
  },
  "153": {
    "id": "4076508695",
    "Company": "TikTok",
    "title": "Data Engineer, Data Application",
    "description": "ResponsibilitiesTikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Singapore, Jakarta, Seoul and Tokyo.Why Join UsCreation is the core of TikTok's purpose. Our platform is built to help imaginations thrive. This is doubly true of the teams that make TikTok possible.Together, we inspire creativity and bring joy - a mission we all believe in and aim towards achieving every day.To us, every challenge, no matter how difficult, is an opportunity; to learn, to innovate, and to grow as one team. Status quo? Never. Courage? Always.At TikTok, we create together and grow together. That's how we drive impact - for ourselves, our company, and the communities we serve.Join us.TikTok's immersive experience, global presence, and high engagement makes it the ideal marketing destination for business, big and small, to showcase their unique brand identity, connect with their consumers, and build strong lasting relationships over time. The Ads Data Team builds and manages the petabyte scale data infrastructure, batch/realtime pipelines and services to support Tiktok's global Ads business. We are committed to building a robust data foundation and scalable data applications, unblocking the full potential of advertising data to optimize advertiser experience, boost business growth, empower strategy execution.We are looking for passionate Data Engineers that have strong problem solving skills to join forces with talented cross functional partners (business operation, data science, engineering and product management) to solve some of the most interesting data challenges with efficiency and quality. In this role, you will contribute to the company's core business across innovative advertising products, campaign management and measurement solutions. You will see a direct impact from your day-to-day work to customer satisfaction and company growth.Responsibilities: Work closely with Product Managers, Data Scientists/Analysts, and Software/Machine Learning Engineers and other stakeholders to understand data requirements and deliver data solutions that meet business needs. Evaluate, implement and maintain data infrastructure tools and technologies to support efficient data processing, storage and query. Design, build and optimize scalable data pipelines to ingest, process and transform large volumes of data. Design and implement robust data models and visualization to support complex analytical queries and reporting requirements. Ensure the data integrity, accuracy and consistency of data by implementing data quality checks, validation processes and monitoring mechanisms. Continously optimize data pipelines, queries and processes to improve performance, reduce latency and enhance scalability. Provide rapid response to SLA oncall support to business critical data pipelines. Create and maintain good documentation for data assets and promote best practices for data governance within the data user community.QualificationsQualifications: Bachelor's degree in Computer Science, Engineering, or a related field. Proven 1~3 years' experience as a Data Engineer or similar role in supporting data-centric business. Strong knowledge of SQL and experience working with relational and non-relational databases. Proficiency in programming languages such as Python, Java, Go etc. Solid understanding of data modeling and data warehousing concepts, data integration and ETL/ELT techniques. Effective communication skills and ability to collaborate effectively with cross-functional teams. Excellent problem-solving skills, attention to detail, and ability to thrive in a fast-paced environment.Preferred Qualifications: Experience with big data technologies(e.g. Apache Hadoop, Spark, Kafka, Flink) and working with terabyte to petabyte scale data. Experience with cloud data warehouses(eg. Snowflake, Databricks, BigQuery) and modern business intelligence/data stack. Experience with data governance, data privacy and compliance. Experience in the advertising, e-commerce or gaming industry.TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.TikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at https://shorturl.at/cdpT2Job Information\u3010For Pay Transparency\u3011Compensation Description (Annually)The base salary range for this position in the selected city is $129960 - $194750 annually.Compensation may vary outside of this range depending on a number of factors, including a candidate\u2019s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.Benefits may vary depending on the nature of employment and the country work location. Employees have day one access to medical, dental, and vision insurance, a 401(k) savings plan with company match, paid parental leave, short-term and long-term disability coverage, life insurance, wellbeing benefits, among others. Employees also receive 10 paid holidays per year, 10 paid sick days per year and 17 days of Paid Personal Time (prorated upon hire with increasing accruals by tenure).The Company reserves the right to modify or change these benefits programs at any time, with or without notice.For Los Angeles County (unincorporated) Candidates:Qualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state, and local laws including the Los Angeles County Fair Chance Ordinance for Employers and the California Fair Chance Act. Our company believes that criminal history may have a direct, adverse and negative relationship on the following job duties, potentially resulting in the withdrawal of the conditional offer of employment: Interacting and occasionally having unsupervised contact with internal/external clients and/or colleagues; Appropriately handling and managing confidential information including proprietary and trade secret information and access to information technology systems; and Exercising sound judgment.\n",
    "location": "Seattle, WA"
  },
  "154": {
    "id": "4161434640",
    "Company": "BeaconFire Inc.",
    "title": "Data Engineer",
    "description": "About the jobBeaconFire is based in Central NJ and specializes in software development, web development, and business intelligence. We are looking for candidates with a strong background in software engineering or computer science for a Data Engineer Developer position.Preferred Qualifications:Passion for data and a deep desire to learn. Bachelor\u2019s Degree in Computer Science/Information Technology, Data Analytics/Data Science, or related discipline. Intermediate Python. Experience in data processing is a plus. (Numpy, Pandas, etc) Strong written and verbal communication skills. Ability to work both independently and as part of a team. Job Responsibilities:Collaborate with the analytics team to find reliable data solutions to meet business needs. Design and implement scalable ETL or ELT processes to support the business demand for data. Perform data extraction, manipulation, and production from database tables. Build utilities, user-defined functions, and frameworks to better enable data flow patterns. Build and incorporate automated unit tests, and participate in integration testing efforts. Work with teams to resolve operational & performance issues. Work with architecture/engineering leads and other teams to ensure quality solutions are implemented, and engineering best practices are defined and adhered to. Compensation: $65,000.00 to $80,000.00 /yearBeaconFire is an E-verified company that provides equal employment opportunities (visa sponsorship provided).\n",
    "location": "New Jersey, United States"
  },
  "155": {
    "id": "4188601355",
    "Company": "Innovative Defense Technologies (IDT)",
    "title": "Data Engineer",
    "description": "DescriptionBackground Information:Innovative Defense Technologies (IDT), provider of automated software testing, data analysis, and cybersecurity solutions for complex, mission-critical systems in the US Department of Defense (DOD), is seeking a Data Engineer to be based in our Arlington, VA office.Overview:The Data Engineer position offers the opportunity to join our growing team of data scientists and analytics experts. The hire will be responsible for designing and implementing various data pipeline architectures across several projects, as well as optimizing data flow and collection for cross functional teams. The candidate must be experienced in the state-of-the-art data management technologies to support differing data requirements and objectives. Individuals must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.All applicants must be able to obtain/maintain an active U.S. Security Clearance.Responsibilities Include: Design, implement, deploy, and maintain optimal data pipeline and data management architectures. Assemble large, complex data sets that meet functional / non-functional project requirementsBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, streaming and \u2018big data\u2019 technologiesImplement data pipelines to ingest data to the platform, standardize and transform the dataSupport the development of analytics tools that utilize the data pipeline to provide actionable insights into the data and optimization of objectives. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. Design and architect solutions with Big Data technologies (e.g Hadoop, Hive, Spark, Kafka)Design and implement systems that run at scale leveraging containerized deploymentsDesign, build, and scale data pipelines across a variety of source systems and streams (internal, third-party, as well as cloud-based), distributed/elastic environments, and downstream applications and/or self-service solutionsMinimum Required Qualifications:Bachelor\u2019s Degree in Computer Science, Computer Engineering, Informatics, Information Systems, or another quantitative fieldMinimum 5 years of experience in a Data Engineer roleRequired Skills:Experience with big data tools: Hadoop, Spark, etc. Experience with relational SQL and NoSQL databases, including PostgresExperience with AWS cloud or remote services: EC2, EMR, RDS, RedshiftExperience with stream-processing systems: Kafka, Storm, Spark-Streaming, etc. Experience with object-oriented/object function scripting languages: Python, Julia, Java, C++, Scala, etc. Experience with data encryption/security features applied to data-in-transitAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databasesExperience building and optimizing \u2018big data\u2019 data pipelines, architectures and data setsExperience performing root cause analysis on internal and external data and processes to answer specific questions and identify opportunities for improvementExperience with development, management, and manipulation of large, complex datasetsExperience with database & ETL technologiesDemonstrated knowledge of data management competencies and implementationUnderstands the Big Data related problems and requirements to identify the correct technical approach. Core understanding of Big Data principles and architectural patternsWorking knowledge of message queuing, stream processing, and highly scalable \u2018big data\u2019 data storesExperience supporting and working with cross-functional teams in a dynamic environmentPreferred Skills:Experience with containerized deployment technologies (Kubernetes, Openshift, etc.)Experience with instantiating and configuring Virtual Machines (VMware, VirtualBox, etc.)Experience with Machine Learning algorithms and applications interfacing with data management solutionsCompetencies:Solid analytical abilities, coupled with a strong sense of ownership, urgency, and driveAttention to detailInitiative, creativity, reliability, teamworkAbility to adapt to and thrive in a fast-paced environmentEEO Statement:IDT is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, sexual orientation, gender identity, national origin, disability, protected Veteran status, or any other basis protected by federal, state, or local law.\n        ",
    "location": "Arlington, VA"
  },
  "156": {
    "id": "4181439353",
    "Company": "Slalom",
    "title": "Data Engineer (Databricks, AWS)",
    "description": "This is a hybrid role (Consultant or Senior Consultant level; not eligible for 100% remote), all employees must live within a commutable distance from our Miami or Tampa Bay office. We will also consider candidates in West Palm Beach.Who You\u2019ll Work WithAs a modern technology company, we've never met a technical challenge we didn't like. We enable our clients to learn from their data, create incredible digital experiences, and make the most of new technologies. We blend design, engineering, and analytics expertise to build the future. We surround our technologists with interesting challenges, innovative minds and emerging technologies.What You\u2019ll DoLead the implementation of cutting-edge data solutions based modern data platforms like Databricks, Snowflake and hyperscalers like AWS, Azure, GCP Spearhead the design and implementation of robust data architectures to solve complex business challenges, aligning with client objectives and industry best practices Collaborate closely with cross-functional teams in an agile framework to develop and deploy scalable data engineering solutions Continuously learn and master emerging data technologies and frameworks to remain at the forefront of the field Act as a mentor and knowledge source within the company and for clients, sharing insights and project learnings regularlyWhat You\u2019ll BringYou have over 2 years of demonstrated expertise in data architecture and engineering, specializing in constructing data-focused solutions with major cloud providers (e.g., AWS, Azure, GCP) and data platforms (e.g., Databricks or Snowflake).Strong hands-on programming skills, proficiency in Python (preferred), SQL (preferred) or Java, with a passion for coding and a commitment to delivering high-quality data solutions. Knowledge in infrastructure-as-code is beneficial.About UsSlalom is a purpose-led, global business and technology consulting company. From strategy to implementation, our approach is fiercely human. In six countries and 43 markets, we deeply understand our customers\u2014and their customers\u2014to deliver practical, end-to-end solutions that drive meaningful impact. Backed by close partnerships with over 400 leading technology providers, our 13,000+ strong team helps people and organizations dream bigger, move faster, and build better tomorrows for all. We\u2019re honored to be consistently recognized as a great place to work, including being one of Fortune\u2019s 100 Best Companies to Work For seven years running. Learn more at slalom.com.Compensation And BenefitsSlalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud to invest in benefits that include\u202fmeaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer yearly $350 reimbursement account for any well-being-related expenses, as well as discounted home, auto, and pet insurance.EEO and AccommodationsSlalom is an equal opportunity employer and is committed to inclusion, diversity, and equity in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veterans\u2019 status, or any other characteristic protected by federal, state, or local laws. Slalom will also consider qualified applications with criminal histories, consistent with legal requirements. Slalom welcomes and encourages applications from individuals with disabilities. Reasonable accommodations are available for candidates during all aspects of the selection process. Please advise the talent acquisition team if you require accommodations during the interview process.\n",
    "location": "Miami, FL"
  },
  "157": {
    "id": "4150692905",
    "Company": "Baker Tilly US",
    "title": "Associate Data Engineer",
    "description": "OverviewBaker Tilly is a leading advisory, tax and assurance firm, providing clients with a genuine coast-to-coast and global advantage in major regions of the U.S. and in many of the world\u2019s leading financial centers \u2013 New York, London, San Francisco, Los Angeles, Chicago and Boston. Baker Tilly Advisory Group, LP and Baker Tilly US, LLP (Baker Tilly) provide professional services through an alternative practice structure in accordance with the AICPA Code of Professional Conduct and applicable laws, regulations and professional standards. Baker Tilly US, LLP is a licensed independent CPA firm that provides attest services to its clients. Baker Tilly Advisory Group, LP and its subsidiary entities provide tax and business advisory services to their clients. Baker Tilly Advisory Group, LP and its subsidiary entities are not licensed CPA firms.Baker Tilly Advisory Group, LP and Baker Tilly US, LLP, trading as Baker Tilly, are independent members of Baker Tilly International, a worldwide network of independent accounting and business advisory firms in 141 territories, with 43,000 professionals and a combined worldwide revenue of $5.2 billion. Visit bakertilly.com or join the conversation on LinkedIn, Facebook and Instagram.Please discuss the work location status with your Baker Tilly talent acquisition professional to understand the requirements for an opportunity you are exploring.Baker Tilly is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status, gender identity, sexual orientation, or any other legally protected basis, in accordance with applicable federal, state or local law.Any unsolicited resumes submitted through our website or to Baker Tilly Advisory Group, LP, employee e-mail accounts are considered property of Baker Tilly Advisory Group, LP, and are not subject to payment of agency fees. In order to be an authorized recruitment agency (\"search firm\") for Baker Tilly Advisory Group, LP, there must be a formal written agreement in place and the agency must be invited, by Baker Tilly's Talent Attraction team, to submit candidates for review via our applicant tracking system.ResponsibilitiesJob Description:Baker Tilly has an incredible career opportunity for an Associate Data Engineer to join our growing Enterprise Technology team.The Associate Data Engineer role will be focused specifically on Data Warehousing, Data Management, BI and Data Analytics. All supporting the need to define the businesses strategy and bring light and understanding to the vast amounts of data that Baker Tilly utilizes.Assemble large, complex data sets that meet functional / non-functional business requirements.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure technologiesIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etcUtilize development best practices including technical design reviews, implementing test plans, monitoring/alerting, peer code reviews, and documentationWork with our developers to implement data solutions in SQL Server (both on-premises and Azure instances)Work with the team to develop our Databricks data lakehouse environment for firmwide usage within applications, data integrations, and reporting solutions.QualificationsStrong understanding of data modeling, algorithms, and data transformation techniques.Well versed in BI and data analytics, SQL, Python, R, the MS Stack, Azure and other cloud servicesDatabricks Lakehouse Platform and Boomi MDH skills are preferred Experience developing, deploying and supporting high-quality, fault-tolerant data pipelinesHave hands on experience in Microsoft business intelligence technologies.Exhibit responsibility and accountability toward quality completion of projects and consistently hitting project timelines.Outstanding customer service skills following proper business requirements and human resources expectationsDisciplined to be able to work in a variety of business environments.Maintained a Bachelor\u2019s degree in Computer Science, Engineering, Math, Information Technology, or other related discipline or 4 + years of commensurate experience.\n",
    "location": "Greater Houston"
  },
  "158": {
    "id": "4138409641",
    "Company": "Booz Allen Hamilton",
    "title": "Data Engineer",
    "description": "Job Number: R0213628Data EngineerThe Opportunity: Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there\u2019s more structured and unstructured data available today than ever before. As a data engineer, you know that organizing data can yield pivotal insights when it\u2019s gathered from disparate sources. We need an experienced data engineer like you to help our clients find answers in their data to impact important missions.As a data engineer, you\u2019ll implement data engineering activities on some of the most mission-driven projects in the industry. You\u2019ll deploy and develop pipelines and platforms that organize and make disparate data meaningful.Here, you\u2019ll work with a multi-disciplinary team of data analysts and scientists, electrical engineers, test engineers, software developers, and data consumers in a fast-paced, agile environment. You\u2019ll use your experience in analytical exploration and data examination while you help assess, design, build, and maintain scalable data platforms for your clients. You will help develop and maintain database systems, continually seek opportunity to optimize the organization's data ecosystem, and build custom ETL and ELT workflows as necessary to address critical analytical questions.What You'll Work On:Apply your skills and data analytics experience by simplifying technical requirements and trends, based on audience.Utilize your technical expertise and problem-solving skills to build and maintain robust data infrastructure.Design, construct, install, test, and maintain highly scalable data management systems, ensuring they meet business requirements and industry practices.Employ a variety of languages and tools to marry systems together or try to hunt down opportunities to acquire new data from other systems.Interpret and analyze data from various source systems to support data integration and data reporting needs.Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, or re-designing infrastructure for greater scalability.Develop analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.Create data tools for analytics and data scientist team members that assist them in building and optimizing our data products.Grow your communication and technical skills by merging big data to create data-centric solutions across the testing and evaluation industry.Join us. The world can't wait.You Have:2+ years of experience using programming languages such as Python2+ years of experience working with databases and data modeling and developing and maintaining scalable data stores that supply big data in forms needed for business analysisExperience with SQL and relational and NoSQL databases, and Python and SQL-based transformation frameworksExperience in data modeling techniques such as normalized vs. denormalized models and various schemasExperience developing scalable ETL or ELT workflows for reporting and analyticsExperience with a public cloud such as AWS, Microsoft Azure, or Google CloudKnowledge of data cataloging, metadata management, and lineage trackingAbility to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor, and operate data platformsSecret clearanceBachelor's degreeNice If You Have:\u202fExperience with the data versioning and change management processesExperience in Developmental or Operational Test and EvaluationExperience designing, building, and maintaining scalable data models that support analytical and operational use casesExperience with UNIX and Linux, including basic commands and Shell scripting, and AWS ETL services such as Glue or LambdaExperience with data warehousing and relational database solutions using AWS RDS, AWS Redshift, or MySQLExperience in building self-service data platforms for analysts and business usersExperience with version control systems such as GitExperience with Agile engineering practicesTop Secret clearanceCDMP Certification or equivalent data governance and management certification\u202fClearance:Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Secret clearance is required.CompensationAt Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen\u2019s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.Salary at Booz Allen is determined by various factors, including but not limited to location, the individual\u2019s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $60,400.00 to $137,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen\u2019s total compensation package for employees. This posting will close within 90 days from the Posting Date.Identity StatementAs part of the application process, you are expected to be on camera during interviews and assessments. We reserve the right to take your picture to verify your identity and prevent fraud.Work ModelOur people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.If this position is listed as remote or hybrid, you\u2019ll periodically work from a Booz Allen or client site facility.If this position is listed as onsite, you\u2019ll work with colleagues and clients in person, as needed for the specific role.Commitment to Non-DiscriminationAll qualified applicants will receive consideration for employment without regard to disability, status as a protected veteran or any other status protected by applicable federal, state, local, or international law.\n        ",
    "location": "Kirtland, NM"
  },
  "159": {
    "id": "4179101149",
    "Company": "The Bank of Missouri",
    "title": "Data Engineer",
    "description": "Working at The Bank of MissouriAt The Bank of Missouri we know it takes great people to support the communities we serve! We are passionate about caring for people and communities, and know how to recognize and reward our employees for their talent and contributions. When you work at The Bank of Missouri, you not only get to help others, but you also get the resources, opportunities and support to grow your own career.About our Company:Staying in business for more than 130 years is hard. Really hard. But, thanks to our great customers in the communities we serve and our entire team, we\u2019ve grown from one bank, once known as The Bank of Perryville, to now 29 locations in 22 communities across Missouri.If you think about it, since 1891 when the bank was founded, we\u2019ve endured many events: the Great Depression, two World Wars and the Financial Crisis of 2007-2008. We are a growing, $.2.8 billion institution with plans to further expand as opportunities present themselves. With our roots firmly planted in Perryville, Missouri, we continue to grow; evolving and expanding with our amazing team, customers and communities.Those principles are the reason we are still in business today: caring for people and communities is our foundation. With every financial decision we make, we bear those values in mind, every time. We are proud, but above all, very privileged to say, \u201cWe\u2019ve been community banking since 1891.\u201d We look forward to the bright future which lies ahead for the shareholders, employees, customers and communities served by The Bank of Missouri.Here are some of the great benefits you will enjoy as a member of our team:Competitive SalaryPersonal paid time off, as eligible and paid holidays401K and Employee Stock Option Plan, as eligibleGenerous medical, dental, vision, life and disability insuranceFitness reimbursementLearning, development and growth opportunitiesAbout the Data Engineer:The Data Engineer is a key contributor to The Bank of Missouri's Data Governance team, focusing on ensuring data quality and establishing repeatable processes to build trust in our data assets. The Data Engineer collaborates with various teams to transform business requirements into technical solutions using technologies like Informatica IDMC, Snowflake, Tableau, SQL, and Python. This role supports data-driven decision-making across the organization by assisting in the design, development, and implementation of accurate and efficient data solutions aligned with evolving business priorities. By contributing to initiatives that align with our Data Governance Strategy, the Data Engineer enhances both employee and customer experiences through trusted data.As a Data Engineer you will:Data Pipeline Development and Maintenance:Build, maintain, test, deploy, and enhance data pipelines and systems that clean, transform, and aggregate data from various source systems.Implement best practices in ETL/ELT workflows, ensuring efficiency, robustness, and compliance with governance standards.Process Improvement and Documentation:Assist in the development of data management procedures and documentation as required.Develop, implement, and maintain change control and testing processes for modifications in our data environment, assist in defining data quality metrics and monitoring processes.Collaboration and Stakeholder Engagement:Collaborate with various teams and stakeholders to understand business needs and contribute to project delivery, support data democratization initiatives, ensuring secure access while upholding compliance.Partner with business teams to transform requirements into actionable data solutions that align with organizational objectives.Analytical Support and Visualization:Support predictive and AI/ML initiatives to analyze large, complex datasets to identify meaningful patterns and provide actionable insights.Create informative visualizations using tools like Tableau to effectively display significant amounts of data and complex relationships.Positive Work Environment and Technology Advocacy:Contribute to a positive working environment that enhances both the customer and employee experience.Support the adoption and evolution of advanced technologies, staying informed about emerging trends in data engineering and promote best practices.Initiative and Professional Development:Demonstrate initiative while remaining open to feedback, effectively prioritizing tasks and taking purposeful action.Participate in the ongoing enhancement of the bank's Data Governance Roadmap through continuous learning and skill development.Qualifications for SuccessRequired - Bachelor's degree in Data, Business, Finance, Information Management, or equivalent work experience.Preferred \u2013 Hands on experience with cloud technologies like Informatica IDMC, Snowflake, and Tableau. Exposure to cloud platforms like AWS, Azure or GCP is beneficial.Experience as a Data Engineer or in a similar role, knowledge of data warehousing concepts and experience with data models, relational database systems and ETL/ELT integration processes, and experience with programming languages like SQL or Python.Relevant certifications (Informatica, AWS Data Analytics, Azure Data Engineer) are advantageous but not required.Attention to detail, ensuring clarity and accuracy in data handling and documentation, experience with version control systems like Git is a plus.Basic understanding of data quality principles and the ability to assist in translating data into actionable insights for stakeholders.Awareness of evolving data security best practices and regulatory compliance standards relevant to the banking industry.Solid communication and presentation skills, capable of delivering technical information to both technical and non-technical audiences.The Bank of Missouri is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. If you have a disability and you believe you need a reasonable accommodation in order to search for a job opening or to submit an online application, please contact us or call toll-free 888-547-6541.\n",
    "location": "Missouri, United States"
  },
  "160": {
    "id": "3908043737",
    "Company": "Orion Innovation",
    "title": "Big Data Engineer",
    "description": "Orion Innovation is a premier, award-winning, global business and technology services firm. Orion delivers game-changing business transformation and product development rooted in digital strategy, experience design, and engineering, with a unique combination of agility, scale, and maturity. We work with a wide range of clients across many industries including financial services, professional services, telecommunications and media, consumer products, automotive, industrial automation, professional sports and entertainment, life sciences, ecommerce, and education.Job ResponsibilitiesRequirement analysis, requirement gathering, design, business impact analysis, gap analysis, estimation, development, testing, coding, code review, unit testing and deployment of the application.Design, build, install, configure, and support Hadoop (Big Data). Design and implement big data pipelines to ingest and process data in real-time and monitor for data miss or error in progress to ensure that data reaches the end system. Measure the performance of various APIs and implement optimization on slow services to enhance the responsiveness of the system. Deliver functional enhancements to the existing big data applications. Provide inputs on solution architecture based on evaluation/understanding of solution alternatives, frameworks, and products. Develop Hadoop batch jobs for data extraction from multiple unstructured and structured data sources for populating into various repositories (Hadoop, Neo4J, MongoDB, Apache SOLR). Interact with clients to elicit architectural and non-functional requirements such as performance, scalability, reliability, availability, maintainability. Participate in designing the data model structured data in RDBMS into a graph database solution and NoSQL database solution. Develop near real-time data processing solutions using Kafka and Spark Streaming. Participate in designing Spark architecture with Databricks and Structured Streaming.Set up Microsoft Azure with Databricks, and Databricks Workspace for business analytics. Contribute to the automate build process by using Jenkins and Ansible to achieve continuous integration and continuous deployment.RequirementsMaster\u2019s in Computer Science/Applications, Information Technology/Systems or Electronics/Electrical Engineering + minimum 1 year experience as Big Data Engineer, Data Engineer, Data Engineering Specialist, Data Warehousing Analyst or related occupation.Must be willing to travel/relocate to anywhere in the US.Edison, NJ and unanticipated locations throughout USOrion is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, citizenship status, disability status, genetic information, protected veteran status, or any other characteristic protected by law.Candidate Privacy PolicyOrion Systems Integrators, LLC And Its Subsidiaries And Its Affiliates (collectively, \u201cOrion,\u201d \u201cwe\u201d Or \u201cus\u201d) Are Committed To Protecting Your Privacy. This Candidate Privacy Policy (orioninc.com) (\u201cNotice\u201d) ExplainsWhat information we collect during our application and recruitment process and why we collect it;How we handle that information; andHow to access and update that information.Your use of Orion services is governed by any applicable terms in this notice and our general Privacy Policy.\n        ",
    "location": "Edison, NJ"
  },
  "161": {
    "id": "4097642374",
    "Company": "UBS",
    "title": "Data Engineer (Spark, Databricks, Azure, Scala & Java)",
    "description": "Job Reference #305663BRJob TypeFull TimeYour roleDo you want to drive the next level of application architecture in the Compliance IT space? How about shaping the future of Compliance technology together with a fun-loving cross-functional team? Are you an experienced Technical Lead Engineer with a strong focus on delivery and passionate about leading-edge modern core Java, data platforms and cloud technologies?\u202fWe are looking for a hands-on Data Engineer (Spark, Databricks, Azure, Scala & Java) to: implement and deliver high quality software solutions for our Non-Financial Risk Metrics Assessment Tool and Workbench platforms.\u202f help us build state of the art solutions that underpin GCRG using cutting edge technologies like Scala, Databricks, Java, Azure Data Lake, Apache Spark, Spring Boot, ReactJS, Kubernetes, Python, Postgres etc. demonstrate superior collaboration skills in working closely with other development, testing and implementation teams to roll-out important regulatory and business improvement programs develop required web services by analyzing business functional/technical requirements and implement them support the technical development and ensure delivery of well tested, signed-off and fit-for-purpose solutions ability to review stories / requirements, design solutions, and develop software components within agile delivery framework collaborate with other global IT and cross-functional teams as group moves towards agile delivery solutionsYour teamYou will be a key member of a expanding Data, Analytics & Reporting team which is part of the Compliance Technology function. We are a small friendly bunch who take pride in the quality of work that we do. As a team, we provide application design, development and support to various model hosting platforms within Group Compliance stream, working closely with peers from the business team as well as other engineering teams. Your role will be based in Weehawken, NJ.Diversity helps us grow, together. That\u2019s why we are committed to fostering and advancing diversity, equity, and inclusion. It strengthens our business and brings value to our clients.Your expertise a degree level education; preferably Computer Science (Bachelor or Master\u2019s degree) ideally 8+ years of hands-on design and development experience in several of the relevant technology areas, preferably in a cloud environment (Lakehouse architecture, Azure Data Lake, Scala, Databricks, Spark/Spark SQL, Spring Boot, ReactJS, Kubernetes, Postgres)\u202f ideally 3+ years of hand-on experience in distributed processing using Databricks, Apache Python/Spark, Kafka & leveraging Airflow scheduler/executor framework ideally 2+ years of hand-on experience programming experience in Scala (must have), Python & Java (preferred) experience working with Agile development methodologies and delivering within Azure DevOps, automated testing on tools used to support CI and release management coding skills in Java, SQL, Spring Boot and other Java based frameworks (added advantage) coding skills in UI development using ReactJS (Optional) demonstrate strong communication skills, both to management and teams background in compliance and Risk Management collaborative approach towards problem solving, working closely with other colleagues in the global team and sensitive towards diversityAbout UsUBS is the world\u2019s largest and the only truly global wealth manager. We operate through four business divisions: Global Wealth Management, Personal & Corporate Banking, Asset Management and the Investment Bank. Our global reach and the breadth of our expertise set us apart from our competitors..We have a presence in all major financial centers in more than 50 countries.How We HireWe may request you to complete one or more assessments during the application process. Learn moreJoin usAt UBS, we embrace flexible ways of working when the role permits. We offer different working arrangements like part-time, job-sharing and hybrid (office and home) working. Our purpose-led culture and global infrastructure help us connect, collaborate, and work together in agile ways to meet all our business needs.From gaining new experiences in different roles to acquiring fresh knowledge and skills, we know that great work is never done alone. We know that it's our people, with their unique backgrounds, skills, experience levels and interests, who drive our ongoing success. Together we\u2019re more than ourselves. Ready to be part of #teamUBS and make an impact?Disclaimer / Policy StatementsUBS is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills and experiences within our workforce.\n        ",
    "location": "New Jersey, United States"
  },
  "162": {
    "id": "4131992393",
    "Company": "Dow Jones",
    "title": "Data Engineer",
    "description": "About Our OrganizationDow Jones is a global provider of news and business information, delivering content to consumers and organizations around the world across multiple formats, including print, digital, mobile and live events. Dow Jones has produced unrivaled quality content for more than 130 years and today has one of the world\u2019s largest news-gathering operations globally. It is home to leading publications and products including the flagship Wall Street Journal, America\u2019s largest newspaper by paid circulation; Barron\u2019s, MarketWatch, Mansion Global, Financial News, Investor\u2019s Business Daily, Factiva, Dow Jones Risk & Compliance, Dow Jones Newswires, OPIS and Chemical Market Analytics. Dow Jones is a division of News Corp (Nasdaq: NWS, NWSA; ASX: NWS, NWSLV).About The TeamOPIS, a Dow Jones company, provides price transparency across the global fuel supply chain, including the Spot, Wholesale Rack and Retail markets. OPIS enables customers to buy and sell energy commodities with confidence with multi-platform access to accurate data, real-time news, powerful software and educational events. Our commitment to reliability is reinforced by personalized customer service and constant innovation. OPIS listens to what the energy community needs and responds with flexible and easy-to-use products. Navigating world fuel markets is complex \u2013 OPIS makes it simpler.About The RoleWe are looking for a Data Engineer with SQL experience that will develop and maintain database architecture and code initiatives, delivering excellent products in scope and on time, and meeting current and long-term business needs for the business area. Participate in an agile team of motivated, driven, high-energy developers, building trust and confidence with team members and business stakeholders. Play a key role in analyzing data, identifying data patterns/anomalies and implementing algorithms with a focus on accuracy and completeness.You WillWork in a collaborative agile development environmentDevelop database code, objects and scripts to support project initiativesManage code and releases through Azure DevOps CI/CD build/release pipelinesOptimize database systems for performance and optimal resource utilizationIdentify and troubleshoot issues related to data qualityContinue to enhance your skills in data analysis, database development and data validationYou HaveBachelor\u2019s Degree in computer science, related technical field, or equivalent experience6 months database experience or related coursework (SQL Server, PostgreSQL preferred)Ability to create and maintain stored procedures, views and functionsAbility to write SQL queries and statements to explore source data and data issuesFamiliarity with agile development, code repositories such as Git, and CI/CD principlesFamiliarity with Python and SnowflakeMust speak and write in English fluentlyOur BenefitsComprehensive Healthcare PlansPaid Time OffRetirement PlansComprehensive Insurance PlansLifestyle programs & Wellness ResourcesEducation BenefitsFamily Care Benefits & Caregiving SupportCommuter Transit ProgramSubscription DiscountsEmployee Referral ProgramLearn More About All Our US BenefitsReasonable accommodation: Dow Jones, Making Careers Newsworthy - We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law.EEO/AA/M/F/Disabled/Vets. Dow Jones is committed to providing reasonable accommodation for qualified individuals with disabilities, in our job application and/or interview process. If you need assistance or accommodation in completing your application, due to a disability, email us at talentresourceteam@dowjones.com. Please put \"Reasonable Accommodation\" in the subject line and provide a brief description of the type of assistance you need. This inbox will not be monitored for application status updates.Business Area: Dow Jones - OPISJob Category: Data Analytics/Warehousing & Business IntelligenceUnion StatusNon-Union rolePay Range: $75,000 - $95,000We recognize that attracting the best talent is key to our strategy and success as a company.As a result, we aim for flexibility in structuring competitive compensation offers to ensure we are able to attract the best candidates.The quoted salary range represents our good faith estimate as to what our ideal candidates are likely to expect, and we tailor our offers within the range based on the selected candidate's experience, industry knowledge, location, technical and communication skills, and other factors that may prove relevant during the interview process.Pay-for-performance is a key element in our strategy to attract, engage, and motivate talented people to do their best work. Similarly to salary, for bonus eligible roles, targets are set based on a variety of factors including competitive market practice.For benefits eligible roles, in addition to cash compensation, the company provides a comprehensive and highly competitive benefits package, with a variety of physical health, retirement and savings, caregiving, emotional wellbeing, transportation, and other benefits, including \"elective\" benefits employees may select to best fit the needs and personal situations of our diverse workforce..Since 1882, Dow Jones has been finding new ways to bring information to the world\u2019s top business entities. Beginning as a niche news agency in an obscure Wall Street basement, Dow Jones has grown to be a worldwide news and information powerhouse, with prestigious brands including The Wall Street Journal, Dow Jones Newswires, Factiva, Barron\u2019s, MarketWatch and Financial News.This longevity and success is due to a relentless pursuit of accuracy, depth and innovation, enhanced by the wisdom of past experience and a solid grasp on the future ahead. More than its individual brands, Dow Jones is a modern gateway to intelligence, with innovative technology, advanced data feeds, integrated solutions, expert research, award-winning journalism and customizable apps and delivery systems to bring the information that matters most to customers, when and where they need it, every day.Req ID: 44604\n        ",
    "location": "Gaithersburg, MD"
  },
  "163": {
    "id": "4168180750",
    "Company": "Dropbox",
    "title": "Data Engineer, Data Finance",
    "description": "Dropbox is a Virtual First company. For this role, we are hiring in Zones 2 and 3. Please refer to our Compensation section below to see what neighborhoods fall under each Zone.Company DescriptionDropbox isn\u2019t just a workplace\u2014it\u2019s a living lab for more enlightened ways of working. We're a global community of more than 2,000 bold visionaries and resourceful doers who are shaping the future of Dropbox\u2014and with it the future of work. Our Virtual First model combines the flexibility of a distributed workplace with the power of human connection, making space for both meaningful work and meaningful relationships. With our start-up mindset and enterprise-level opportunities, you can be who you are and grow into who you\u2019re meant to be. Here, you can own your impact to make work more intuitive, joyful, and human\u2014for you as a Dropboxer and for hundreds of millions of people worldwide. If you're ready to push boundaries\u2014and yourself\u2014 Dropbox is ready for you.Team DescriptionThe Dropbox Engineering Team builds the technology that creates more enlightened ways of working for hundreds of millions of people. Every day, our platforms\u2014including Dropbox Dash, Dropbox Sign, and our core sync engine\u2014handle over a billion files for users worldwide, creating engineering challenges as great as the opportunity for impact. Our software engineering team uses a range of technologies to solve interesting problems, including Python, React, Node.js, JavaScript, MongoDB, PostgreSQL, and Android development. We think like a startup but build for an enterprise, exploring new possibilities that transform how people work. If you're excited about turning complex technical challenges into intuitive solutions at scale, join our Engineering team. Areas of work include Machine Learning Engineers, Infrastructure Engineer, Product SWE Frontend and Backend, Mobile Software Engineers (iOS and Android), Engineering Manager, Data Engineer, Software Development Engineer in Test, Security Engineering, Site Reliability Engineer, Technical Program Managers, Network Engineer, Datacenter Engineer, Technical Supply Chain Manager and more.Role DescriptionDropbox is seeking a highly skilled and motivated Data Engineer to join our dynamic Financial Data Engineering team. You will be responsible for building next-generation financial data pipelines that support crucial business decisions across the organization. The ideal candidate will have extensive experience migrating from other platforms to Databricks, a strong culture of innovation and accountability, and expertise in developing data health metrics that integrate with data governance, observability, and quality management tools.If you enjoy thinking about how businesses can utilize data and figuring out how to build it, this role is perfect for you. With a solid foundation in test-driven development and experience in building scalable data pipelines, as well as familiarity with traditional data warehousing (DW) and ETL architectures, and significant experience with ecosystems like Databricks, Snowflake, EMR, and Airflow, you would be a great fit for our team. By collaborating with cross-functional teams, you will have the opportunity to drive substantial business impact, as high data quality and effective tooling are key to achieving significant growth at Dropbox.Our Engineering Career Framework is viewable by anyone outside the company and describes what\u2019s expected for our engineers at each of our career levels. Check out our blog post on this topic and more here.ResponsibilitiesParticipate in data migration from legacy platforms to Databricks and develop scalable, efficient, and cost-optimized data pipelinesBuild and integrate data health metrics and quality management tools, ensuring robust data governance and consistent standardsDesign and maintain tools for efficient data investigations, issue detection, and automated mitigation to uphold data quality and consistencyReplace outdated infrastructure with modern systems and provide operational support for critical data pipelinesSolve complex data integration challenges using optimal ETL patterns, frameworks, and techniques for structured and unstructured dataCollaborate with cross-functional teams to meet technical and business needs while fostering a culture of innovation and continuous improvementDefine and manage SLAs for high-priority datasets, including those driving critical (P0) business metricsApply Agile methodologies and industry best practices to ensure consistent delivery and alignment with business objectivesMany teams at Dropbox run Services with on-call rotations, which entails being available for calls during both core and non-core business hours. If a team has an on-call rotation, all engineers on the team are expected to participate in the rotation as part of their employment. Applicants are encouraged to ask for more details of the rotations to which the applicant is applyingRequirements5+ years of experience in data engineering or related rolesBachelor\u2019s degree or foreign equivalent in Computer Science or a closely related fieldProven experience with data migration projects, specifically to DatabricksStrong expertise in data health metrics, data governance, and quality management, with experience integrating tools like Monte Carlo and AtlanSolid experience in building and maintaining data pipelines and infrastructureExcellent problem-solving skills and the ability to troubleshoot complex data issuesStrong programming skills in Python, Scala, or JavaDemonstrated ability to innovate and drive accountability within a teamExperience with version control systems like Git and test automation and CICDPreferred Qualifications5+ years of Python or Java, Scala development experience5+ years of SQL experience5+ years of experience with schema design and dimensional data modelingCompensationUS Zone 1This role is not available in Zone 1US Zone 2$145,400\u2014$196,600 USDUS Zone 3$129,200\u2014$174,800 USDThe range(s) listed above is the expected annual salary/OTE (On-Target Earnings) for this role, subject to change.Please note, OTE are for sales roles only.Salary/OTE is just one component of Dropbox\u2019s total rewards package. All regular employees are also eligible for the corporate bonus program or a sales incentive (target included in OTE) as well as stock in the form of Restricted Stock Units (RSUs).Dropbox takes a number of factors into account when determining individual starting pay, including job and level they are hired into, location/metropolitan area, skillset, and peer compensation. We target most new hire offers between the minimum up to the middle of the range.Dropbox uses the zip code of an employee\u2019s remote work location to determine which metropolitan pay range we use. Current US Zone locations are as follows: US Zone 1: San Francisco metro, New York City metro, or Seattle metro US Zone 2: Austin (TX) metro, Chicago metro, California (outside SF metro), Colorado, Connecticut (outside NYC metro), Delaware, Massachusetts, New Hampshire, New York (outside NYC metro), Oregon, Pennsylvania (outside NYC or DC metro), Washington DC metro, and West Virginia (DC metro) US Zone 3: All other US locationsBenefitsDropbox is committed to investing in the holistic health and wellbeing of all Dropboxers and their families. Our benefits and perks programs include, but are not limited to:Competitive medical, dental, and vision coverage401(k) plan with a generous company match and immediate vestingFlexible PTO/Paid Time Off, paid holidays, Volunteer Time Off, and more, allowing you time to unplug, unwind, and refreshIncome Protection Plans: Life and disability insuranceBusiness Travel Protection: Travel medical and accident insurancePerks Allowance to be used on what matters most to you, whether that\u2019s wellness, learning and development, food and groceries, and much moreParental benefits including: Parental Leave, Child and Adult Care, Day Care FSA, Fertility Benefits, Adoption and Surrogacy Support, and Lactation SupportAccess to over 10,000 global co-working spaces through Gable.tomaking it easy to book flexible workspaces for collaboration or individual workQuarterly Cell phone and internet allowanceMental health and wellness benefitsDisability and neurodivergence support benefitsAdditional benefit details are available upon request.Dropbox is an equal opportunity employer. We are a welcoming place for everyone, and we do our best to make sure all people feel supported and connected at work.\n        ",
    "location": "United States"
  },
  "164": {
    "id": "4161478404",
    "Company": "JPMorganChase",
    "title": "Data Engineer II - Python, Pyspark",
    "description": "Job DescriptionYou thrive on diversity and creativity, and we welcome individuals who share our vision of making a lasting impact. Your unique combination of design thinking and experience will help us achieve new heights.As a Data Engineer II at JPMorgan Chase within the Payments Trust & Safety team, you are part of an agile team that works to enhance, design, and deliver the data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. Our goal is to keep JPMorgan Chase and our clients safe as they transact through ACH, Wire and Credit card channels. The Data team is responsible to make large-scale data across lines of Business for applying machine-learning to our most critical and wide-range customer products to solve not only Trust & Safety problems (e.g. Fraud) but also related problems (e.g. payment optimization, forecasting). As a member of this team you will work with many lines of business and develop Machine Learning solutions that have a broader impact for the bank. We work closely with our engineering and product partners to develop and deploy solutions to reach our customers.Job ResponsibilitiesCollaborate with all of JPMorgan\u2019s lines of business and functions to delivery software solutions.Experiment, Architect, develop and productionize efficient Data pipelines, Data services and Data platforms contributing to the Business.Design and implement highly scalable, efficient and reliable data processing pipelines and perform analysis and insights to drive and optimize business result.Acts on previously identified opportunities to converge physical, IT, and data security architecture to manage access Champions the firm\u2019s culture of diversity, equity, inclusion, and respectRequired Qualifications, Capabilities And SkillsFormal training or certification on large scale technology program concepts and 2+ years applied experience in Data Technologies.Experienced programming skills with Java, Python or other equivalent languages.Experience across the data lifecycle, building Data frameworks, working with Data lakes.Experience with Batch and Real time Data processing with Spark or FlinkWorking knowledge of AWS Glue and EMR usage for Data processingExperience working with DatabricksExperience working with Python/Java, PySpark etc.Working experience with both relational and NoSQL databases Experience in ETL data pipelines both batch and real-time data processing, Data warehousing, NoSQL DB.Preferred Qualifications, Capabilities And SkillsCloud computing: Amazon Web Service, Docker, Kubernetes.Experience in big data technologies: Hadoop, Hive, Spark, Kafka.Experience in distributed system design and developmentAbout UsJPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world\u2019s most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.We offer a competitive total rewards package including base salary determined based on the role, experience, skill set and location. Those in eligible roles may receive commission-based pay and/or discretionary incentive compensation, paid in the form of cash and/or forfeitable equity, awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants\u2019 and employees\u2019 religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.JPMorgan Chase & Co. is an Equal Opportunity Employer, including Disability/VeteransAbout The TeamJ.P. Morgan\u2019s Commercial & Investment Bank is a global leader across banking, markets, securities services and payments. Corporations, governments and institutions throughout the world entrust us with their business in more than 100 countries. The Commercial & Investment Bank provides strategic advice, raises capital, manages risk and extends liquidity in markets around the world.\n        ",
    "location": "Tampa, FL"
  },
  "165": {
    "id": "4152661933",
    "Company": "Travelers",
    "title": "Data Engineer I \u2013 AWS Cloud/ETL",
    "description": "Who Are We?Taking care of our customers, our communities and each other. That\u2019s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.Compensation OverviewThe annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.Salary Range$106,300.00 - $175,400.00Target Openings1What Is the Opportunity?Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate the stories found in data. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Artificial Intelligence, Machine Learning and business intelligence/insights.What Will You Do?Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.Design data solutions.Analyze sources to determine value and recommend data to include in analytical processes.Incorporate core data management competencies including data governance, data security and data quality.Collaborate within and across teams to support delivery and educate end users on data products/analytic environment.Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate.Test data movement, transformation code, and data components.Perform other duties as assigned. What Will Our Ideal Candidate Have?Bachelor\u2019s Degree in STEM related field or equivalentSix years of related experienceProficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices.The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions.Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on.Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems.Strong verbal and written communication skills with the ability to interact with team members and business partners.Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities.Experience with some of the following tools & platforms (or similar): Ab Initio, Autosys, AWS (s3, Lambda, Kinesis, API Gateway, IAM, Glue, SNS, SQS, EventBridge, EKS, VPC, Step Functions, ECS/EKS, DynamoDB, etc.), Databricks, Python, Kafka, dbt, Terraform, Snowflake, SQL, Jenkins, Github,, Talend, Alation, Hashicorp Vault / AWS Secrets Manager, Docker, BlackDuck, SonarQubeKnowledge and experience with the some of the following concepts: Real-time & Batch Data Processing, Workload Orchestration, Cloud, Datalakes, Data Security, Serverless, Testing/Test Automation (Unit, Integration, Performance, etc.), DevOps, Logging, Monitoring, and Alerting, Encryption / Decryption, Data Masking, Cost & Performance Optimization.What is a Must Have?Bachelor\u2019s degree or equivalent training with data tools, techniques, and manipulation.Four years of data engineering or equivalent experience.What Is in It for You?Health Insurance: Employees and their eligible family members \u2013 including spouses, domestic partners, and children \u2013 are eligible for coverage from the first day of employment.Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.Wellness Program: The Travelers wellness program is comprised of tools, discounts and resources that empower you to achieve your wellness goals and caregiving needs. In addition, our mental health program provides access to free professional counseling services, health coaching and other resources to support your daily life needs.Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.Employment PracticesTravelers is an equal opportunity employer. We value the unique abilities and talents each individual brings to our organization and recognize that we benefit in numerous ways from our differences.In accordance with local law, candidates seeking employment in Colorado are not required to disclose dates of attendance at or graduation from educational institutions.If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email so we may assist you.Travelers reserves the right to fill this position at a level above or below the level included in this posting.To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/.\n        ",
    "location": "Atlanta, GA"
  },
  "166": {
    "id": "4117851033",
    "Company": "INVISTA",
    "title": "Data Engineer",
    "description": "Your JobAs a Data Engineer at INVISTA, you will be a vital contributor to our Enterprise Data Platform, responsible for designing and implementing data pipelines, optimizing data workflows, and ensuring data reliability and accessibility. Your work will be instrumental in empowering our organization to make data-driven decisions and fueling innovation across the company.Our TeamJoining our Data and Analytics team means becoming a key player in a dynamic and innovative group of professionals dedicated to unlocking the power of data. Here, you'll find a diverse and collaborative environment where your ideas and expertise will shape the future of our data-driven organization. We pride ourselves on fostering a culture of continuous learning, creativity, and teamwork, and we're looking for individuals who are eager to contribute their skills and passion to our shared mission.This role may work a hybrid schedule out of our Wichita (KS) or Katy (TX) office location.What You Will DoCollaborate closely with business partners and data scientists to align data engineering efforts with their specific needs and objectives, ensuring that data solutions contribute effectively to the organization's overall goals. This includes proposing alternative solutions, with their pros and cons, to help guide the workstream forward. Understand, develop, maintain/troubleshoot, automate, and optimize orchestration and data pipelines using tools such as Snowflake, dbt, GitHub, AWS, Power BI, and Neo4J. Implement data integration strategies for streaming, event driven, and batch data sources. Design and implement data modeling and ETL/ELT processes to ensure data quality, consistency, availability and data centricity.Manage and maintain data warehouses and ensure data security and compliance with company policies and relevant regulations.Perform data transformations, aggregations, and data cleansing to support analytics and reporting needs.Document data engineering processes, pipelines, and architecture for knowledge sharing and compliance.Stay up to date with industry best practices, emerging technologies, and trends in data engineering.Who You Are (Basic Qualifications)Experience in data modeling with SQLExperience collaborating with a Finance team or in the Financial Services industryExperience programming in PythonAbility to travel up to 20% of the timeThis role is not eligible for visa sponsorshipWhat Will Put You AheadExperience with Snowflake data warehouse technologyProficiency in designing software and data solutions that scaleProficiency in Power BI, strong in DAX and analytics report buildingHands-on experience with AWS cloud computing platformSAP Financial domain knowledge \u2013 Financial accounting and controlling modulesExperience with DevOps practices and tools, such as continuous integration (CI) and continuous deployment (CD) pipelines, version control systems (e.g., Git)Knowledge of data governance and compliance best practicesAt Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate's knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.Hiring PhilosophyAll Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here.Who We AreAs a Koch company, INVISTA has a long history of working to make the world around you a better place. From parts for the automotive industry to medical equipment, air bags, food packaging and clothing, our ingredients in the nylon 6,6 and polypropylene value chains help bring many of life\u2019s essential products to market.At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.Our BenefitsOur goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength - focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes - medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.Additionally, everyone has individual work and personal needs. We seek to enable the best work environment that helps you and the business work together to produce superior results.Equal OpportunitiesEqual Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, some offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please click here for additional information. (For Illinois E-Verify information click here, aqu\u00ed, or tu).\n",
    "location": "Wichita, KS"
  },
  "167": {
    "id": "4098898356",
    "Company": "CereCore",
    "title": "Big Data Engineer",
    "description": "Classification: ContractContract Length: 12 MonthsPosition SummaryThe Big Data Engineer/Consulting-Level serves as a primary development resource for design, writing code, test, implementation, document functionality, and maintain of NextGen solutions for the GCP Cloud enterprise data initiatives. The role requires working closely with data teams, frequently in a matrixed environment as part of a broader project team. Due to the emerging and fast-evolving nature of GCP/Hadoop technology and practice, the position requires that one stay well-informed of technological advancements and be proficient at putting new innovations into effective practice. In addition, this position requires a candidate who can analyze business requirements, perform design tasks, construct, test, and implement solutions with minimal supervision. This candidate will have a record of accomplishment of participation in successful projects in a fast-paced, mixed team environment.ResponsibilitiesThis role will provide application development for specific business environments.Responsible for building and supporting a GCP based ecosystem designed for enterprise-wide analysis of structured, semi-structured, and unstructured data.Bring new data sources into GCP, transform and load to databases and support regular requests to move data from one cluster to anotherDevelop a strong understanding of relevant product area, codebase, and/or systemsDemonstrate proficiency in data analysis, programming, and software engineeringWork closely with the Lead Architect and Product Owner to define, design and build new features and improve existing productsProduce high quality code with good test coverage, using modern abstractions and frameworksWork independently, and complete tasks on-schedule by exercising strong judgment and problem-solving skillsClosely collaborates with team members to successfully execute development initiatives using Agile practices and principlesParticipates in the deployment, change, configuration, management, administration and maintenance of deployment process and systemsProven experience effectively prioritizing workload to meet deadlines and work objectivesWorks in an environment with rapidly changing business requirements and prioritiesWork collaboratively with Data Scientists and business and IT leaders throughout the company to understand their needs and use cases.Work closely with management, architects and other teams to develop and implement the projects.Actively participate in technical group discussions and adopt any new technologies to improve the development and operations.RequirementsThis role will provide application development for specific business environments.Responsible for building and supporting a GCP based ecosystem designed for enterprise-wide analysis of structured, semi-structured, and unstructured data.Bring new data sources into GCP, transform and load to databases and support regular requests to move data from one cluster to anotherDevelop a strong understanding of relevant product area, codebase, and/or systemsDemonstrate proficiency in data analysis, programming, and software engineeringWork closely with the Lead Architect and Product Owner to define, design and build new features and improve existing productsProduce high quality code with good test coverage, using modern abstractions and frameworksWork independently, and complete tasks on-schedule by exercising strong judgment and problem-solving skillsClosely collaborates with team members to successfully execute development initiatives using Agile practices and principlesParticipates in the deployment, change, configuration, management, administration and maintenance of deployment process and systemsProven experience effectively prioritizing workload to meet deadlines and work objectivesWorks in an environment with rapidly changing business requirements and prioritiesWork collaboratively with Data Scientists and business and IT leaders throughout the company to understand their needs and use cases.Work closely with management, architects and other teams to develop and implement the projects.Actively participate in technical group discussions and adopt any new technologies to improve the development and operations.\n",
    "location": "Nashville, TN"
  },
  "168": {
    "id": "4188211600",
    "Company": "eBusiness Technologies Corp.",
    "title": "Data Engineer (AWS)",
    "description": "We are seeking a skilled Data Engineer with expertise in SQL, window functions, Python, AWS, Redshift, and Lambda. The ideal candidate should be proficient in writing cost-efficient, high-performance code for data processing and optimization in AWS environments.Key Skills:Strong proficiency in SQL, including advanced concepts like window functions, CTEs, and query optimization. Experience in optimizing data pipelines for cost and speed efficiencyExtensive experience with AWS services, particularly Redshift (cluster management, cost optimization) and Lambda (serverless workflows).Strong problem-solving and debugging skillsHands-on experience with multithreading and multiprocessing in Python to handle large-scale data tasks efficiently.Experience with Airflow, Glue, and TerraformKnowledge of data lake architectures and best practicesSend your resume to kaykay@ebusinesstechcorp.comLets Connect!\n",
    "location": "McKinney, TX"
  },
  "169": {
    "id": "4167857842",
    "Company": "TEKsystems",
    "title": "Data Engineer",
    "description": "Description Working on the insights team within the EDO, this person will be joining a team working with the latest tools to help drive better customer engagement and experience leveraging data to make decisions. Adjacent to the data science and decision team this person will work in an agile squad to deliver customer focused work. MUST BE HYBRID ONSITE Skills Sql, Data, Etl, snowflake, cloud, data warehouse Top Skills Details Sql,Data,Etl Additional Skills & Qualifications Strong communication HYBRID ONSITE Experience Level Intermediate Level Pay And Benefits The pay range for this position is $60.00 - $75.00/hr. Requirements Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to specific elections, plan, or program terms. If eligible, the benefits available for this temporary role may include the following: Medical, dental & vision Critical Illness, Accident, and Hospital 401(k) Retirement Plan \u2013 Pre-tax and Roth post-tax contributions available Life Insurance (Voluntary Life & AD&D for the employee and dependents) Short and long-term disability Health Spending Account (HSA) Transportation benefits Employee Assistance Program Time Off/Leave (PTO, Vacation or Sick Leave) Workplace Type This is a hybrid position in Cincinnati,OH. Application Deadline This position is anticipated to close on Mar 3, 2025. About TEKsystems We're partners in transformation. We help clients activate ideas and solutions to take advantage of a new world of opportunity. We are a team of 80,000 strong, working with over 6,000 clients, including 80% of the Fortune 500, across North America, Europe and Asia. As an industry leader in Full-Stack Technology Services, Talent Services, and real-world application, we work with progressive leaders to drive change. That's the power of true partnership. TEKsystems is an Allegis Group company.The company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law.\n        ",
    "location": "Cincinnati, OH"
  },
  "170": {
    "id": "4088562717",
    "Company": "SHEIN Technology LLC",
    "title": "Big Data Engineer",
    "description": "Job Title: Big Data Engineer Reports to: Senior Manager, Big DataJob Location: San Diego, CAJob Status: ExemptAbout SHEINSHEIN is a global online fashion and lifestyle retailer, offering SHEIN branded apparel and products from a global network of vendors, all at affordable prices. Headquartered in Singapore, with more than 15,000 employees operating from offices around the world, SHEIN is committed to making the beauty of fashion accessible to all, promoting its industry-leading, on-demand production methodology, for a smarter, future-ready industry.Position SummaryWe're seeking a full-time Big Data Engineer (official title is Big Data Engineer II) our San Diego-based corporate office. The Big Data team has the unique opportunity to own the US-based Big Data operations, refactor and redesign, while partnering with an established global organization. We are looking for an experienced Big Data Engineer to support operations, solve complex issues, and drive growth. We achieve this goal by building and deploying highly scalable data pipelines, adhering to software/data engineering best practices, and ensuring the security and quality of our data to the delight of our consumers.Job ResponsibilitiesPartner with global teams across data, security, infrastructure, and business functions to understand data requirements and deliver scalable solutions.Implement efficient and scalable data pipelines to extract, transform, and load (ETL) data, ensuring the smooth movement of data across systems.Conduct data quality checks to maintain the accuracy, consistency, and completeness of data in the pipelines.Optimize data pipelines for performance and reliability, addressing bottlenecks or inefficiencies.Ensure data integrity, security, privacy, and high availability across all systems.Monitor data pipelines for issues, diagnose problems when they arise, and take necessary actions to resolve them.Collaborate cross-functionally to address root causes and implement long-term preventive measures.Perform detailed root cause analysis of data processes to identify and resolve issues, while providing actionable insights for process improvements.Maintain documentation for data pipeline designs, processes, and troubleshooting guides, while clearly communicating updates and issues with stakeholders.Provide 24x7x365 on-call support for critical data operations on a rotational basis, ensuring the continuous availability of data systems.Job RequirementsBachelor\u2019s degree in computer science or information systems or equivalent technical discipline.5+ years of hands-on experience engineering data pipelines using big data technologies (Hive, Presto, Spark, Flink).Demonstrated ability to write complex SQL, and high degree of competency programing in Python and passion for writing clean, supportable code.Proficient in more than one scripting language (shell/Perl, etc.)Experience working in Unix/Linus environments.Experience using Azure/AWS/GCP or any other cloud providerHigh level of personal integrity, with the ability to professionally handle confidential matters and exudes the appropriate level of judgment and mature.The ability to remain calm during crises and resolve issues in accordance with SLAsDemonstrated leadership in driving projects and initiatives within the team and provide constructive feedback and guidance to junior team members.Good verbal and written communication skills and be able to work effectively with geographically remote teams.Nice to HaveBilingual in MandarinPay$123,000.00 min. \u2013 $209,800.00 max annually. Bonus offered.Benefits and PerksHealthcare (medical, dental, vision, prescription drugs)Health Savings Account with Employer FundingFlexible Spending Accounts (Healthcare and Dependent care)Company-Paid Basic Life/AD&D insuranceCompany-Paid Short-Term and Long-Term DisabilityVoluntary Benefit Offerings (Voluntary Life/AD&D, Hospital Indemnity, Critical Illness, and Accident)Employee Assistance ProgramBusiness Travel Accident Insurance401(k) Savings Plan with discretionary company match and access to a financial advisorVacation, paid holidays, floating holiday and sick daysEmployee discountsFree weekly catered lunchDog-friendly office (available at select locations)Free gym access (available at select locations)Free swag giveawaysAnnual Holiday PartyInvitations to pop-ups and other company eventsComplimentary daily office snacks and beveragesSHEIN Technology LLC is an equal opportunity employer committed to a diverse workplace environment.\n",
    "location": "San Diego, CA"
  },
  "171": {
    "id": "4161850746",
    "Company": "Garmin",
    "title": "Data Engineer 2",
    "description": "OverviewWe are seeking a full-time Data Engineer 2 in our Olathe, KS Support Center location. In this role, you will support Garmin's Operations team by transforming raw operational data into curated analytics datasets, which will drive reporting and Data Science activities.Essential FunctionsProvide technical input to feature development plans and concept documentsUnderstand production and operations issues as they relate to engineeringInteract/collaborate with business leadership to develop projects from an idea to implemented solutionDesign/create SQL tables for data warehouse using star schema principlesInvestigate/optimize complex SQL queries used in data loadsAdminister/maintain a container-based Airflow installationParticipate in design, code, test, maintenance, enhance and decommission phases throughout software life cycle to contribute technical expertise and to identify issuesParticipate in and help guide research POCs, including design, coding, and performance and efficacy measurementDemonstrate broad understanding of Garmin's business modelContribute to and triages major incident troubleshooting involving multiple disciplinesApply technical expertise and analysis to initiatives and contribute input to broader technology solutions outside of disciplineProvide 24x7 on call supportBasic QualificationsBachelor\u2019s Degree in Computer Science, Information Technology, Management Information Systems, Business or related field AND a minimum of 1 year relevant experienceExcellent academics (cumulative GPA greater than or equal to 3.0 as a general rule)In-depth knowledge of Airflow, Kafka, and pySparkExperience working with Docker, Kubernetes and Linux systemsExperience using Hadoop and various componentsConsistently demonstrates quality and effectiveness in work documentation and organization and demonstrates ability to implement new technologies effectivelyDesired QualificationsOutstanding academics (cumulative GPA greater than or equal to 3.5)Deep knowledge of star schema modeling and other data warehousing conceptsPassionate about data engineering concepts and tools, and willing to devote time towards continued learningHumble and let's results speak for themselvesA strong team player that cares about collective team successUndeterred by hard workDrive to understand the business units supported, Product Support, Product Quality, Manufacturing and Logistics teamsExperience with Hadoop, NoSQL, ElasticSearch, OpenSearch, S3 and CassandraGarmin International is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, citizenship, sex, sexual orientation, gender identity, veteran\u2019s status, age or disability.This position is eligible for Garmin's benefit program. Details can be found here: Garmin Benefits\n",
    "location": "Olathe, KS"
  },
  "172": {
    "id": "4181783310",
    "Company": "Amazon",
    "title": "Data Engineer I, FAE",
    "description": "DescriptionOperations Finance Technology team, responsible for building technical solutions for multi-billion WW Operational Cost analytic including Inbound cost (Receiving / PO), variable Cost, fixed cost, outbound (Customer Shipments / Transportation), and Customer Service, is looking for a Data Engineer.As a data engineer, you will get the exciting opportunity to work on very large data sets in one of the world's largest and most complex data warehouse environments. You will work closely with the business teams in analysis on various cost savings initiatives, many non-standard and unique business problems and use creative-problem solving to deliver actionable output.You will be responsible for designing and implementing an analytical environment using third-party and in-house reporting tools, modeling metadata, building reports and dashboards. You will have an opportunity to work with leading edge technologies like Redshift, Hadoop/Hive/Pig. You will be writing scalable queries and tuning performance on queries running over billion of rows of data.Basic Qualifications 1+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala) Experience with one or more scripting language (e.g., Python, KornShell)Preferred Qualifications Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Experience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc.Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status.Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you\u2019re applying in isn\u2019t listed, please contact your Recruiting Partner.Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $91,200/year in our lowest geographic market up to $185,000/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.Company - Amazon.com Services LLCJob ID: A2923705\n        ",
    "location": "Seattle, WA"
  },
  "173": {
    "id": "4187050117",
    "Company": "Verizon",
    "title": "Data Engineer",
    "description": "When you join VerizonYou want more out of a career. A place to share your ideas freely \u2014 even if they\u2019re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love \u2014 driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together \u2014 lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the V Team Life.What You\u2019ll Be Doing\u2026At Verizon, we are building a world-class Verizon Global services Data Analytics (D&A) Hub organization. We have begun a comprehensive program to streamline processes, improve systems, realign the organization and add opportunities for personal skill-building and professional growth. We\u2019re finding new ways to add value and provide strategic and executive support to our stake holders. This is the kind of work we do. We want to build leading D&A practices within Verizon, setting the stage to become the place any Data and Analytics professional wants to work. You can become an impetus for change.Enter the new era of Data product ownership and work with business solving real world problems by compiling and analyzing data and help tell the story behind the numbers. This position offers opportunities to drive better business partnering and insights, while developing your Data Intelligence skill set and leadership as we continue to grow as a world class organization.You are a valuable business partner, finding new and better ways of working by leveraging modern and digital data products and capabilities.You\u2019ll become involved in, but not limited to, discovery, planning, integrating, modeling, analysis and reporting that will impact important decisions around the growth and development of Verizon business.Responsibilities IncludePerforming subject matter expertise & ad-hoc analysis, including identifying new revenue streams and improving operational efficiencies, reduction in man hours, new metrics insights and drivers with the help of the, logistic and transportation and end to end data operations and data products.Ensuring timely and accurate delivery of data intelligence applications for planning, reporting & analysis for the business.Liaison with cross-functional teams and business partners to build a network and acquire advanced business & technical acumen.Identifying improvement opportunities and executing projects which may include leveraging digital tools for cloud technologies, data workflow creation, system integration, automation tools and dashboards.Play a crucial role in defining the data architecture framework, standards and principles, including modeling, metadata, security and reference data.Creating and enhancing metrics that track Logistics and transportation volume and spend metrics, On time delivery - carrier metrics and DC ops metrics.What We\u2019re Looking For\u2026You\u2019ll need to have:Bachelor\u2019s degree in Computer Science or related field or four or more years of work experience.Four or more years of relevant experience required, demonstrated through work experience and/or military experience.Experience in data lifecycle management.Proven track record in design and build of the infrastructure for data extraction, preparation, and loading of data from a variety of sources using technology such as SQL ,Non -SQL and Big data.Experience identifying ways to improve data reliability, efficiency and quality by various data solution techniques.Experience in Google Cloud Platform technologies like Big query, Composer, Dataflow.Experience or transferable skills leveraging digital tools such as Tableau, Qlik, Looker, ThoughtSpot, Alteryx, SQL, Python, or R.Experience in dashboard development using Looker/Tableau/Thoughtspot.Experience in analyzing large amounts of information to discover trends and patterns.Experience with Microsoft Office Suite and Google Suite.Experience creating and enhancing metrics that track Logistics and transportation volume and spend metrics , On time delivery - carrier metrics and DC ops metrics.Even better if you have one or more of the following:Master\u2019s degree or direct work experience in Data analytics, Supply chain or Telecom industry.Expert Knowledge of ETL process and reporting tools.Expert in writing Complex SQL queries and scripts using databases/tools like Oracle, SQL Server, or Google BigQuery, Data Stage, Python, Snowflake and pulling data from SQL/EDW data warehouses.Master Knowledge of common business & cost drivers, operational statement analysis, and Storytelling.Industry standard Data Automation and Proactive Alerting skills.Excellent communication skills and ability to focus on the details.Proficiency with Google Suite.If Verizon and this role sound like a fit for you, we encourage you to apply even if you don\u2019t meet every \u201ceven better\u201d qualification listed above.Where you\u2019ll be workingIn this hybrid role, you'll have a defined work location that includes work from home and a minimum eight assigned office days per month that will be set by your manager.Scheduled Weekly Hours40Equal Employment OpportunityVerizon is an equal opportunity employer. We evaluate qualified applicants without regard to veteran status, disability or other legally protected characteristics.\n        ",
    "location": "Alpharetta, GA"
  },
  "174": {
    "id": "4187056840",
    "Company": "Fairview Health Services",
    "title": "Data Engineer",
    "description": "Job OverviewThe Data Engineer participates in the planning, design, and deployment of enterprise data strategies by analyzing and translating business needs into a long-term solution data model that reduces duplications, streamlines data movements, and improves enterprise information management.Design, development, testing, administration, and maintenance of Master Data Management (MDM) platforms and processes, in partnership with Data Governance.Support the day-to-day operations for the data environment by supervising and monitoring data pipelines and fix any errors or performance issues that may occur in order to ensure complete and timely availability of data.Assists with the management of access to data in the environment to ensure the security of the data, and compliance with HIPAA guidelines.Define and implement data strategies, policies, controls to ensure accurate, complete, secure, trusted, reliable enterprise data, including platform solutions and models to store and retrieve data based on architecture design.Apply leadership skills with IT, business owners, and end users to define and document business, technical, and operational requirements.Champion initiatives to help the organization understand and improve data consistency, quality, and flows following internal and external regulations.Brings together, cleanses, and transforms data to provide insight and actionable recommendations to improve new and existing database systems and pipelines.Build analytics tools that apply the data pipeline to deliver actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.Introduce and advocate for industry standard methodologies of modern data warehouse such as customer-centric, adaptable, automated, elastic, governed, secure, etcPrepare accurate database design and structure documentation for management and executive teams.Builds and leads initiatives to produce written narrative summaries that outline project purpose, methodology, summary of findings, trends of interest, and opportunities.Educate team members on standards and best processes through training and individual support.Benefit OverviewFairview offers a generous benefit package including but not limited to medical, dental, vision plans, life insurance, short-term and long-term disability insurance, PTO and Sick and Safe Time, tuition reimbursement, retirement, early access to earned wages, and more! Please follow this link for additional information: https://www.fairview.org/careers/benefits/noncontractCompensation DisclaimerThe posted pay range is for a 40-hour workweek (1.0 FTE). The actual rate of pay offered within this range may depend on several factors, such as FTE, skills, knowledge, relevant education, experience, and market conditions. Additionally, our organization values pay equity and considers the internal equity of our team when making any offer. Hiring at the maximum of the range is not typical.EEO StatementEEO/AA Employer/Vet/Disabled: All qualified applicants will receive consideration without regard to any lawfully protected status\n        ",
    "location": "Minneapolis, MN"
  },
  "175": {
    "id": "4182141101",
    "Company": "Envista Forensics",
    "title": "Data Engineer",
    "description": "Inspiring People - Impactful ExperiencesIf there was one common theme to describe what our team members get from a career with Envista Forensics, it\u2019s: An Experience. Envista prides itself on being One Company/One Team.Forensic Consulting relies on scientific principles to investigate all types of failures impacting service, people, and business production\u2014 from minor to catastrophic. It\u2019s our job to analyze and determine why it happened.We\u2019re always looking for great professionals, in all disciplines and locations \u2013 contact us for more information about other opportunities.We\u2019re looking for someone who:Is Passionate. You have a genuine passion to problem solve.Is motivated and cultivates innovation. You\u2019re driven to be the very best. You challenge yourself to grow and learn every day and are encouraged by other team members.Is collaborative. You\u2019re excited to work with others throughout a global organization to help foster a superior workplace and culture. You are constantly thinking of new ways to make Envista successful. Wants to make an impact to drive results. You\u2019re looking to do amazing work. You\u2019re all about helping our clients both internally and externally.Operates with integrity and instills trust. You always conduct yourself with honesty and operate ethically in everything you do.Job DescriptionSUMMARY OF JOB PURPOSE: Do you have a passion for building data pipelines that unlock the power of information? Are you an expert in data modeling with a strong understanding of Kimball methodologies? If so, we want to hear from you! We are seeking a highly skilled Data Engineer with 5+ years of experience to join our growing team. You will play a critical role in designing, developing, and maintaining data pipelines that transform raw data into valuable insights for our business.PRIMARY JOB RESPONSIBILITIES:Design and implement data pipelines using established data modeling techniques, with a focus on the Kimball methodology. Develop and maintain data warehouses with a clear understanding of facts, dimensions, and the differences between type 1 and type 2 updates. Utilize Azure Data Factory to orchestrate data movement and transformations across various data sources. Design and manage data storage solutions in Azure SQL Server databases. Secure sensitive data using Azure Key Vaults and implement best practices for access control. Automate data pipeline deployments through CI/CD pipelines using Azure DevOps. Collaborate with data analysts and data scientists to understand data requirements and ensure data quality. Required Skills/Abilities/Experience:5-10 years of experience in data engineering or a related field. Proven experience with data modeling methodologies, particularly Kimball. Strong understanding of data warehousing concepts, including facts, dimensions, star schemas, and data quality. Expertise in designing and implementing data pipelines using tools like Azure Data Factory. Experience with Azure SQL Server databases and cloud data storage solutions. Proficiency with Azure Key Vaults and secure data management practices. Preferred Skills/Abilities/Experience:Working knowledge of Power BI data visualization tools. Education:Bachelor\u2019s degree in Computer Science, Information Technology, or related field OR equivalent work experience One Company/One Team is not just one of our 5 Guiding Principles, it\u2019s we how separate ourselves from our competitors. We learn together, we win together and through our team members voices, we bring certainty to an uncertain world. At Envista Forensics, we recognize that our potential team members come with a wealth of experience and talent beyond just the technical requirements of a role. We strive to reflect the communities and clients we serve to drive innovation, excellence, and meaningful work\u2014We want you to bring your authentic self to Envista. If your experience is close to what you see listed here, please still consider applying. Please let us know if you require reasonable accommodations during the interview process. Envista Forensics embraces diversity and is proud to be an equal opportunity employer. We are committed to building a team that represents a variety of backgrounds and perspectivesEnvista Forensics believes that Veterans arrive with not only translatable skills and technical expertise but in addition come with the intangibles; leadership and values that we believe align with our 5 Guiding Principles. Simply put, these qualities enable our success, so we encourage all Guardsmen, Reservists, and Veterans to consider Envista as their next career destination.\n",
    "location": "Baltimore, MD"
  },
  "176": {
    "id": "4145037652",
    "Company": "Funko",
    "title": "Data Engineer 2",
    "description": "DescriptionFunko OverviewWelcome to the Funko-verse, a world built on pure imagination, a land governed by the philosophy that stories matter, a universe comprised of characters from countless fandoms, a galaxy of once upon a times and happily ever afters.But what does Funko do?Funko is a purveyor of pop culture and licensed-focused collectibles company. Funko currently holds thousands of lenses and the rights to create tens of thousands of characters \u2013 one of the largest portfolios in the pop culture and collectibles industry. Funko\u2019s Pop! Vinyl is the number one stylized vinyl collectible on the market, selling millions of figures to fans around the world.DescriptionWe are seeking a Data Engineer 2 to join our growing Data and Analytics team as we revolutionize data at Funko. As a Data Engineer 2, you will be responsible for designing and implementing scalable and reliable data solutions, including data ingestion pipelines, data warehouses, and data lakes. You will collaborate closely with cross-functional teams to gather requirements, understand data needs, and develop data architecture to support business goals. The ideal candidate will have a strong background in data engineering and programming, with a passion for continuous learning, collaborative solutioning, and building data products that drive business outcomes.What You\u2019ll DoDesign, build, and maintain efficient, reusable, and reliable data pipelines to move data across systems from various sourcesDevelop ETL/ELT processes to ensure data quality and consistencyBuild and optimize data models and schema designs for performance and scalabilityImplement data governance and security principlesCollaborate with analysts to understand data requirements and provide scalable solutionsWork closely with stakeholders to understand business needs and translate them into technical requirementsMonitor and optimize the performance of the data infrastructureMentor junior members of the team and promote best practices in data engineeringWhat You\u2019ll BringBachelor's degree in Computer Science or related field, or equivalent training and experience 3+ years of experience in data engineering or a similar roleProficient in SQL, data modeling, and schema designStrong programming skills in Python, Java, or Scala; Python preferredExtensive experience with relational (e.g., PostgreSQL, MySQL) databases Experience with Snowflake (e.g., Snowflake Data Warehouse, Snowpipe, Snowflake Streams, Tasks)Experience with dbt for data transformation and modelingExperience with cloud platforms (e.g., AWS, Azure, GCP) and their data servicesExperience with data orchestration tools (e.g., Airflow, Fivetran, Luigi)Experience partnering with data analytics teams to develop data modelsSolid understanding of data warehousing concepts and data modeling techniquesExcellent problem-solving and analytical skillsStrong communication and collaboration skillsSalary InformationThe base salary range for this position in the selected city is $120,000-145,000 annually. Compensation may vary outside of this range depending on a number of factors, including a candidate\u2019s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.Work EnvironmentThe noise level in the work environment is usually moderate. While performing the duties of this Job, the employee is regularly required to sit; use hands to finger, handle, or feel and talk or hear. The employee is frequently required to reach with hands and arms. The employee is occasionally required to stand and walk. The employee must frequently lift and/or move up to 10 pounds and occasionally lift and/or move up to 25 pounds.This position is based in Everett, WA and is a hybrid schedule with 3 days onsite. What Funko OffersFunko offers a competitive compensation package with full benefits and a 401(K) plan with matching contributions from the company. Most importantly, we offer a creative work environment with people who love pop culture just as much as you do. Can\u2019t wait to gush about your latest binge? Neither can we! Looking for a place where your favorite pop culture t-shirt will receive the compliments it deserves? We know how you feel!Funko is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.\n        ",
    "location": "Burbank, CA"
  },
  "177": {
    "id": "4182386472",
    "Company": "Monogram Foods",
    "title": "Data Engineer",
    "description": "Join Our Team at Monogram Foods!Are you passionate about ensuring food safety and quality in a fast-paced, innovative environment? Do you thrive on leading transformative initiatives and inspiring teams to achieve excellence? If so, Monogram Foods invites you to embark on a rewarding career journey with us!At Monogram Foods, we\u2019re not just creating food products, we\u2019re crafting experiences that bring families together. Since 2004, we\u2019ve been at the forefront of food manufacturing, delivering exceptional products like meat snacks, appetizers, sandwiches, bacon, USDA baked goods, and more. With 12 state-of-the-art facilities across six states, we\u2019re proud to be a leader in co-manufacturing, private label and food service channels.When you join Monogram Foods, you join a family of over 4,000 talented individuals dedicated to innovation, teamwork, and excellence.Why Choose Monogram Foods?Innovation at the Core: Collaborate on cutting-edge food solutions that set industry benchmarks.Career Advancement: With our rapid growth and diverse product lines, you\u2019ll have endless opportunities to develop and excel.Culture of Collaboration: Be part of a supportive team that values your contributions and celebrates your successes.Competitive Benefits: We offer a comprehensive benefits package, including competitive pay, health coverage, retirement plans, and professional development programs.Your RoleAre you passionate about data and innovation? Monogram Foods is looking for a Data Engineer to join our IT team, supporting our co-manufacturing, private label and foodservice channels across the United States. In this role, you will build data processing pipelines and analytics solutions using Azure cloud tools, transform data, detect and extract changes and create models for business insights. This position requires collaboration with business analysts and stakeholders to meet their data needs. If you have a background in data engineering, proficiency in Azure tools and a drive for excellence, we encourage you to apply!Essential Job Duties & ResponsibilitiesIngest data from various sources using Azure Data Factory.Transform data visually with data flows or by using compute services.Monitor and optimize ETL processes and data pipelines for performance, reliability, and scalability.Collaborate with business analysts, stakeholders, and other teams to understand their data needs.Write complex SQL queries, stored procedures, and views to support data processing and reporting requirements.Migrate data and processes to Microsoft Fabric to leverage its advanced data management and analytics capabilities.Utilize Python, PySpark and other programming languages for data processing and analytics within Azure services. Education & ExperienceThe Ideal Candidate will have the Following Qualifications:The ideal candidate will have a bachelor's degree in computer science, information technology, mathematics, or a related field.3 years of experience in data engineering or analytics. Proficiency with Azure Data Factory, Azure SQL Analytics, and SQL Server is required. Experience with Microsoft Fabric is a plus. The candidate should also be skilled in programming languages such as SQL, Python, or Scala, and possess knowledge of parallel processing and data architecture patterns. Excellent communication and problem-solving skills are essential for success in this role.Must be 18 years or older.Competencies & SkillsStrong expertise in Azure Data Factory, Azure SQL Analytics, and SQL Server.Proficiency in SQL, Python, and/or Scala for data processing and analytics.Experience with Logic Apps, Azure Storage, REST APIs\n",
    "location": "Memphis, TN"
  },
  "178": {
    "id": "4187610272",
    "Company": "Hexaware Technologies",
    "title": "Data Engineer",
    "description": "What Working at Hexaware Offers:Hexaware is a dynamic and innovative IT organization committed to delivering cutting-edge solutions to our clients worldwide. We pride ourselves on fostering a collaborative and inclusive work environment where every team member is valued and empowered to succeed.Hexaware provides access to a vast array of tools that enhance, revolutionize, and advance professional profiles. We complete the circle with excellent growth opportunities, chances to collaborate with high-profile customers, opportunities to work alongside brilliant minds, and the perfect work-life balance.With an ever-expanding portfolio of capabilities, we delve deep into and identify the source of our motivation. Although technology is at the core of our solutions, it is the people and their passion that fuel Hexaware\u2019s commitment to creating smiles.\u201cAt Hexaware, we encourage individuals to challenge themselves to achieve their full potential and propel growth. We trust and empower them to disrupt the status quo and innovate for a better future. We promote an open and inspiring culture that fosters learning and brings talented, passionate, and caring people together.We are always interested in, and want to support, both the professional and personal aspects of our employees. We offer a wide array of programs to help expand skills and supercharge careers. We help discover passion, the driving force that makes one smile, innovate, create, and make a difference every day.The Hexaware Advantage: Your Workplace BenefitsExcellent health benefits with low-cost employee premiums.A wide range of voluntary benefits such as legal, identity theft, and critical care coverage.Unlimited training and upskilling opportunities through Udemy and Hexavarsity.Role - Blockchain Data Engineer (AWS, Ethereum)Location: BostonJob Mode - HybridNote : We are seeking a Blockchain Data Engineer with expertise in Ethereum and AWS to design and develop scalable blockchain data solutions.Key Responsibilities & Requirements:\u2022 Develop data pipelines to process and analyze Ethereum blockchain data (on-chain & off-chain).\u2022 Work with Ethereum node APIs, smart contracts, and transaction data for real-time and batch processing.\u2022 Leverage AWS services (S3, Glue, Redshift, DynamoDB, Lambda) to build scalable infrastructure.\u2022 Optimize data storage, indexing, and retrieval for blockchain analytics.\u2022 Proficiency in SQL, Python, Spark, and Kafka for data engineering.\u2022 Strong understanding of Web3, DeFi protocols, and smart contract interactions.\u2022 Implement event-driven architectures for blockchain data\u2022 Ensure data security, governance, and compliance best practices.\u2022 Experience with decentralized applications (dApps) and Layer 2 solutions is a plus.\u2022 Passion for blockchain technology and solving complex data challenges.Benefits offered by Hexaware:Competitive SalaryJob Description:Company Pension SchemeComprehensive Health InsuranceFlexible Work Hours and Hybrid Work OptionsPaid annual holidays + public holidays.Professional Development and Training OpportunitiesEmployee Assistance Program (EAP)Diversity, Equity, and Inclusion InitiativesCompany Events and Team-Building ActivitiesEqual Opportunities Employer: Hexaware Technologies is an equal opportunity employer. We are dedicated to providing a work environment free from discrimination and harassment. All employment decisions at Hexaware are based on business needs, job requirements, and individual qualifications. We do not discriminate based on race including colour, nationality, ethnic or national origin, religion or belief, sex, age, disability, marital status, sexual orientation, parental status, gender reassignment, or any other status protected by law. We encourage candidates of all backgrounds to apply.\n",
    "location": "Boston, MA"
  },
  "179": {
    "id": "4184461890",
    "Company": "Wound Pros Management Group, Inc.",
    "title": "Data Engineer",
    "description": "WHO ARE WE?The Wound Pros is the nation\u2019s largest wound care management company, with a presence in 19 states and counting. Our mission is to facilitate the standardization of evaluating and treating chronic wounds in long-term care facilities by leveraging the power of AI and technology. We offer various essential services, including Digital Wound Management, Telemedicine, Advanced EHR Systems, Mobile Vascular Assessment, Digital Supply Tracking, Advanced Wound Care Dressings, and participation as a Medicare Part B provider.Kickstart your career by joining a growing number of professionals committed to healing wounds and saving lives. At The Wound Pros, we live and breathe diversity. We pride ourselves in our team of passionate professionals from all over the world. Our core values are: we Listen, Innovate, Never give up, and \u2018Kultivate\u2019 & grow (LINK) people in their careers.JOB SUMMARYWe are seeking an experienced and forward-thinking Data Engineer to design, implement, and optimize our evolving data infrastructure. In this pivotal role, you will lead initiatives across AWS, Azure, and Databricks, ensuring our data ecosystems are secure, scalable, and performance-driven. You will collaborate closely with data scientists, DevOps, and software engineering teams to deliver robust data solutions that support advanced analytics, AI/ML workflows, and critical healthcare compliance requirements.WHAT ARE YOUR ASSIGNMENTSData Architecture & Pipeline DevelopmentArchitect & Implement: Design and implement scalable, multi-cloud data pipelines (AWS and Azure) that handle data ingestion, transformation, and integration across diverse sources.Data Warehouses & Data Lakes: Lead the development and maintenance of data lakehouses, warehouses, and lake architectures using platforms like Databricks (Delta Lake, Iceberg), Azure Data Lake Storage (ADLS), AWS S3, Redshift, and more.ETL/ELT Processes: Build and optimize end-to-end data pipelines using dbt, Airflow, Dagster, or similar orchestration tools to ensure reliability, consistency, and high performance.Databricks & Advanced Data SolutionsSpark Development: Leverage Databricks to manage large-scale data processing, batch/streaming jobs, and ML model deployments.Modern Table Formats: Implement and optimize Delta Lake or Iceberg for fast, ACID-compliant transactions and scalable data analytics.Collaboration & Best Practices: Promote best practices for Spark job creation, resource utilization, and distributed data processing to ensure efficient use of Databricks clusters.Cloud Infrastructure & IntegrationMulti-Cloud Expertise: Utilize services from both AWS (RDS, Lambda, Glue, S3, Redshift) and Azure (Data Factory, Data Lake Storage, Synapse) to build resilient, cost-effective data solutions.Infrastructure as Code (IaC): Work with DevOps to implement and maintain infrastructure via Terraform, CloudFormation, or ARM/Bicep templates where applicable.Cross-Functional Collaboration: Partner with DevOps to ensure robust monitoring, logging, and alerting solutions are in place (e.g., Datadog, ELK Stack, CloudWatch, Azure Monitor) for all data pipelines.Data Governance, Security & ComplianceData Governance & Quality: Develop and enforce data governance policies, standards, and frameworks\u2014ensuring high data quality, lineage, and stewardship.Healthcare Compliance: Ensure compliance with healthcare data regulations (e.g., HIPAA), implementing robust access controls, audit trails, and encryption strategies.Security Monitoring: Implement advanced security measures and monitor data access patterns, collaborating with the InfoSec team to conduct scans, penetration tests, and incident response drills.Performance Optimization & TroubleshootingData Performance: Continuously evaluate and optimize data storage/queries for performance and cost efficiency at scale (SQL tuning, partitioning strategies, caching).Monitoring & Alerting: Set up proactive alerting and monitoring systems (e.g., Datadog, Prometheus, Grafana, CloudWatch Metrics) to promptly identify and address pipeline bottlenecks and failures.Incident Management: Investigate and resolve data-related issues, working closely with DevOps, Data Science, and Software Engineering teams to minimize downtime and maintain SLAs.Technical Leadership & MentorshipTeam Mentorship: Guide junior and mid-level data engineers, providing code reviews, technical direction, and professional development opportunities.Stakeholder Communication: Work closely with Product, Analytics, and AI teams to capture requirements, translate business needs into technical solutions, and communicate progress effectively.Innovation & Thought Leadership: Evaluate emerging technologies, tools, and frameworks; recommend and implement solutions that enhance the data platform\u2019s capabilities.WHAT YOU HAVE ALREADY ACHIEVEDEducation: Bachelor\u2019s or Master\u2019s degree in Computer Science, Data Engineering, or a related field.Experience: 5+ years in data engineering roles with a proven record of leading complex data projects in both AWS and Azure environments.Cloud Expertise: Advanced proficiency in AWS (RDS, Lambda, Glue, Redshift, S3) and Azure (Data Factory, ADLS).Databricks & Spark: Hands-on experience with Databricks, Spark, and modern table formats (Delta, Iceberg).ETL/ELT & Orchestration: Strong background in building and maintaining pipelines using dbt, Airflow, Dagster, or similar tools.SQL & Python: Expert-level skills in SQL and Python for data processing, transformation, and automation.Version Control: Proficiency with Git for version control and CI/CD workflows.Data Modeling: Deep understanding of data modeling (3NF, star schema) and database design principles.Healthcare Compliance: Familiarity with HIPAA or similar regulatory frameworks, ensuring data privacy and security.Communication & Collaboration: Excellent verbal and written communication skills, with the ability to work effectively in cross-functional teams.ATTRIBUTES NEEDED FOR THE ROLEInfrastructure as Code: Experience with Terraform, CloudFormation, Azure Resource Manager (ARM) templates, or AWS CDK.AI/ML Pipelines: Exposure to AI/ML workflows, model training, and model hosting on Databricks or other platforms.Containerization & Orchestration: Familiarity with Docker, Kubernetes, or similar technologies for packaging and deploying data applications.Performance Tuning: Experience optimizing large-scale data warehouse/lake architectures for cost, speed, and reliability.Mentorship & Leadership: Previous experience in a senior or lead capacity, mentoring junior engineers and driving team-wide best practices.EQUAL OPPORTUNITY EMPLOYERThe Wound Pros is an equal opportunity employer committed to being an employer of choice, not just an excellent place to work but a great and inclusive place to work. We strive to recruit and maintain a diverse workforce. Qualified applicants will receive consideration for employment without regard to race, physical or mental disability, color, religious creed, ancestry, national origin, religion, age, sex, marital status, genetic information or testing, gender identity and expression, sexual orientation or status as a Vietnam-era or special disabled veteran or any characteristic protected by law.\n",
    "location": "United States"
  },
  "180": {
    "id": "4191396479",
    "Company": "TieTalent",
    "title": "Junior Data Engineer",
    "description": "AboutWork in an exciting and agile environment where you will have the opportunity to collaborate with a team of developers to shape our technical runway.At Macquarie, our advantage is bringing together diverse people and empowering them to shape all kinds of possibilities. We are a global financial services group operating in 34 markets and with 55 years of unbroken profitability. You'll be part of a friendly and supportive team where everyone - no matter what role - contributes ideas and drives outcomes.What role will you play?Press space or enter keys to toggle section visibility Working with senior engineers, data analysts, and business users you would be building new pipelines and technical solutions that provide data to our data platform and enable reporting and analytics. You would be responsible for supporting existing pipelines and working with central platform teams to resolve errors and to implement new features.What you offerPress space or enter keys to toggle section visibility Engineering skills with the ability to work in Python using AWS to build data pipelines and microservices Understanding of design patterns, testing methodologies and overall SDLC Comfortable proposing designs, gathering feedback from others, building solutions with reusability in mind and documenting / explaining how the solution works Comprehension of security, including authentication and authorisation Familiarity with Linux, Shell Scripting, load balancing and basic networking We love hearing from anyone inspired to build a better future with us, if you're excited about the role or working at Macquarie we encourage you to apply.About TechnologyPress space or enter keys to toggle section visibility Technology enables every aspect of Macquarie, for our people, our customers and our communities. We're a global team that is passionate about accelerating the digital enterprise, connecting people and data, building platforms and applications and designing tomorrow's technology solutions.BenefitsPress space or enter keys to toggle section visibility Macquarie employees can access a wide range of benefits which, depending on eligibility criteria, include:Hybrid and flexible working arrangements One wellbeing leave day per year and minimum 25 days of annual leave Primary caregivers are eligible for 20 weeks paid leave along with 12 days of transition leave upon return to work and 6 weeks paid leave for secondary caregivers Paid volunteer leave and donation matching Range of benefits to support your physical, psychological and financial wellbeing Employee Assistance Program, a robust behavioural health network with counselling and coaching services Recognition and service awardsOur commitment to diversity, equity and inclusionPress space or enter keys to toggle section visibility We are committed to providing a working environment that embraces diversity, equity and inclusion. As an inclusive employer, Macquarie provides equal opportunities to all individuals regardless of race, color, religion, sex, sexual orientation, national origin, age, disability, protected veteran status, genetic information, marital status, gender identity or any other impermissible criterion or circumstance. Our aim is to provide reasonable accommodations to individuals who may need support during the recruitment process and through working arrangements. If you require additional assistance, please let us know during the application process.Nice-to-have skillsPythonAWSLinuxShell ScriptingLoad BalancingNetworkingMicroservicesPennsylvania, United StatesWork experienceData EngineerData InfrastructureLanguagesEnglish\n",
    "location": "Pennsylvania, United States"
  },
  "181": {
    "id": "4190689750",
    "Company": "UnitedHealthcare",
    "title": "Data Engineer - Remote",
    "description": "At UnitedHealthcare, we\u2019re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us to start Caring. Connecting. Growing together. Position in this function design, develops, implements and run data solutions that improve data efficiency, reliability and quality; Design, develop, implement and run cross-domain, modular, optimized, flexible, scalable, secure, reliable and quality data solutions that transform data for meaningful analyses and reporting; Build processes to troubleshoot, maintain and optimize solutions and respond to customer and production issues; Maintain high quality documentation of data definitions, transformations, and processes to ensure data governance and security; Identify solutions to non-standard requests and problems. You'll work closely with business leaders to develop innovative analytic solutions, create actionable insights and drive better decisions and performance to help Government Operation fulfill its mission.You\u2019ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.Primary ResponsibilitiesDevelop innovative analytic solutions, create actionable insights and drive better decisions and performance to help the business achieve its business objectives Support operational analytics, reporting and process improvement data deep dive, identify and execute on new opportunities for increases in efficiencies and performance Data transformation to support front-end reporting analytics, along with ability to understand and utilize ETL \u2018back end\u2019 data transformation, development, and configuration Analyze technical and domain related issues in live applications, support business queries and questions related to application output and business logic Data quality -\u202fdevelop QA processes; create validation and test scenarios and datasets; validate results against data sources and expected business trends; root cause analysis into why trends differ; test &\u202fQA data loading, ETL, and reporting processes Develop visionary and creative analytic solutions to measure business performance; identifies / quantifies drivers, risks and opportunities and defines solutions to mitigate risks and leverage opportunities Influences senior leadership to adopt new ideas, projects and / or approaches CompetenciesSelf-starter with solid influencing and facilitation skills that will quickly develop relationships across the organization while building a collaborative work environmentAnalytic thought leadership with skills in developing innovative and creative analytic solutions to complex problems Solid business acumen and critical thinking ability Solid problem-solving skills with the ability to anticipate, identify and diagnose problems and make recommendations Candidate must be goal-directed, persistent and driven to achieve positive resultsYou\u2019ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.Required Qualifications3+ years of experience with relational databases, including MS SQL Server, Oracle, Teradata, IBM DB2, or MySQL3+ years in reporting and analysis, data management, and reconciliation3+ years of experience with ETL/Data Warehouse concepts, handling large volumes of data, and using ETL tools2+ years working with Cloud TechnologiesExperience with data visualization, utilizing tools like Tableau or PowerBIProven solid ability to analyze and comprehend data within its business context, map or extend existing data models, and design new structuresProven track record of deriving successful business solutions from analysis and deep learningPreferred QualificationsExperience with advanced analytical tools (Python / R / Tiger Graph) Experience with Healthcare Exchange / Marketplace / Medicaid / Medicare Experience in data process map, workflow, user acceptance testing, and database management All employees working remotely will be required to adhere to UnitedHealth Group\u2019s Telecommuter PolicyThe salary range for this role is $71,600 to $140,600 annually based on full-time employment. Pay is based on several factors including but not limited to local labor markets, education, work experience, certifications, etc. UnitedHealth Group complies with all minimum wage laws as applicable. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you\u2019ll find a far-reaching choice of benefits and incentives.Application Deadline: This will be posted for a minimum of 2 business days or until a sufficient candidate pool has been collected. Job posting may come down early due to volume of applicants.At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone-of every race, gender, sexuality, age, location and income-deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes - an enterprise priority reflected in our mission.Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.UnitedHealth Group is a drug-free workplace. Candidates are required to pass a drug test before beginning employment.\n",
    "location": "Minnetonka, MN"
  },
  "182": {
    "id": "4189814534",
    "Company": "PFSbrands",
    "title": "Data Engineer",
    "description": "DescriptionTHIS POSITION IS NOT ELIGIBLE FOR SPONSORSHIP.Data EngineerReports to: Manager, Data and ReportingFLSA Status: ExemptAs the Database Engineer you will be vital in designing, developing, and managing our cloud-based data platform using Snowflake. You\u2019ll also collaborate closely with our analytics and operations teams to ensure seamless data flow across tools like Matillion and Power BI. This is a perfect opportunity for a recent graduate or someone with up to 2 years of experience in data management to learn and grow with us. We\u2019re seeking someone passionate about learning and a continuous improvement mindset.Essential Duties And ResponsibilitiesDatabase Design & Architecture: Assist in designing scalable and optimized Snowflake data structures to support re- porting, analytics, and business operations. You will be accountable for documentation and systems maintenance and work with other managers on data governance and security.Database Administration: Manage, monitor, and maintain database performance, security, and availability. Handle user roles and permissions within Snowflake.ELT Processes: Work with Matillion to manage Extract, Load, Transform (ELT) workflows, ensuring efficient data movement and transformation. Other tools may become part of our data stack as needed. You will be expected to become an expert user of them and will be involved in the vetting process for onboarding any new BI software.Data Integration: Collaborate with various teams to ensure timely, accurate data integration from multiple sources.Reporting & Analytics Support: support analysts and business teams by ensuring Power BI dashboards are fed with accurate and up-to-date data.Process Improvement: Proactively identify opportunities to enhance data quality, performance, and process efficiency.Documentation: Maintain detailed documentation of processes, data models, and architecture decisions.Support & Troubleshooting: Assist with troubleshooting and resolving data-related issues quickly to support business operations. Maintain system performance levels to ensure adherence to performance criteria approved by review and senior management.Embrace the PFSbrands\u2019 mission, work toward our vision statement, and live according to our core values.Perform all other as requested or assignedQualificationsTo perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, experience and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.0-2 years of experience in database architecture, administration, or data managementBS/BA in computer science, information systems, data analytics, or a related fieldSkilled in SQL and cloud-based databases (Snowflake preferred).Familiarity with data integration or ETL/ELT tools (Matillion a plus).Basic understanding of data visualization tools (Power BI preferred) and database schema.Proficient with data analysisStrong problem-solving skills and attention to detailWillingness to learn and grow in a fast-paced environmentEffective communication and collaboration abilitiesAbility to quickly process information and make decisionsExcellent time-management with the ability to meet tight deadlinesDetail-oriented and excellent follow throughMust be dependable and able to work independently with little supervision as well as with a team of individuals.WORKING CONDITIONSWork is typically performed in normal office conditions. Position requires occasional overnight travel.PHYSICAL DEMANDSThe physical demands described here are representative of those that must be met by an employee To successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.While performing the duties of this job, the employee is required to use hands to fingers, reach, climb stairs, walk, sit for long periods of time, talk and hear.Must be able to sit for prolonged periods of time in front of a computer daily.Must have visual and hearing acuity.Digital dexterity and hand/eye coordination in operation of office equipmentAbility to speak to and hear customers and/or employees via phone and in personAbility to use hands and fingers to reach, lift and carry 25 pounds on an infrequent basis\n",
    "location": "Holts Summit, MO"
  },
  "183": {
    "id": "4187114376",
    "Company": "Electric Power Engineers",
    "title": "Data Engineer",
    "description": "Overview We are designing the grid of the future! Be a part of an innovative team shaping the grid of the future through advanced energy intelligence  . For more than half a century, Electric Power Engineers, (EPE) has partnered with power and energy clients across the globe providing consulting expertise and energy intelligence software solutions for complex engineering and grid modeling challenges. As leaders in the renewables space, we are focused on building a modern, secure, and resilient gid. Join us in making an impact on the communities we serve and the environment in which we live. Together we can transform the future of energy.Responsibilities Join us in leading the change!Our software team develops and deploy innovative cloud-based analytics and engineering products to optimize the energy sector. We are seeking a skilled Data Engineer to join our data engineering team. In this role, you will design, develop, and maintain data pipelines and infrastructure that support our organization's analytical and operational needs. You will work under the guidance of the Principal Data Engineer to implement efficient data solutions while ensuring data quality, reliability, and accessibility.How You Can Make An Impact Build and maintain scalable data pipelines for ingestion, transformation, and delivery of data  Implement data models and schemas based on business requirements  Collaborate with data scientists, analysts, and other stakeholders to understand data needs  Write efficient, maintainable code for data transformation and processing  Implement data quality checks and validation procedures  Contribute to the optimization of existing data workflows  Participate in code reviews and knowledge sharing sessions  Assist in troubleshooting data issues and implementing solutions  Document data processes, architectures, and workflows  Prepare curated datasets for analytics use cases  Maintain running pipelines to ensure data availability for downstream systems and teams Qualifications Bring your passion, here's what\u2019s needed:  3+ years of experience in data engineering or related field  Experience with ETL/ELT processes and tools (e.g. Glue, Glue Catalog, dbt)  Familiarity with cloud-based data platforms (AWS, Azure, or GCP)  Experience with data orchestration tools (Airflow, Dagster, Prefect, etc.)  Proficiency in Python or another programming language used for data processing  Strong SQL skills and experience with relational databases  Knowledge of big data technologies (e.g. Spark, Athena, Snowflake DB)  Knowledge of integration with downstream analytics tools (e.g. Tableau, PowerBI,  Knowledge of data warehousing and data lake concepts and best practices  Experience with NoSQL databases  Understanding of data governance practices  Exposure to CI/CD pipelines for data workflows  Knowledge of infrastructure as code concepts  Exposure to Data Warehouse and Data Lake concepts  Experience with containerization (Docker)  Experience with version control systems (Git)  Understanding of data quality principles  Effective communication and collaboration abilitiesPreferred Qualifications Experience in the power and energy industry, working with SCADA, AMI, EMS, and grid modeling datasets. Expertise in AI/ML-driven energy forecasting, anomaly detection, and optimization models. Knowledge of geospatial data (GIS) for grid planning and renewable energy analytics. Experience with data governance frameworks, compliance (FERC, NERC), and regulatory reporting. How We Support You Comprehensive health and wellness benefits including medical, dental, and vision with 100% premium coverage for you.  Generous PTO and paid holidays  Work with industry leaders  401K, up to a 4% match (100% vested from day 1)  Flexible Work including hybrid and remote possibilities based on position.  Location  : This position is open to remote Travel  : Occasional travel may be needed (10 % or less )EPE is an equal opportunity/AA/Disability/Veteran employer. The EEO is the Law poster, and its supplement are available using the following links: EEOC is the Law PosterIf you are interested in applying for employment with EPE and need special assistance to apply for a posted position, please send an email to: applicationassistance@epeconsulting.com Third-Party Recruiting Notification EPE does not accept unsolicited resumes from third-party recruiters. Any unsolicited third-party resumes forwarded by recruiters to EPE via our career page or to any of our managers or employees will be considered public information, may be treated as a direct application from the person identified in the resume, and will not be eligible for placement fee payment to the agency. EPE will not pay a fee to a third-party recruiter or agency without a previously signed third-party agreement and has not coordinated their recruiting activity with the appropriate member of the Talent Acquisition team.\n",
    "location": "Austin, TX"
  },
  "184": {
    "id": "4184710743",
    "Company": "Raft",
    "title": "Data Engineer",
    "description": "This is a U.S. based position. All of the programs we support require U.S. citizenship to be eligible for employment. All work must be conducted within the continental U.S.Who we are:Raft (https://teamraft.com) is a customer-obsessed non-traditional small business with a purposeful focus on Distributed Data Systems, Platforms at Scale, and Complex Application Development, with headquarters in McLean, VA. Our range of clients include innovative federal and public agencies leveraging design thinking, cutting edge tech stack, and cloud native ecosystem. We build digital solutions that impact the lives of millions of Americans.About the role:We\u2019re looking for an experienced Data Engineer to support our customers and join our passionate team of high-impact problem solvers. A successful candidate should have a diverse technical background and be leader and problem solver with a proven ability to deliver superior results as part of a high performing team in an Agile, fast-paced environment.What you will do:Assembles large, complex sets of data that meet non-functional and functional requirementsIdentifies, designs, and implements internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processesBuilds required infrastructure for optimal extraction, transformation and loading of data from various data sourcesCurate and maintain data that is stored in support of metrics and evaluation.Working with developers to make sure key data points are stored in a Postgres database.Working with data providers and back-end API developers to normalize schemas and define data flows for persistence and analytics.Design, build, test, maintain, and automate data collection/pipeline.Develop Dashboards for Data Analytics.Implement Artificial Intelligence/Machine LearningUsing Agile methodologies to develop software.What we are looking for:Bachelor of Science or higher in Computer Science or related field.Minimum three (3) years of relevant experience as a Data Engineer/Scientist.Experience developing data pipelines and normalizing data.Experience using AWS cloud services.Experience with PostgreSQL database.Ability to work efficiently on small, distributed teams.Familiarity with Agile principles.Highly preferred:Experience developing Tableau Dashboards.ELK Stack Experience.Real-time event streaming.AWS S3, EC2, Lambda, Kinesis.Apache Spark, NiFi.Confluent Kafka.Experience with Containers (Docker, Docker Compose).DevSecOps utilizing test-driven development.Gitlab experience.Clearance Requirements:Active DoD Secret Clearance.Top Secret (TS) security clearance (with SCI and SAP eligibility) preferredWork Type: Hybrid in Colorado SpringsMay require up to 10% travelSalary Range:$125,000 - $158,400 The determination of compensation is predicated upon a candidate's comprehensive experience, demonstrated skill, and proven abilitieWhat we will offer you: Highly competitive salary Fully covered healthcare, dental, and vision coverage 401(k) and company match Take as you need PTO + 11 paid holidays Education & training benefits Annual budget for your tech/gadgets needsTeam off-site in fun places! Generous Referral Bonuses And More!Our Vision Statement: We bridge the gap between humans and data through radical transparency and our obsession with the mission.Our Customer Obsession:We will approach every deliverable like it's a product. We will adopt a customer-obsessed mentality. As we grow, and our footprint becomes larger, teams and employees will treat each other not only as teammates but customers. We must live the customer-obsessed mindset, always. This will help us scale and it will translate to the interactions that our Rafters have with their clients and other product teams that they integrate with. Our culture will enable our success and set us apart from other companies.How do we get there?Public-sector modernization is critical for us to live in a better world. We, at Raft, want to innovate and solve complex problems. And, if we are successful, our generation and the ones that follow us will live in a delightful, efficient, and accessible world where out-of-box thinking, and collaboration is a norm.Raft\u2019s core philosophy is Ubuntu: I Am, Because We are. We support our \u201cnadi\u201d by elevating the other Rafters. We work as a hyper collaborative team where each team member brings a unique perspective, adding value that did not exist before. People make Raft special. We celebrate each other and our cognitive and cultural diversity. We are devoted to our practice of innovation and collaboration.We\u2019re an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.We\u2019re an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.\n",
    "location": "Colorado Springs, CO"
  },
  "185": {
    "id": "4136877147",
    "Company": "MHK",
    "title": "Data Engineer",
    "description": "Job DescriptionThe Data Engineer will focus on the development, implementation, and maintenance of our data warehouse using Amazon Redshift. The ideal candidate will have a strong background in data engineering, experience with AWS infrastructure, and expertise in managing data sourced from structured and unstructured data sources. Additionally, you will be responsible for creating insightful reports using Amazon QuickSight.Key Responsibilities:Design, develop, and maintain robust data warehouse solutions utilizing Amazon Redshift.Collaborate with cross-functional teams to gather data requirements and ensure seamless data integration from databases, external files, and other structured and unstructured data sources.Define, create, and optimize ETL processes to enhance data extraction, transformation, and loading using AWS services.Maintain data integrity and ensure high availability and performance of data storage systems.Implement data governance practices to ensure compliance and security of data assets.Develop and maintain data pipelines and workflows using AWS tools such as Glue, Lambda, Step Functions, Athena, DMS, Redshift, and others.Utilize Amazon QuickSight to design, develop, and publish dynamic reports and dashboards for business analytics.Perform data analysis and validation to ensure data accuracy and consistency across various data sources.Troubleshoot and resolve complex data processing issues in a timely manner.Stay current with industry trends and best practices related to data engineering and AWS technologies.Skill Requirements: Minimum of 5 years\u2019 experience as a Data Engineer, with a focus on data warehouse architecture and development.Proficiency in SQL and experience with AWS services including Redshift, S3, and QuickSight is required.Familiarity with data extraction and transformation using MySQL databases and Parquet files.Strong understanding of data modeling, database management, and data warehousing concepts.Experience with ETL tools and frameworks.Ability to work collaboratively in a team environment and communicate effectively.Strong problem-solving skills and attention to detail.Education/Certification Requirements: Bachelor\u2019s degree in Computer Science, Information Technology, or a related field.  AWS Certified Data Analytics or AWS Certified Solutions Architect certification preferred. Additional Requirements:Experience with big data tools and technologies.Familiarity with data visualization and reporting tools.Ability to work across all time zones as needed and after hours on Deployments Prospective candidates must be able to pass a background check and drug screening. At MHK we help health plans and pharmacy benefit managers deliver optimal care management across every member\u2019s health journey. We do this through state-of-the-art technology that provides critical insights from member enrollment and maintenance through every stage of care and compliance. COMMITMENT TO DIVERSITY & INCLUSION:We are committed to cultivating and preserving a culture of inclusion and connectedness through collaboration. We grow and learn better together with a diverse team of employees. We welcome the unique contributions and the different perspectives of a diverse team. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.If you need a reasonable accommodation for any part of the employment process, please contact us by email at careers@mhk.com and let us know the nature of your request and your contact information. Requests for accommodation will be considered on a case-by-case basis. Please note that only inquiries concerning a request for reasonable accommodation will be responded to from this email address.For more information, please visit  Know Your Rights ,  Pay Transparency , and  MHK EEO/AA StatementBenefits Snapshot:Medical, vision, and dental plans for full time employees401(k) offered with a generous matchBenefits begin on first day of the month following employment Exercise/Health Club reimbursement opportunity Monthly dependent care reimbursement opportunity Short Term and Long Term disability Basic Term Life and AD&D InsurancePaid Time Off17 days Paid Time Off 15 Company Paid Holidays 2 Community Service days\n",
    "location": "Tampa, FL"
  },
  "186": {
    "id": "4065665156",
    "Company": "Sweetspot (YC S23)",
    "title": "Backend/Data Engineer",
    "description": "As a Backend Engineer at Sweetspot, you will join a team dedicated to pushing the boundaries of LLM usage over disparate and unstructured data sources across United States federal and state governments. You will play a critical role in building and integrating data pipelines that feed directly into a framework built for the aforementioned. More broadly, you will join a team that is highly focused on building a product that bolsters the American economy and provides every business, regardless of size, the tools they need to secure government opportunities.Please note that this is role is only open to US citizens.Things You Should Know About UsWe work extensively with industry experts to build software that is useful for our clients. This means frequent customer calls and a constant level of iteration.Talent density is far more important than team size. We want to hire the absolute best, and are willing to wait to find the right person.If you\u2019re early in your career and/or don\u2019t come from a \u201cprestigious\u201d background, don\u2019t be afraid to apply - we all went to a state school and don\u2019t place a large emphasis on prestige.Our office is in Newport/Jersey City.What You\u2019ll DoDesign, develop, and maintain scalable and efficient data pipelines to process and transform large volumes of data from disparate sources across the internetImplement and optimize data pipelines using Python/SQLAlchemy and Dagster, ensuring high performance, reliability, and scalabilityWork with PostgreSQL databases, including design, optimization, and troubleshootingCollaborate with teammates to build and maintain software that can query hundreds of millions of vectors in a single requestParticipate in code reviews, testing, and deployment processes to ensure code quality and maintainabilityMonitor and troubleshoot data infrastructure to identify and resolve issues proactivelyStay up-to-date with industry trends and best practices in data engineering and incorporate them into your workRequirementsBachelor's degree in Computer Science, Engineering, or a related fieldStrong proficiency in Python programming languageSolid experience with PostgreSQL databases, including writing efficient queries, performance tuning, and database administrationFamiliarity with data pipeline frameworks such as Dagster, Airflow, Luigi, or similarKnowledge of data processing and transformation techniques, including ETL processesExperience with GitExperience with DockerExcellent problem-solving skills and ability to debug complex systemsStrong communication and collaboration skills, with the ability to work effectively in a team environmentUS citizenPreferred QualificationsExperience with cloud platforms such as AWS, GCP, or AzureExperience with data engineering best practicesExperience with FastAPI and asynchronous programmingExperience with SQLAlchemyExperience with data warehousing and business intelligence toolsWe offer a competitive salary, comprehensive benefits package, and opportunities for growth and development within a fast-paced and innovative company.If you are passionate data engineering, have a strong foundation in software development, and are interested in contributing to the growth of the American economy, we would love to hear from you. Sweetspot Chat, Inc. is an equal opportunity employer and values diversity in the workplace. We encourage all qualified applicants to apply.\n        ",
    "location": "New York, NY"
  },
  "187": {
    "id": "4158264444",
    "Company": "Skanska",
    "title": "Data Analytics Engineer",
    "description": "Skanska is searching for a dynamic Data Analytics Engineer. This is a great opportunity to start a career with a company that builds things that matter and values its team. We are proud to share our culture of diversity and inclusion.Our work makes a clear contribution to society and the environment around us. We build in many different verticals. Whether we are building schools to provide inspiring spaces for learning, roads to connect communities, or hospitals to care for patients, it all contributes to our purpose - we build for a better society.Skanska's values \u2014Be Better Together, Act Ethically and Transparently, Commit to Customer, and Care for Life\u2014are deeply engrained in how we work, which is why our values support and drive our D&I efforts.The Data Analytics Engineer plays a crucial role in designing, implementing, and supporting enterprise data initiatives and analyses. The Data Analytics Engineer focuses on creating intuitive data models and solutions that enable business partners to easily understand and leverage high-quality, well-structured data. The ideal candidate will collaborate with data analysts and engineers to develop visual representations of data patterns and insights, effectively communicating their significance to internal and external stakeholders, colleagues, and end-users.Data Analytics Engineer Required Qualifications:3+ years of experience data modeling in data warehouses with modern design patterns. 3+ years of experience with Microsoft Azure, including key components like Azure Data Lake, Azure Data Factory, Azure Blob Storage, Azure Synapse and Azure SQL3+ years of experience with Power BI, including report building, DAX functions, and visualization best practices. Bachelor\u2019s Degree - Computer Science, Data Engineering, or a related field or 8 years equivalent experience plus minimum 5 years prior relevant experience.Our Investment in you:We believe that Benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That\u2019s why we provide an array of options (including medical, dental, and vision insurance plans), expert guidance, and always-on tools, that are personalized to meet the needs of your reality - to help support you physically, financially, and emotionally through the big milestones and in your everyday life. Please visit the compensation and Benefits summary on our careers site for more details.As a Skanska community, our values ground us, and our diversity of experience propels us forward. No matter what your career stage, there\u2019s a place for you to thrive here and partner with us in shaping how our world lives, moves, and connects.At Skanska, we Care for Life. And we\u2019re committed to supporting your whole health and peace of mind through inclusive and personalized total rewards.We\u2019re committed to your success by developing you in your role and supporting your career growthCompensation and financial well-being - Competitive base salary, excellent bonus program, 401k, & Employee ownership program. Come work with us and join a winning team!\n",
    "location": "Charlotte, NC"
  },
  "188": {
    "id": "4174499945",
    "Company": "Twitch",
    "title": "Data Engineer - People Analytics",
    "description": "About UsTwitch is the world\u2019s biggest live streaming service, with global communities built around gaming, entertainment, music, sports, cooking, and more. It is where thousands of communities come together for whatever, every day.We\u2019re about community, inside and out. You\u2019ll find coworkers who are eager to team up, collaborate, and smash (or elegantly solve) problems together. We\u2019re on a quest to empower live communities, so if this sounds good to you, see what we\u2019re up to on LinkedIn and X, and discover the projects we\u2019re solving on our Blog. Be sure to explore our Interviewing Guide to learn how to ace our interview process.About The RoleAs the Data Engineer in the People & Places organization, you will be responsible for building and managing the core data pipelines and analytics tooling that power critical HR reporting functionality. This highly sensitive work requires the utmost care and discretion to ensure the security and integrity of confidential employee data as you develop the systems that HR relies on for important decision-making. Your role will involve partnering closely with an Analyst and Program Manager to design, implement, and optimize a robust data infrastructure to support the organization's talent management and workforce analytics initiatives.We are a distributed HR team with team members all over the country. You can work from Twitch\u2019s headquarters in San Francisco, CA; or from one of our hub locations in Seattle, WA; Irvine, CA; or New York City, NY.You Will:Develop and manage scalable data pipelines to extract, transform, and load HR data from various sourcesEnsure the security, reliability, and performance of the data infrastructureCollaborate with stakeholders to understand business requirements and translate them into data-driven solutionsAutomate data processing and reporting workflows to improve efficiency and data integrityImplement data quality checks and monitor processes to maintain high data accuracyYou Have:2+ years of experience working in a data engineering or software engineering capacityExtensive experience in developing and optimizing data pipelines using PythonProficient in working with AWS services such as EMR, Redshift, Glue, and S3Demonstrates expertise in schema management, data lineage, and auditability, coupled with a strong command of data modeling, ETL processes, and data warehousing principlesProven experience implementing data integrity practices in ETL/ELT pipelines, ensuring accuracy, consistency, and reliabilityPerksMedical, Dental, Vision & Disability Insurance401(k)Maternity & Parental LeaveFlexible PTOAmazon Employee DiscountWe are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.Twitch values your privacy. Please consult our Candidate Privacy Notice, for information about how we collect, use, and disclose personal information of our candidates.Job ID: TW8747Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from our lowest geographic market up to our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. This position will remain open until filled. For more information, please visit https://www.twitch.tv/jobs/en/#learn-more. Applicants should apply via our internal or external career site.US Pay Per Year$118,900\u2014$205,600 USD\n        ",
    "location": "San Francisco, CA"
  },
  "189": {
    "id": "4117851034",
    "Company": "INVISTA",
    "title": "Data Engineer",
    "description": "Your JobAs a Data Engineer at INVISTA, you will be a vital contributor to our Enterprise Data Platform, responsible for designing and implementing data pipelines, optimizing data workflows, and ensuring data reliability and accessibility. Your work will be instrumental in empowering our organization to make data-driven decisions and fueling innovation across the company.Our TeamJoining our Data and Analytics team means becoming a key player in a dynamic and innovative group of professionals dedicated to unlocking the power of data. Here, you'll find a diverse and collaborative environment where your ideas and expertise will shape the future of our data-driven organization. We pride ourselves on fostering a culture of continuous learning, creativity, and teamwork, and we're looking for individuals who are eager to contribute their skills and passion to our shared mission.This role may work a hybrid schedule out of our Wichita (KS) or Katy (TX) office location.What You Will DoCollaborate closely with business partners and data scientists to align data engineering efforts with their specific needs and objectives, ensuring that data solutions contribute effectively to the organization's overall goals. This includes proposing alternative solutions, with their pros and cons, to help guide the workstream forward. Understand, develop, maintain/troubleshoot, automate, and optimize orchestration and data pipelines using tools such as Snowflake, dbt, GitHub, AWS, Power BI, and Neo4J. Implement data integration strategies for streaming, event driven, and batch data sources. Design and implement data modeling and ETL/ELT processes to ensure data quality, consistency, availability and data centricity.Manage and maintain data warehouses and ensure data security and compliance with company policies and relevant regulations.Perform data transformations, aggregations, and data cleansing to support analytics and reporting needs.Document data engineering processes, pipelines, and architecture for knowledge sharing and compliance.Stay up to date with industry best practices, emerging technologies, and trends in data engineering.Who You Are (Basic Qualifications)Experience in data modeling with SQLExperience collaborating with a Finance team or in the Financial Services industryExperience programming in PythonAbility to travel up to 20% of the timeThis role is not eligible for visa sponsorshipWhat Will Put You AheadExperience with Snowflake data warehouse technologyProficiency in designing software and data solutions that scaleProficiency in Power BI, strong in DAX and analytics report buildingHands-on experience with AWS cloud computing platformSAP Financial domain knowledge \u2013 Financial accounting and controlling modulesExperience with DevOps practices and tools, such as continuous integration (CI) and continuous deployment (CD) pipelines, version control systems (e.g., Git)Knowledge of data governance and compliance best practicesAt Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate's knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.Hiring PhilosophyAll Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here.Who We AreAs a Koch company, INVISTA has a long history of working to make the world around you a better place. From parts for the automotive industry to medical equipment, air bags, food packaging and clothing, our ingredients in the nylon 6,6 and polypropylene value chains help bring many of life\u2019s essential products to market.At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.Our BenefitsOur goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength - focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes - medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.Additionally, everyone has individual work and personal needs. We seek to enable the best work environment that helps you and the business work together to produce superior results.Equal OpportunitiesEqual Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, some offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please click here for additional information. (For Illinois E-Verify information click here, aqu\u00ed, or tu).\n",
    "location": "Katy, TX"
  },
  "190": {
    "id": "4183445245",
    "Company": "Travelers",
    "title": "Data Engineer I (AWS/Snowflake/Databricks/Abinitio)",
    "description": "Who Are We?Taking care of our customers, our communities and each other. That\u2019s the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.Compensation OverviewThe annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.Salary Range$106,300.00 - $175,400.00Target Openings1What Is the Opportunity?Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate the stories found in data. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Artificial Intelligence, Machine Learning and business intelligence/insights.What Will You Do?Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.Design data solutions.Analyze sources to determine value and recommend data to include in analytical processes.Incorporate core data management competencies including data governance, data security and data quality.Collaborate within and across teams to support delivery and educate end users on data products/analytic environment.Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate.Test data movement, transformation code, and data components.Perform other duties as assigned. What Will Our Ideal Candidate Have?Bachelor\u2019s Degree in STEM related field or equivalentSix years of related experienceDemonstrated proficiency in utilizing tools and techniques on AWS cloud platforms, including Databricks, and Snowflake, as well as programming languages such as SQL and Python, along with a solid understanding of data engineering practices. Familiarity with Abinitio, Terraform, PySpark, GitHub and Jenkins is also preferred.The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions.Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on.Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems.Strong verbal and written communication skills with the ability to interact with team members and business partners.Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities.What is a Must Have?Bachelor\u2019s degree or equivalent training with data tools, techniques, and manipulation.Four years of data engineering or equivalent experience.What Is in It for You?Health Insurance: Employees and their eligible family members \u2013 including spouses, domestic partners, and children \u2013 are eligible for coverage from the first day of employment.Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.Wellness Program: The Travelers wellness program is comprised of tools, discounts and resources that empower you to achieve your wellness goals and caregiving needs. In addition, our mental health program provides access to free professional counseling services, health coaching and other resources to support your daily life needs.Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.Employment PracticesTravelers is an equal opportunity employer. We value the unique abilities and talents each individual brings to our organization and recognize that we benefit in numerous ways from our differences.In accordance with local law, candidates seeking employment in Colorado are not required to disclose dates of attendance at or graduation from educational institutions.If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email so we may assist you.Travelers reserves the right to fill this position at a level above or below the level included in this posting.To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/.\n        ",
    "location": "Hartford, CT"
  },
  "191": {
    "id": "4107198182",
    "Company": "Amazon",
    "title": "Data Engineer",
    "description": "DescriptionHundreds of millions of customers, billions of transactions, petabytes of data\u2026 How to use the world\u2019s richest collection of e-commerce data to provide superior value and better paying experience to customers ? The Amazon Payments Team manages all Amazon branded payment offerings globally. These offerings are growing rapidly and we are continuously adding new market-leading features and launching new products. Amazon.com has a culture of data-driven decision-making and demands business intelligence that is timely, accurate, and actionable. This team provides a fast-paced environment where every day brings new challenges and new opportunities.Our team of high caliber software developers, data scientists, statisticians and product managers use rigorous quantitative approaches to ensure that we target the right product to the right customer at the right moment, managing tradeoffs between click through rate, approval rates and lifetime value. In order to accomplish this we leverage the wealth of Amazon\u2019s information to build a wide range of probabilistic models, set up experiments that ensure that we are thriving to reach global optimums and leverage Amazon\u2019s technological infrastructure to display the right offerings in real time.As a Data Engineer you will be working in one of the world's largest and most complex data warehouse environments. You should be passionate about working with huge data sets and be someone who loves to bring datasets together to answer business questions. You should have deep expertise in creation and management of datasets. You will build data analytical solutions that will address increasingly complex business questions.You should be expert at implementing and operating stable, scalable data flow solutions from production systems into end-user facing applications/reports. These solutions will be fault tolerant, self-healing and adaptive. You will be working on developing solutions that provide some of the unique challenges of space, size and speed. You will implement data analytics using cutting edge analytics patterns and technologies that are inclusive of but not limited to various AWS Offerings - EMR, Lambda, Kinesis, and Spectrum. You will extract huge volumes of structured and unstructured data from various sources (Relational /Non-relational/No-SQL database) and message streams and construct complex analyses. You will write scalable code and tune performance running over billion of rows of data. You will implement data flow solutions that process data on Spark, Redshift and store in both Redshift and File based storage (S3) for reporting and adhoc analysis.You should be detail-oriented and must have an aptitude for solving unstructured problems. You should work in a self-directed environment, own tasks and drive them to completion.You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions and to build data sets that answer those questions. You own customer relationship about data and execute tasks that are manifestations of such ownership, like ensuring high data availability, low latency, documenting data details and transformations and handling user notifications and training.Basic Qualifications 3+ years of data engineering experience Experience with SQL Experience with data modeling, warehousing and building ETL pipelines Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Experience with Redshift, Oracle, NoSQL etc. Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets Exposure to large databases, BI applications, data quality and performance tuningPreferred Qualifications Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases) Experience providing technical leadership and mentoring other engineers for best practices on data engineering Experience with distributed systems as it pertains to data storage and computing Experience partnering with business owners directly to understand their requirements and provide data which can help them observe patterns and spot anomalies. Practical Knowledge of Linux or Unix shell scripting. Knowledge of software engineering best practices across the development life cycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status.Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you\u2019re applying in isn\u2019t listed, please contact your Recruiting Partner.Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.Company - Amazon.com Services LLCJob ID: A2860213\n        ",
    "location": "Seattle, WA"
  },
  "192": {
    "id": "4186433109",
    "Company": "Jobs via Dice",
    "title": "Data Engineer",
    "description": "Dice is the leading career destination for tech experts at every stage of their careers. Our client, LTIMindtree, is seeking the following. Apply via Dice today!Job Title: Data EngineerWork LocationIrving TXJob Description:Mandatory Certification should be in Databricks Certified Associated Developer for Apache SparkExperience in Big Data technologies Python Pyspark Hadoop HiveStrong Hands on programming experience working with large volumes of data including different patterns of data ingestion processing batch realtime movement storage and access for both internal and external to BU and ability to make independent decisions within scope of projectGood understanding of data structures and algorithmsCan test debug and fix issues within established SLAs Understanding data access patterns streaming technology data validation data performance cost optimizationStrong SQL NOSQL skillsETL TalenD preferred Any ETL toolKnowledge in Basic Devops technologies shell scripting and code versioning tools such as Github GitlabExperience with Linux OS user levelSkillsMandatory Skills : Apache Spark, Big Data Hadoop Ecosystem, Python, SparkSQLBenefits/perks listed below may vary depending on the nature of your employment with LTIMindtree ( LTIM ):Benefits and Perks:Comprehensive Medical Plan Covering Medical, Dental, VisionShort Term and Long-Term Disability Coverage401(k) Plan with Company matchLife InsuranceVacation Time, Sick Leave, Paid HolidaysPaid Paternity and Maternity LeaveThe range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.Disclaimer: The compensation and benefits information provided herein is accurate as of the date of this posting.LTIMindtree is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.Safe return to office:In order to comply with LTIMindtree s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree s applicable processes.\n        ",
    "location": "Irving, TX"
  },
  "193": {
    "id": "4175045038",
    "Company": "Brooksource",
    "title": "Junior Data Engineer",
    "description": "Junior Data EngineerColumbus, OHBrooksource is looking for entry-level Data Engineers to join our Banking Client in July 2025. We are hiring for multiple positions that will be part of a professional development program. You will receive technical and professional training. If you are interested, please apply!Primary Responsibilities:Work within the team to expand and optimize our data and data pipeline.Participate in the build processes supporting data transformation, data structures, metadata, dependency, and workload management.Analyze and organize raw data.Participate in the build and support of data systems and pipelines.Work with stakeholders to evaluate business needs and objectives.Develop and update code required to meet the requirements of the business.Continuously maintain code.Support the quality of data sourcing, profiling, standardization, mapping, validation, and documentation.Aid the team in evangelizing the value of good data and insights.Develop a deep understanding of business intelligence and analytic tools utilized within the Enterprise Data organization.Aid in the planning, scoping, and estimation of work efforts as well as the production of quality deliverables. Job Requirements:Skills:Applicants should have experience in at least one of the following areas:PythonSQLNoSQLETL development\n",
    "location": "Columbus, Ohio Metropolitan Area"
  },
  "194": {
    "id": "4178251699",
    "Company": "Nationwide",
    "title": "Specialist, Data Engineer (Databricks)",
    "description": "If you\u2019re passionate about innovation and love working in an environment where you can constantly improve and adopt new technologies to drive business results, then Nationwide\u2019s Information Technology team could be the place for you! At Nationwide\u00ae, \u201con your side\u201d goes beyond just words. Our customers are at the center of everything we do and we\u2019re looking for associates who are passionate about delivering extraordinary care.This role does not qualify for employer-sponsored work authorization. Nationwide does not participate in the Stem OPT Extension program.Come join the Data Platforms team to work on Databricks platform support.We are looking for candidates that have experience with Databricks and responsibilities include -Manage and administer the Databricks environment, including clusters, workspaces, and notebooks, to ensure optimal performance, reliability, and scalability.Configure and optimize Databricks clusters and resources based on workload requirements and best practices.Monitor system performance, resource utilization, and availability using Databricks monitoring and logging tools.Troubleshoot and resolve issues related to Databricks infrastructure, networking, security, and integration with other systems.Implement and enforce security best practices to protect data and resources in the Databricks environment, including authentication, authorization, encryption, and data governance.Collaborate with data engineers, data scientists, and analysts to optimize and streamline data workflows, ETL processes, and analytical pipelines on the Databricks platform.Provide technical guidance and support to users of the Databricks platform, including training, documentation, and troubleshooting assistance.Stay current with Databricks releases, updates, and best practices to continuously improve the Databricks environment and maximize its value to the organization.Technical SkillsDatabricksDevOps Tools (Kubernetes (CNP), Jenkins, Harness) / DevSecOps Practices - Working knowledgePython - Must haveUnix shell scriptingWorking knowledge in LINUXAWS Services (e.g S3, EC2, Cloud formation pipeline)General understanding of Data governance and discovery toolsIntegration and API developmentSoft SkillsStrong Communication skills both verbal and writtenProvide technical leadership and help develop/grow junior developersStrong problem solving skillsCapable of being proactive with business leadership on solution options and recommendations that address their requirementsExperience using Agile methodologies and creating good stories that complete a solutionAutomation/Innovative and Continuous Improvement mindsetWorking experience with SaaS and other data tools vendorsStrong problem solving, oral and written communication skillsCapable of leading technical projects and decomposing work across team membersCompensation Grade F5This role will be filled in a hybrid capacity, with collaboration taking place in one of our office locations in Columbus, OH, Scottsdale, AZ, or Des Moines, IA.Job Description SummaryNationwide\u2019s industry leading workforce is passionate about creating data solutions that are secure, reliable and efficient in support of our mission to provide extraordinary care. Nationwide embraces an agile work environment and collaborative culture through the understanding of business processes, relationship entities and requirements using data analysis, quality, visualization, governance, engineering, robotic process automation, and machine learning to produce targeted data solutions. If you have the drive and desire to be part of a future forward data enabled culture, we want to hear from you.As a Data Engineer you\u2019ll be responsible for acquiring, curating, and publishing data for analytical or operational uses. Data should be in a ready-to-use form that creates a single version of the truth across all data consumers, including business users, data scientists, and Technology. Ready-to-use data can be for both real time and batch data processes and may include unstructured data. Successful data engineers have the skills typically required for the full lifecycle software engineering development from translating requirements into design, development, testing, deployment, and production maintenance tasks. You\u2019ll have the opportunity to work with various technologies from big data, relational and SQL databases, unstructured data technology, and programming languages.Job DescriptionKey Responsibilities:Provides basic to moderate technical consultation on data product projects by analyzing end to end data product requirements and existing business processes to lead in the design, development and implementation of data products.Produces data building blocks, data models, and data flows for varying client demands such as dimensional data, standard and ad hoc reporting, data feeds, dashboard reporting, and data science research & explorationTranslates business data stories into a technical story breakdown structure and work estimate so value and fit for a schedule or sprint is determined.Creates simple to moderate business user access methods to structured and unstructured data by such techniques such as mapping data to a common data model, NLP, transforming data as necessary to satisfy business rules, AI, statistical computations and validation of data content. Assists the enterprise DevSecOps team and other internal organizations on CI/CD best practices experience using JIRA, Jenkins, Confluence etc.Implements production processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.Develops and maintains scalable data pipelines for both streaming and batch requirements and builds out new API integrations to support continuing increases in data volume and complexity.Writes and performs data unit/integration tests for data quality With input from a business requirements/story, creates and executes testing data and scripts to validate that quality and completeness criteria are satisfied. Can create automated testing programs and data that are re-usable for future code changes.Practices code management and integration with engineering Git principle and practice repositories.May Perform Other Responsibilities As Assigned.Reporting Relationships: Reports to Manager/Director Data Leader.Typical Skills And ExperiencesEducation: Undergraduate studies in computer science, management information systems, business, statistics, math, a related field or comparable experience and education strongly preferred. Graduate studies in business, statistics, math, computer science or a related field are a plus.License/Certification/Designation: Certifications are not required but encouraged.Experience: Three to five years of relevant experience with data quality rules, data management organization/standards, practices and software development. Experience in data warehousing, statistical analysis, data models, and queries. One to three years\u2019 experience with Cloud technology and infrastructure including security and access management. Insurance/financial services industry knowledge a plus.Knowledge, Abilities and Skills: Data application and practices knowledge. Moderate to advanced skills with modern programming and scripting languages (e.g., SQL, R, Python, Spark, UNIX Shell scripting, Perl, or Ruby). Good problem solving, oral and written communication skills.Other criteria, including leadership skills, competencies and experiences may take precedence.Staffing exceptions to the above must be approved by the hiring manager\u2019s leader and HR Business Partner.Values: Regularly and consistently demonstrates the Nationwide Values.Job ConditionsOvertime Eligibility: Exempt (Not Eligible)Working Conditions: Normal office environment.ADA: The above statements cover what are generally believed to be principal and essential functions of this job. Specific circumstances may allow or require some people assigned to the job to perform a somewhat different combination of duties.BenefitsWe have an array of benefits to fit your needs, including: medical/dental/vision, life insurance, short and long term disability coverage, paid time off with newly hired associates receiving a minimum of 18 days paid time off each full calendar year pro-rated quarterly based on hire date, nine paid holidays, 8 hours of Lifetime paid time off, 8 hours of Unity Day paid time off, 401(k) with company match, company-paid pension plan, business casual attire, and more. To learn more about the benefits we offer, click here.Nationwide is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive culture where everyone feels challenged, appreciated, respected and engaged. Nationwide prohibits discrimination and harassment and affords equal employment opportunities to employees and applicants without regard to any characteristic (or classification) protected by applicable law.Smoke-Free Iowa Statement: Nationwide Mutual Insurance Company, its affiliates and subsidiaries comply with the Iowa Smokefree Air Act. Smoking is prohibited in all enclosed areas on or around company premises as well as company issued vehicles. The company offers designated smoking areas in which smoking is permitted at each individual location. The Act prohibits retaliation for reporting complaints or violations. For more information on the Iowa Smokefree Air Act, individuals may contact the Smokefree Air Act Helpline at 888-944-2247.\n        ",
    "location": "Columbus, OH"
  },
  "195": {
    "id": "4175543469",
    "Company": "Eaton",
    "title": "Cloud Data Engineer",
    "description": "Eaton\u2019s Corporate Sector division is currently seeking a Cloud Data Engineer.The expected annual salary range for this role is $105000.03 - $154000.04 a year.Please note the salary information shown above is a general guideline only. Salaries are based upon candidate skills, experience, and qualifications, as well as market and business considerations.What You\u2019ll DoJob Summary:At Eaton we are modernizing the way we use data to accelerate our digital transformation. Join our Information Technology team and help positively impact our business through leading technologies, exciting and challenging enterprise projects, and new platforms. We are transforming to a Product Centric organization utilizing agile practices to help us work more incrementally and fluidly, allowing us to respond more quickly to changing business needs.In this role, you will be instrumental in implementing the integration strategy for the data mesh architecture as part of the Chief Data Management office. You will be the subject matter expert on the integration platforms and will lead research into exploring and enabling the right toolset for the Hybrid integration platform and enabling democratized agile integration across the organization. You will also enable the team to rapidly experiment and iterate on strategies, helping drive critical decisions on how we shape the product and services provided by integration services.Making what matters work at Eaton takes the passion of every employee around the world. We create an environment where creativity, invention and discovery become reality, each and every day. It\u2019s where bold, bright professionals like you can reach your full potential\u2014and where you can help us reach ours.\"Job Responsibilities Create robust and automated data pipelines using traditional and modern data integration technologies, including ETL/ELT, data replication/CDC, streaming, message-oriented data movement and API-based data acquisition leveraging cloud native tool-set. Monitor and optimize data integration processes for performance and scalability and implement data security measures for the data ingestion pipelines Drive development and adoption of self-service capabilities and champion as an enabler for adopters of the platform Troubleshoot and resolve data integration issues and provide operational support for releases and maintenance. Adhere to operational SLAs and KPIs. Establish end to end continuous integration and deployment and guide the team on best practices Simplify the tooling landscape to sunset legacy applications while minimizing business disruption.\"QualificationsBasic Qualifications(required): Bachelor's degree in STEM (Science, Technology, Engineering, Mathematics) from accredited university. 6+ years of experience in data integration, data lifecycle management and programming. 3+ years of experience with one of the Cloud platforms ( Azure, GCP or AWS ) Must be legally authorized to work in the United States without company sponsorship both now and in the future. No relocation benefit is being offered for this role. All candidates considered must live within 50 miles of job posting location. Active-Duty Military Service member candidates are exempt from the geographical area limitation. This role must report on-site 3 days a week at one of the following locations: Beachwood, OH; Charlotte, NC ; Moon Township, PA ; Houston, TX . Preferred Qualifications(required) Demonstrated experience working with large datasets building ETL/ELT pipelines using technologies such as Azure Data Factory, Informatica Cloud, dbt etc. Experience with distributed systems, streaming platforms such as Confluent Kafka, Azure Event . Proficiency with SQL,and using Python or Java for data processing pipelines. Strong experience with Terraform, GitHub and GitHub Actionsc. Experience with data replication/CDC tools such as Qlik Replicate, Fivetran or Golden Gate is a plus. Strong understanding of cloud-based data solutions such as Azure Data Lake Storage, Snowflake, Redshift, Databricks etc. Ability to lead as the subject matter expertise in the space through diagnosing, isolating, and resolving complex issues and recommending and implementing strategies to resolve problems. Curious and solution oriented, ability to handle problems from all sides, work outside the comfort zone , collaborative, open minded and effective communicator. Ability to identify operational issues and proactively solutions to resolve problems, operate in a KPI driven framework.We are committed to ensuring equal employment opportunities for all job applicants and employees. Employment decisions are based upon job-related reasons regardless of an applicant's race, color, religion, sex, sexual orientation, gender identity, age, national origin, disability, marital status, genetic information, protected veteran status, or any other status protected by law.Eaton considers qualified applicants regardless of criminal histories, consistent with local laws. To request a disability-related reasonable accommodation to assist you in your job search, application or interview process, please call us at 1-800-836-6345 to discuss your specific need. Only accommodation requests will be accepted by this phone number.We know that good benefit programs are important to employees and their families. Eaton provides various Health and Welfare benefits as well as Retirement benefits, and several programs that provide for paid and unpaid time away from work. Click here for more detail: Eaton Benefits Overview. Please note that specific programs and options available to an employee may depend on eligibility factors such as geographic location, date of hire, and the applicability of collective bargaining agreements.]]>\n        ",
    "location": "Houston, TX"
  },
  "196": {
    "id": "4188720423",
    "Company": "Highbrow LLC",
    "title": "Data Engineer ",
    "description": "Job DescriptionWe are seeking a highly skilled Data Engineer Supply Chain with SAP MM PP expertise. The ideal candidate will be responsible for Design, develop, and maintain SAP MM (Materials Management) and PP (Production Planning) data solutions to support business operations. Ensure data accuracy, integration, and optimization across SAP modules and related systems. Collaborate with cross-functional teams to implement data migration, transformation, and reporting strategies. Troubleshoot and enhance SAP data processes to improve efficiency and decision-making.Key Responsibilities:Skilled in data wrangling, cleansing, and transformation of inventory-related datasets from SAP MM PP Modules. Experience in building scalable pipelines, API-based data access, and workflow automation.Experience working with supply chain and operations teams to align data strategies with business objectives.Experience with inventory management, safety stock optimization, and SKU-level inventory flow analytics is a PLUS.Optimize application and data performance, responsiveness, and accessibility.Collaborate with cross-functional teams to ensure seamless integration between frontend and backend components.Monitor data pipelines performance, troubleshoot issues, and optimize system reliability.Required skills and qualifications:Bachelor\u2019s or Master\u2019s degrees in engineering, Mathematics, Statistics, Computer Science, philosophy or a related field.\n",
    "location": "Austin, TX"
  },
  "197": {
    "id": "4178121783",
    "Company": "Xoriant",
    "title": "Data Engineer",
    "description": "W2 Only role.Data Engineer (Must have Databricks) Need Mid level (3-5Yrs) Location : North Quincy MA \u2013 Hybrid Onsite Key Skill: Strong Databricks, AWS, Apache Spark, SQL, Scala/Python, Hadoop Skill Sets RequiredGood decision-making and problem solving skillsSolid understanding of Databricks fundamentals/architecture and have hands on experience in setting up Databricks cluster, working in Databricks modules (Data Engineering and SQL warehouse).Solid Knowledge on medallion architecture, DLT and unity catalog within Databricks.Experience in migrating data from on-prem Hadoop to Databricks/AWSUnderstanding of core AWS services, uses, and AWS architecture best practicesHands-on experience in different domains, like database architecture, business intelligence, machine learning, advanced analytics, big data, etc.Solid knowledge on AirflowSolid knowledge on CI/CD pipeline in AWS technologiesApplication migration of RDBMS, java/python applications, model code, Opensearch etc.Solid programming background on scala and/or python with SparkExperience with Docker and Kubernetes is a plus\n",
    "location": "Boston, MA"
  },
  "198": {
    "id": "4178280551",
    "Company": "TRKKN",
    "title": "Data Analytics Engineer",
    "description": "As a catalyst for innovative digital marketing ideas, what excites us at TRKKN is using a data-driven approach to take our clients\u2019 business to the next level.Working for TRKKN means that you'll be part of an inspiring, energetic, and international team in a fast-growing tech environment. We offer the ideal workplace for anyone who wants to grow, learn, and has a strong desire for solving complex business challenges.Being on the bleeding edge of marketing technology and innovation means you will have an unparalleled opportunity for learning, along with personal and professional growth: there's never a dull moment here.ResponsibilitiesAs an Analytics Engineer, reporting into the Head of Cloud and AI at TRKKN North America (NA), you\u2019ll focus on building, transforming, and helping to grow the strategic database architecture for large scale ETL products and services.Whether working with our existing repository of products or building TRKKN\u2019s next-gen capabilities, you\u2019ll leverage your expertise in SQL, database architecture, and marketing knowledge to develop and deploy scalable solutions within the cloud.Working within TRKKN globally, you will contribute directly to company growth goals by faithfully managing and executing robust and complex database engineering projects.To Be Successful In This Role, You WillAct as Analytics Engineer (SME) in the delivery and development of database creation, transformation, and strategic oversightDeploy data products from our existing proprietary codebase and work within multiple environments to continue to stand-up new data driven features within databases like BigQueryDesign tech infrastructure and data pipelines (ETL, data automation, data querying, etc.) when necessaryRequirements3-5 years of experience, preferably in Data Engineering, Analytics Engineering, or Data ScienceStrong analytical mindset with expert knowledge of SQL and BigQuery, as well as BI tools such DataStudio and Looker (LookML) to implement business reports and BI solutions.Expertise with Google Cloud Platform\u2019s distributed Data Science and Data Engineering tools (e.g. BigQuery, DBT, Docker, Kubernetes)Experience with Git, GitLab, CI/CD pipelines on GitlabStrong analytical mindset with expert knowledge of SQL and BigQuery, as well as BI tools such DataStudio and Looker (LookML) to implement business reports and BI solutions.Experience working on a data science or engineering team, participating in scrums, sprints, and retrospectivesAbility to clearly explain complex ideas to non-expert audiencesFamiliarity with media and media measurement a plusBachelor\u2019s degree in a quantitative field (advanced degree is preferred)#TRKKNThis is the pay range the Company believes it will pay for this position at the time of this posting. Consistent with applicable law, compensation will be determined based on relevant experience, other job-related qualifications/skills, and geographic location (to account for comparative cost of living). The Company reserves the right to modify this pay range at any time. For this role, benefits include: health insurance, vision insurance, dental insurance, 401(k), Healthcare Flexible Spending Account, Dependent Care Flexible Spending Account, vacation days, sick days, personal days, paid parental leave, paid medical leave, and STD/LTD insurance benefits.Compensation Range$120,000\u2014$140,000 USDThis role is hybrid, requiring three (3) days per week in the office. The remaining two (2) days may be worked remotely. Specific in-office days will be discussed during the interview process, with flexibility to align with team needs. Please note that the number or required in-office days may be adjusted over time, potentially increasing the number of required in-office days based on business needs.\n",
    "location": "New York, NY"
  },
  "199": {
    "id": "4188950803",
    "Company": "Envision",
    "title": "Data Engineer",
    "description": "Is Data your thing, well, we have what you are looking for on your next career move.Role / ResponsibilitiesThe position is responsible for providing ongoing maintenance and support of a complex application that supports communicable disease surveillance, registries, and case management systems that are critical to supporting effective responses to public health emergencies and reducing the burden of communicable diseases. You will provide technical oversight to other developers in the team that support other critical applications.Lead the design and development of scalable and high-performance solutions using AWS services.Experience with Databricks, Elastic search, Kibanna, S3.Experience with Extract, Transform, and Load (ETL) processes and data pipelines.Write clean, maintainable, and efficient code in Python/Scala.Experience with AWS Cloud-based Application DevelopmentExperience in Electronic Health Records (EHR) HL7 solutions.Implement and manage Elastic Search engine for efficient data retrieval and analysis.Experience with data warehousing, data visualization Tools, data integrityExecute full software development life cycle (SDLC) including experience in gathering requirements and writing functional/technical specifications for complex projects.Excellent knowledge in designing both logical and physical database modelDevelop database objects including stored procedures, functions,Extensive knowledge on source control tools such as GITDevelop software design documents and work with stakeholders for review and approval.Exposure to flowcharts, screen layouts and documentation to ensure logical flow of the system requirementsExperience working on large agile projects.Experience or Knowledge on creating CI/CD pipelines using Azure DevopsQualifications12+ years developing complex database systems.8+ years Databricks.8+ years using Elastic search8+ years using Python/Scala.8+ years Oracle.5+ years experience with Extract, Transform, and Load (ETL) processes and developing Data Pipelines.5+ years experience with AWS.Over 5+ years experience with data warehousing, data visualization Tools, data integrity .Over 5+ years using CMM/CMMI Level 3 methods and practices.Over 5+ years implemented agile development processes including test driven development.Over 3+ years Experience or Knowledge on creating CI/CD pipelines using Azure Devops- Nice to have\n",
    "location": "Lansing, MI"
  },
  "200": {
    "id": "4168166204",
    "Company": "ICM Inc",
    "title": "Junior Data Analytics Engineer",
    "description": "POSITION PURPOSE:No remote or hybrid opportunity. This position is located onsite in Colwich, KS. The junior data analytics engineer is responsible for the development and implementation of data solutions within our Azure, D365, and Fabric environment. This resource will play a crucial role in extracting, transforming, and loading (ETL) data, and developing reporting solutions to drive ICM\u2019s operations. Additionally, this resource will be responsible for creating reports and must be able to perform technical/analytical work, providing insight into the efficiency and functionality of company systems. The individual must maintain a comprehensive understanding of relational database concepts, table design and structure and this knowledge should be translated and applied to the data based on business needs. This role will work alongside other technical resources and BSA staff to drive initiatives forward.ESSENTIAL FUNCTIONS AND BASIC DUTIES: Data EngineeringDesign, develop, and maintain robust data pipelines using Fabric Data Factory or Visual Studio SSIS.Ingest, clean, and transform data from various sources, including AX2012 and D365, into a suitable format for analysis.Write complex SQL queries to extract, manipulate, and analyze data from relational data sources.Participate in data conversion planning, design, development, testing and implementingData ModelingDesign, develop and implement data models, including dimensional and star schemas, to support business intelligence and analytics.Create data marts and data warehouses to store and standardize data for analysis.Data Analysis and ReportingUtilize advanced analytics techniques, to extract valuable insights from data.Leverage SQL to perform data exploration, analysis, and report engineering.Develop interactive dashboards and reports using Power BI, SAP BOBJ or other visualization tools to communicate findings effectively to stakeholders.Review, interpret, and update existing reports Provide troubleshooting, break/fix support, and optimization of existing solutions in line with industry best practicesAbility to assist with data validation and auditingProvide recommendations to address existing/future risk in business intelligence infrastructureEnsure compatibility and interoperability across on-premises and cloud-based computing systems Complete all other data analytics duties as assignedCloud TechnologiesLeverage cloud technologies, including Azure Data Lake Storage, One Lake, and MS FABRIC, to build scalable and reliable data solutions.Utilize Spark on MS FABRIC to perform big data processing and analysis tasks.Collaboration and CommunicationCollaborate with business systems analysts and other team members to understand business requirements and translate them into technical requirements and solutions.Communicate complex technical concepts to both technical and non-technical audiences.Maintain data documentation and ensure quality and accuracyEducation/Certification: Bachelor Degree in Computer Science (or related field) required, or equivalent training and/or on-the-job experience.Required Knowledge & Experience: 5+ years developing reports5+ years working with SQL3+ years working with SSIS3+ years experience with data warehousing and ETL processes1+ years experience with Apache Spark Strong proficiency in SQL, Python and M programming languagesExperience with data visualization tools like SAP BOBJ, Power BI, TableauKnowledge of cloud computing platforms, specifically AzureExperience integrating with ERP platforms (i.e., Microsoft D365, AX2012, GP, NAV) preferredExperience managing SQL servers, maintenance plans, alerting preferredExtensive experience with Microsoft 365 products\n",
    "location": "Colwich, KS"
  }
}